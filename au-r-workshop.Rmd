--- 
title: "Introduction to R for Natural Resource Scientists"
author: "Ben Staton"
date: "with contributions by Henry Hershey"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: true
linkcolor: "blue"
urlcolor: "blue"
github-repo: bstaton1/au-r-workshop
description: "This is a first course in R programming for natural resource scientists. It was developed and has been primarily instructed at Auburn University."
---

# Overview{-#overview}

`r if(knitr::is_html_output()) '<a href="https://github.com/bstaton1/au-r-workshop/"><img src="img/cover_image.png" width="250" height="375" alt="Cover image" align="right" style="margin: 0 1em 0 1em" /></a>' `This book is intended to be a first course in R programming for natural resource professionals. It is by no means comprehensive (no book about R ever could be), but instead attempts to introduce the main topics needed to get a beginner up and running with applying R to their own work. It is intended to be a companion to in-person workshop sessions, in which each chapter is covered in a 2 hour session, however it can be used as "self-teach" manual as well. Although the examples shown have a natural resource/ecological theme, the general skills presented are general to R users across all scientific disciplines. 

## What is Covered? {-}

The book is composed of six chapters intended to cover a suite of topics in introductory R programming. In general, the material builds in complexity from chapter to chapter and earlier chapters can be seen as prerequisites for later chapters. 

*  **Chapter \@ref(ch1)** covers the basics of working in R through RStudio, including the basics of the R coding language and environment.
*  **Chapter \@ref(ch2)** covers the basics of plotting using the base R graphics functionality. 
*  **Chapter \@ref(ch3)** covers the basics of fitting statistical models using built-in functionality for generalized linear models as well as non-linear models.   
*  **Chapter \@ref(ch4)** covers the basics of simulation modeling in R. 
*  **Chapter \@ref(ch5)** covers the basics of the `{dplyr}` and `{reshape2}` packages for manipulating and summarizing large data sets using highly readable code.
*  **Chapter \@ref(ch6)** covers the basics of producing maps and performing spatial analysis in R. _This chapter was contributed by Henry Hershey_

## Prerequisites {-}

Chapter \@ref(ch1) starts at the first step (installing R) and progresses by assuming no prior knowledge of programming in R or in any other language. In the later chapters, e.g., Chapters \@ref(ch3) and \@ref(ch4), an understanding of statistics at the introductory undergraduate level would be helpful but not strictly essential.

There are, however, some tasks you'll need to complete before using this book, which are described in the two sections that follow.

### Prepare Your Computer {-#comp-prep}

#### Installation {-#install}

First off, you will need to get R and RStudio^[While it is possible to run R on its own, it is clunky. You are strongly advised to use the RStudio IDE (integrated development environment) given its compactness, neat features, code tools (like syntax and parentheses highlighting). This workshop will assume you are using RStudio] onto your computer. Go to:

*  <https://cran.rstudio.com/> to get R and
*  <https://www.rstudio.com/products/rstudio/download/> to get RStudio Desktop.

Download the appropriate installation file for your operating system and run that file. All default settings should be fine. 

#### Optional Configuration {-}

As a matter of personal preference, you are recommended to configure a few settings. Open up RStudio and go to _Tools > Global Options_, and in the section listed "General":

*  Make sure _Restore .RData into workspace at startup_ is **unchecked**
*  Make sure _Save workspace to .RData on exit_ is set to **Never**
*  Make sure _Always save history (even when not saving .RData)_ is **unchecked**.

These settings will prevent you from getting a bunch of useless files and dialog boxes every time you open and close R.

#### Create a Book Directory {-}

You should create a devoted folder on your computer for this book. All examples will assume this folder is located here: `C:/Users/YOU/Documents/R-Book`. Change `YOU` to be specific for your computer.

#### Data Sets {-#data-sets}

The data sets^[Many of the data sets used in this book were simulated by the author. Cases in which the data set used was not simulated are noted and a citation to the data source is provided. More details on the individual data sets can be found on the GitHub repository.]
 used in this book are hosted on a GitHub repository maintained by the author. It is located here: <https://github.com/bstaton1/au-r-workshop-data>.

To acquire the data for this book, you should:

  1.  Navigate to the GitHub repository
  2.  click the green _Clone or download_ button at the top right,
  3.  click _Download ZIP_
  4.  unzip the contents of this folder into the location: `C:/Users/YOU/Documents/R-Book/Data`

File organization will be very important for your success in learning to use R. This book will assume your `R-Book` directory is organized as shown below. Notice that there is a separate folder for the data downloaded from GitHub as well as one for each chapter that will house the R code for that chapter. Do not worry about making all of these folders now, you will do this at the appropriate time as you work your way through this book. For now, just make sure there is a `Data` folder that contains all of the unzipped contents from the GitHub repository your main `R-Book` directory.

```{r, echo = F}
path = c(
  "R-Book/Data/asl.csv",
  "R-Book/Data/...",
  "R-Book/Data/Ch6",
  "R-Book/Chapter1/Ch1.R",
  "R-Book/Chapter1/Ex1A.R",
  "R-Book/Chapter1/Ex1B.R",
  "R-Book/Chapter2/Ch2.R",
  "R-Book/Chapter2/Ex2.R",
  "R-Book/Chapter3",
  "R-Book/Chapter4",
  "R-Book/Chapter5",
  "R-Book/Chapter6"
)

data.tree::as.Node(data.frame(pathString = path))
```

## Exercises {-}

Following each chapter, there is a set of exercises. You should attempt and complete them, as they give you an opportunity to practice what you learned while reading and typing along. Solutions are provided at the end of this book, however you are **strongly** recommended to attempt to figure the problems out on your own before looking to how the author would solve them.

Some exercises have bonus questions. These are intended to challenge you with some of the more difficult tasks shown in the chapter or ask you to extend what you learned to a completely different problem. If you can get all of the non-bonus questions without looking at the solutions too much, you can consider yourself to have good understanding of that chapter's material. If you can complete the bonus questions with little or no help, that means you have mastered that chapter's material!

## Text Conventions {-#notation}

*  Regular text: a description of what you you should do, how some code works, or a general narrative of something.
*  `monospace`: references something in R
    *  `this()` references some function
    *  `this` references some other object
    *  `{this}` references an R package
    *  `C:/This` is a file path
*  **Bold** is intended to provide more emphasis to a word or topic. In general, new topics are introduced this way.
*  [Links](#notation): this is a link to some other location in this book. External links are provided with a full URL.
*  $Equations$: it is sometimes useful to describe concepts mathematically before showing how to do it in R.
*  ^[This is a footnote. If you're viewing this on GitHub Pages, click the arrow to the right to return to the text]: a footnote containing more information.

## Keyboard Shortcuts {-}

Several parts of this book in this book make reference to keyboard shortcuts. They are never necessary, but can help you be more efficient if you commit them to muscle memory. This book assumes you are using a PC for the keyboard shortcuts. If you are using a Mac, they will be different^[For some keyboard shortcuts, you may just need to swap out the **CTRL** keystroke for the **CMD** keystroke for a Mac computer]. For a complete list of RStudio's keyboard shortcuts specific to your operating system, go to _Help > Keyboard Shortcuts Help_.

## Development of this Book {-}

This book represents the third reincarnation of the Auburn R Workshop Series. The first version was written in Fall 2014 using Microsoft Word, but the author found that making even small changes was clunky - each change to code in the document required a copy-paste of code and output from R to Word. Individual session materials (i.e., handout, exercises, solutions, data) were created in separate documents, saved as PDFs and `.xlsx` files, and uploaded to a wordpress webpage. 

The second version was written through R [@R-base] and RStudio using the R packages `{rmarkdown}` [@R-rmarkdown] and `{knitr}` [@R-knitr; @knitr-cite], which allowed the integration of text, code, and output all into one output file. This version was completed in Fall 2015. Like the first version, individual session materials were created in separate documents, and replaced those previously found on the wordpress site. 

This third version was written through R and RStudio but used the R package `{bookdown}` [@R-bookdown] which allowed for the individual sessions to be combined into one "book" by turning each session into a chapter. This facilitated cross-references to topics covered in previous chapters and allows the reader to only refer to one location when trying to remember how to use a skill. It also allowed for multiple formats to be published including both HTML and PDF versions.

The book is hosted on [GitHub Pages](https://pages.github.com/), and was last built on `r format(Sys.time(), format = "%m-%d-%Y")`.

## About the Author {-}

Ben Staton is a PhD candidate in the School of Fisheries at Auburn University. He studies quantitative methods for assessing fish populations for use in harvest management, with a focus on Pacific salmon in western Alaska. His interests are in population dynamics, Bayesian methods, Monte Carlo methods, and reproducible research. Ben has been using R on a daily basis since the beginning of his graduate work in 2014, and is enthusiastic about helping others learn to use R for their own work. 

<!--chapter:end:index.Rmd-->

# Introduction to the R Environment {#ch1}

```{r, echo = F, warning = F, message = F}
library(dplyr)
library(kableExtra)
library(knitr)
```

## Chapter Overview{-}

In this first chapter, you will get familiar with the basics of using R. You will learn:

*  how to use R as a basic calculator
*  some basic object types
*  some basic data classes
*  some basic data structures
*  how to read in data
*  how to produce basic data summaries
*  how to write out data
*  how to write your own functions

## Before You Begin {-}

Before you start this chapter, you should make sure you have read and done everything in the [Overview](#overview) pertaining to [preparing your computer](#comp-prep) for working through this book.

## The R Studio Interface

When you open up RStudio for the first time, you will see three panes: the left-hand side is the **console** where results from executed commands are printed, and the two panes on the right are for additional information to help you code more efficiently - don't worry too much about what these are at the moment. For now, focus your attention on the console.

### Write Some Simple Code

To start off, you will use R as a simple calculator. Type these commands (not the lines with `##`, those are output^[The formatting used here includes `##` on output to denote code and output separately. You won't see the `##` show up in your console.]) one at a time and hit **CTRL + ENTER** to run it. The spaces don't matter at all, they are used here for clarity and for styling. To learn more about standard R code styling, check out the section in @adv-r-cite on it^[Hadley Wickham's R code style guide: <http://adv-r.had.co.nz/Style.html>].

```{r Calculator}
3 + 3
12/4
```

Notice that when you run each line, it prints the command and the output to the console. 

R is an **object oriented language**, which means that you fill objects with data and do things with them. Make an object called `x` that stores the result of the calculation `3 + 3` (type this and run using **CTRL + ENTER**):

```{r}
x = 3 + 3
```

Notice that running this line did not return a value as before. This is because in that line you are **assigning** a value to the object `x`. You can view the contents of `x` by typing its name alone and running just that:

```{r}
x
```

When used this way, the `=` sign denotes assignment of the value on the right-hand side to an object with the name on the left-hand side. The `<-` serves this same purpose so in this context the two are interchangeable:

```{r}
y <- 2 + 5
```

You can highlight smaller sections of a line to run as well. For example after creating `y` above, press the **up arrow** to see the line you just ran, highlight just the `y`, and press **CTRL + ENTER**. From this point forward, the verb "run" means execute some code using **CTRL + ENTER**.

You can use your objects together to make a new object:

```{r}
z = y - x
```

**Here are some things to note about object names:**

*  Object names can contain any of the following:
    - letters
    - numbers
    - the `.` or `_` symbols
*  Object names must start with a letter, not a number or symbol and cannot contain spaces
*  As a general rule, avoid naming your objects things that already have names in R, e.g., `data()`, `mean()`, `sum()`, `sd()`, etc.
*  Capitalization matters: `A` and `a` are two different objects
*  Keep your names short with abbreviations or shorthand

## Saving Your Code: Scripts {#scripts}

If you closed R at this moment, your work would be lost. Running code in the console like you have just done **does not save a record of your work**. To save R code, you must use what is called a **script**, which is a plain-text file with the extension `.R`. To create a new script file, go to _File > New File > R Script_, or use the keyboard shortcut **CTRL + SHIFT + N**. A new pane will open called the **source** pane - this is where you will edit your code and save your progress. R Scripts are a key feature of reproducible research with R, given that if they are well-written they can present a complete road map of your statistical analysis and workflow.

## The Working Directory {#working-dir}

Keeping things organized is essential to efficient use of R. For this book, you should have a separate subfolder for your scripts in each chapter. Create a subfolder called `C:/Users/YOU/Documents/R-Book/Chapter1` and save your `Ch1.R` script there. 

Part of keeping your work organized in R is making sure you know where R is looking for your files. One way to facilitate this is to use a **working directory**. This is the location (i.e., folder) on your computer that your current R session will "talk to" by default. R will read files from and write files to the working directory by default. Because you'll likely be visiting it often, it should probably be somewhere that is easy to remember and not too deeply buried in your computer's file system. 

To set the working directory to `C:/Users/YOU/Documents/R-Book/Chapter1`, you have three options:

1.  **Go to Session > Set Working Directory > Source File Location**. This will set the working directory to the location of the file that is currently open in your source pane.

2. **Go to Session > Set Working Directory > Choose Directory**. This will open an interactive file selection window to allow you to navigate to the desired directory.

3. **Use code**. In the console, you can type `setwd("C:/Users/YOU/Documents/R-Book/Chapter1")`. If at any point you want to know where your current working directory is set to, you can either look at the top of the console pane, which shows the full path or by running `getwd()` in the console. Note the use of `/` rather than `\` for file paths in R. If you are using a Mac, omit  `C:` from your directory name.

**The main benefits of using a working directory are**:

*  Files are read from and written to a consistent and predictable place every time
*  Everything for your analysis is organized into one place on your computer
*  You don't have to continuously type file paths to your work. If `file.txt` is a file in your current working directory, you can reference it your R session using `"file.txt"` rather than with `"C:/Users/YOU/Documents/R-Book/Chapter1/file.txt"` each time.

Note that while it is generally good practice to keep your data in the working directory, this is not recommended for this book. You will be using the same data files in multiple chapters, so it will help if they are all stored in one location. More details on this later (Section \@ref(read)).

## R Object Types

R has a variety of object types that you will need to become familiar with.

### Functions

Much of your work in R will involve functions. A function is called using the syntax:

```{r, eval = F}
fun(arg1 = value1, arg2 = value2)
```

Here, `fun()` is the **function name** and `arg1` and `arg2` are called **arguments**. Functions take input in the form of the arguments, do some task with them, then return some output. The parentheses are a sure sign that `fun()` is a function.

The syntax above passes the function two arguments by name: all functions have arguments, all arguments have names, and there is always a default order to the arguments. If you memorize the argument order of functions you use frequently, you don't have to specify the argument names:

```{r, eval = F}
fun(value1, value2)
```

would give the same result as the command above in which the argument names were specified.

Here's a real example:

```{r}
print(x = z)
```

The function is `print()`, the argument is `x`, and the value you have supplied the argument is the object `z`. The task that `print()` does is to print the value of `z` to the console.

R has a ton of built-in documentation to help you learn how to use a function. Take a look at the help file for the `mean()` function. Run `?mean` in the console: a window on the right-hand side of the R Studio interface should open. The help file tells you what goes into a function and what comes out. For more complex functions it also tells you what all of the options (i.e., arguments) can do. Help files can be a bit intimidating to interpret at first, but they are all organized the same and once you learn their layout you will know where to go to find the information you're looking for.

### Vectors

Vectors are one of the most common data structures. A vector is a set of numbers going in only one dimension. Each position in a vector is called an **element**, and the number of elements is called the **length** of the vector. Here are some ways to make some vectors with different elements, all of length five:

```{r}
# this is a comment. R will ignore all text on a line after a #
# the ; means run everything after it on a new line

# count up by 1
month = 2:6; month

# count up by 2
day = seq(from = 1, to = 9, by = 2); day

# repeat the same number (repeat 2018 5 times)
year = rep(2018, 5); year
```

The `[1]` that shows up is an element position, more on this later (see Section \@ref(sub)). If you wish to know how many elements are in a vector, use `length()`:

```{r}
length(year)
```

You can also create a vector "by-hand" using the `c()` function^[The `c` stands for **concatenate**, which basically means combine many smaller objects into one larger object]:

```{r}
# a numeric vector
number = c(4, 7, 8, 10, 15); number

# a character vector
pond = c("F11", "S28", "S30", "S8", 'S11'); pond
```

Note the difference between the numeric and character vectors. The terms "numeric" and "character"" represent **data classes**, which specify the type of data the vector is holding:

*  A **numeric vector** stores numbers. You can do math with numeric vectors
*  A **character vector** stores what are essentially letters. You can't do math with letters. A character vector is easy to spot because the elements will be wrapped with quotes^[`" "` or `' '` both work as long as you use the same on the front and end of the element]. 

A vector can only hold one data class at a time:

```{r}
v = c(1,2,3,"a"); v
```

Notice how all the elements now have quotes around them. The numbers have been **coerced** to characters^[The coercion works this way because numbers can be expressed as characters, but a letter cannot be unambiguously expressed as a number.]. If you attempt to calculate the sum of your vector:

```{r, error=T}
sum(v)
```

you would find that it is impossible in its current form.

### Matrices {#matrices}

Matrices act just like vectors, but they have two dimensions, i.e., they have both rows and columns. An easy way to make a matrix is by combining vectors you have already made:

```{r}
# combine vectors by column (each vector will become a column)
m1 = cbind(month, day, year, number); m1

# combine vectors by row (each vector will become a row)
m2 = rbind(month, day, year, number); m2
```

Just like vectors, matrices can hold only one data class (note the coercion of numbers to characters):

```{r}
cbind(m1, pond)
```

Each vector should have the same length.

### Data Frames {#data-frames}

Many data sets you will work with require storing different data classes in different columns, which would rule out the use of a matrix. This is where **data frames** come in:

```{r}
df1 = data.frame(month, day, year, number, pond); df1
```

Notice the lack of quotation marks which indicates that all variables (i.e., columns) are stored as their original data class. 

It is important to know what kind of object type you are using, since R treats them differently. For example, some functions can only use a certain object type. The same holds true for data classes (numeric vs. character). You can quickly determine what kind of object you are dealing with by using the `class()` function:

```{r}
class(day); class(pond); class(m1); class(df1)
```

## Factors {#factors}

At this point, it is worthwhile to introduce an additional data class: factors. Notice the data class of the `pond` variable in `df1`:

```{r}
class(df1$pond)
```

The character vector `pond` was coerced to a factor when you placed it in the data frame. A vector with a **factor** class is like a character vector in that you see letters and that you can't do math on it. However, a factor has additional properties: in particular, it is a grouping variable. See what happens when you print the `pond` variable:

```{r}
df1$pond
```

A factor has levels, with each level being a subcategory of the factor. You can see the unique levels of your factor by running:

```{r}
levels(df1$pond)
```

Additionally, factor levels have an assigned order (even if the levels are totally nominal), which will become important in Chapter \@ref(ch3) when you learn how to fit linear models to groups of data, in which one level is the "reference" group that all other groups are compared to (see Section \@ref(anova) for more details). 

If you run into errors about R expecting character vectors, it may be because they are actually stored as factors. When you make a data frame, you'll often have the option to turn off the automatic factor coercion. For example:

```{r, eval = F}
data.frame(month, day, year, number, pond, stringsAsFactors = F)
read.csv("streams.csv", stringsAsFactors = F)  # see below for details on read.csv
```

will result in character vectors remaining that way as opposed to being coerced to factors. This can be preferable if you are doing many string manipulations, as character vectors are often easier to work with than factors. 

## Vector Math {#vector-math}

R does vectorized calculations. This means that if supplied with two numeric vectors of equal length and a mathematical operator, R will perform the calculation on each pair of elements. For example, if you wanted to add the two vectors `day` and `month`, then you would just run:

```{r}
dm = day + month; dm
```

Look at the contents of both `day` and `month` again to make sure you see what R did. You typically should ensure that the vectors you are doing math with are of equal lengths. 

You could do the same calculation to each element of a vector (e.g., divide each element by 2) with:

```{r}
dm/2
```

## Data Subsets/Queries {#sub}

This is perhaps the most important and versatile skill to know in R. Say you have an object with data in it and you want to use it for analysis, but you don't want the whole data set: just a few rows or just a few columns, or perhaps you need just a single element from a vector. This section is devoted to ways you can extract certain parts of data objects (the terms **query** and **subset** are often used interchangeably to describe this task). There are three main methods:

1.  **By Index** -- This method allows you to pull out specific rows/columns by their location in an object. However, you must know exactly where in the object the desired data are. An **index** is a location of an element in a data object, like the element position or the position of a specific row or column. The syntax for subsetting a vector by index is `vector[element]` and for a matrix it is `matrix[row,column]`. Here are some examples:

```{r}
# show all of day, then subset the third element
day; day[3]

# show all of m1, then subset the cell in row 1 col 4 
m1; m1[1,4]

# show all of df1, then subset the entire first column
df1; df1[,1]
```

Note this last line: the `[,1]` says "keep all the rows, but take only the first column".

Here is another example: 
```{r}
# show m1, then subset the 1st, 2nd, and 4th rows and every column
m1; m1[c(1,2,4),]
```

Notice how you can pass a vector of row indices here to exclude the 3^rd^ and 5^th^ rows.

**2. By name** -- This method allows you to pull out a specific column of data based on what the column name is. Of course, the column must have a name first. The name method uses the `$` operator:

```{r}
df1$month
```

You can combine these two methods:

```{r}
df1$month[3]

# or
df1[,c("year", "month")]
```

The `$` method is useful because it can be used to add columns to a data frame:

```{r}
df1$dm = df1$day + df1$month; df1
```

3.  **Logical Subsetting** -- This is perhaps the most flexible method, but requires more explaining. It is described in Section \@ref(logsub). 

---

## Exercise 1A {-}

Take a break to apply what you've learned so far to enter the data found in Table `r if(is_html_output()) "\\@ref(tab:ex-1-table-html)" else "\\@ref(tab:ex-1-table-pdf)"` into R by hand and do some basic data subsets.

_The solutions to this exercise are found at the end of this book ([here](#ex1a-answers)). You are **strongly recommended** to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped._

```{r, echo = F}
Lake = c("Big", "Small", "Square", "Circle")
Area = c(100, 25, 45, 30)
Time = c(1000, 1200, 1400, 1600)
Fish = c(643, 203, 109, 15)
```

```{r ex-1-table-html, echo = F, eval = is_html_output()}
kable(data.frame(Lake, Area, Time, Fish), 
      caption = "The table you should enter in to R by hand for Exercise 1A") %>%
  kable_styling(full_width = F) %>%
  column_spec(1:4, width = "5em")
```

```{r ex-1-table-pdf, echo = F, eval = is_latex_output()}
kable(data.frame(Lake, Area, Time, Fish), "latex",
      caption = "The table you should enter in to R by hand for Exercise 1A")
```

1.	Create a new file in your working directory called `Ex_1A.R`.
2.	Enter these data into vectors. Call the vectors whatever you would like. Should you enter the data as vectors by rows, or by columns? (_Hint: remember the properties of vectors_).
3.	Combine your vectors into a data frame. Why should you use a data frame instead of a matrix?
4.	Subset all of the data from Small Lake.
5.	Subset the area for all of the lakes.
6.	Subset the number of fish for Big and Square lakes.
7.	You realize that you sampled 209 fish at Square Lake, not 109. Fix the mistake. There are two ways to do this, can you think of them both? Which do you think is better?
8.	Save your script. Close R and re-open your script to see that it was saved.

---

## Read External Data Files {#read}

It is rare that you will enter data by hand as you did in Exercise 1A. Often, you have a data set that you wish to analyze or manipulate that is stored in a spreadsheet. R has several ways to read information from data files and in this book, you will be using a common and simple method: reading in `.csv` files. `.csv` files are data files that separate columns with commas^[Note that if your computer is configured for a Spanish-speaking country, Microsoft Excel might convert decimals to commas. This can really mess with reading in data - I would suggest changing the language of Excel if you find this to be the case.]. If your data are in a Microsoft Excel spreadsheet, you can save your spreadsheet file as a `.csv` file (_File > Save As > Save as Type> CSV (Comma Delimited)_). Several dialog boxes will open asking if you are sure you want to save it as a `.csv` file. 

The syntax for reading in a `.csv` file is:

```{r, eval = F}
dat = read.csv("Path/To/FileName.csv")
```

Make sure your working directory is set to the location of your current file, which should be in the location `C:/Users/YOU/Documents/R-Book/Chapter 1`. According to the [instructions](#data-sets) on acquiring the data, you should have placed all of the data files for this book (downloaded from the [GitHub repository]()) in the location `C:/Users/YOU/Documents/R-Book/Data`. If you have done this already and your working directory is set to the location of the `Ch1.R` script, you can simply run:

```{r, eval = F}
dat = read.csv("../Data/streams.csv")
```

The `../` tells R to look up one directory from the working directory for a folder called `Data`, then within that, look for a file called `streams.csv`.

If you do not get an error, congratulations! However, if you get an error that looks like this:

```{r, echo = F, error = T}
dat = read.csv("streams.csv")
```

then fear not. This must be among the most common errors encountered by R users world-wide. It simply means the file you told R to look for doesn't exist where you told R to find it. Here is a trouble-shooting guide to this error:

1.  The exact case and spelling matters, as do the quotes and `.csv` at the end. Ensure the file name is typed correctly.
2.  Check what files are in the path you are specifying: run `dir("../Data")`. This will return a vector with the names of the files located in the `Data` directory (which is one folder up from your working directory). Is the file you told R was there truly in there? Is your working directory set to where you thought it was?

A simpler method is to put the data file in your working directory, in which case it can be read into R using `dat = read.csv("streams.csv")`. This method is not recommended for this book, because you will be using the same data files in multiple chapters, and it will help if they are all located in the same location.

If you did not get any errors, then the data are in the object you named (`dat`) and that object is a data frame. Do not proceed until you are able to get `read.csv` to run successfully.

**A few things to note about reading in `.csv` files**: 

*  R will assume the first row are column headers by default.
*  If there is a space in one of the header cells, a `"."` will be inserted. For example, the column header `Total Length` would become `Total.Length`. 
*  R brings in `.csv` files in as data frames by default.
*  If a record (i.e., cell) is truly missing and you want R to treat it that way (i.e., as an `NA`), you have three options:
    - Hard code an `NA` into that cell in your `.csv` file
    - Leave that cell completely empty in your `.csv` file
    - Enter in some other character (e.g., `"."`) alone in all cells that are meant to be coded as `NA` in R and use the `na.strings = "."` argument of `read.csv()`.
*  If at some point you did "Clear Contents" in Microsoft Excel to delete rows or columns from your `.csv` file, these "deleted" rows/columns will be read in as all `NAs`, which can be annoying. To remove this problem, open the `.csv` file in Excel, then highlight and **delete** the rows/columns in question and save the file. Read it back into R again using `read.csv()`.
*  If even a single character is found in a numeric column in `FileName.csv`, the _entire column_ will be coerced to a character/factor data class after it is read in (i.e., no more math with data on that column until you remove the character). A common error is to have a `#VALUE!` record left over from an invalid Excel function result. You must remove all of these occurrences in order to use that column as numeric. Characters include anything other than a number ([0-9]) and a period when used as a decimal. None of these characters: `!?[]\/@#$%^&*()<>_-+=[a-z];[A-Z]` should never be found in a column you wish to do math with (e.g., take the mean of that column). **This is an incredibly common problem!**

## Explore the Data Set {#data-summaries}

```{r, echo = F}
dat = read.csv("Data/streams.csv")
```

Have a look at the data. You could just run `dat` to view the contents of the object, but it will show the whole thing, which may be undesirable if the data set is large. To view the first handful of rows, run `head(dat)` or the last handful of rows with `tail(dat)`.

You will now use some basic functions to explore the simulated streams data before any analysis. The `summary()` function is very useful for getting a coarse look at how R has interpreted the data frame: 

```{r}
summary(dat)
```

You can see the spread of the numeric data and see the different levels of the factor (`state`) as well as how many records belong to each level. Note that there is one `NA` in the variable called `flow`.

To count the number of elements in a variable (or any vector), remember the `length()` function:

```{r}
length(dat$stream_width)
```

Note that R counts missing values as elements as well:

```{r}
length(dat$flow)
```

To get the dimensions of an object with more than one dimension (i.e., a data frame or matrix) you can use the `dim()` function. This returns a vector with two elements: the first number is the number of rows and the second is the number of columns. If you only want one of these, use the `nrow()` or `ncol()` functions (but remember, only for objects with more than one dimension; vectors don't have rows or columns!).

```{r}
dim(dat); nrow(dat); ncol(dat)
```

You can extract the names of the variables (i.e., columns) in the data frame using `colnames()`:

```{r}
colnames(dat)
```

Calculate the mean of all the `stream_width` records:

```{r}
mean(dat$stream_width)
```

Calculate the mean of all of the `flow` records:

```{r}
mean(dat$flow)
```

`mean()` returned an `NA` because there is an `NA` in the data for this variable. The way to tell R to ignore this `NA` is by including the argument `na.rm = TRUE` in the `mean()` function (separate arguments are always separated by commas). This is a **logical** argument, meaning that it asks a question. It says "do you want to remove `NAs` before calculating the mean?" `TRUE` means "yes" and `FALSE` means "no." `TRUE` and `FALSE` can be abbreviated as `T` and `F`, respectively. Many of R's functions have the `na.rm` argument (e.g. `mean()`, `sd()`, `var()`, `min()`, `max()`, `sum()`, etc. - most anything that collapses a vector into one number).  

```{r}
mean(dat$flow, na.rm = T)
```

which is the same as (i.e., the definition of the mean with the `NA` removed):

```{r}
sum(dat$flow, na.rm = T)/(nrow(dat) - 1)
```

What if you need to apply a function to more than one variable at a time? One of the easiest ways to do this (though as with most things in R, there are many) is by using the `apply()` function. This function applies the same summary function to individual subsets of a data object at a time then returns the individual summaries all at once:

```{r}
apply(dat[,c("stream_width", "flow")], 2, FUN = var, na.rm = T)
```

The first argument is the data object to which you want to apply the function. The second argument (the number `2`) specifies that you want to apply the function to columns, `1` would tell R to apply it to rows. The `FUN` argument specifies what function you wish to apply to each of the columns; here you are calculating the variance which takes the `na.rm = T` argument. This use of `apply()` alone is very powerful and can help you get around having to write the dreaded `for()` loop (introduced in Chapter \@ref(ch4)).

There is a whole family of `-apply()` functions, the base `apply()` is the most basic but a more sophisticated one is `tapply()`, which applies a function based on some grouping variable (a factor). Calculate the mean stream width **separated by state**:

```{r}
tapply(dat$stream_width, dat$state, mean)
```

The first argument is the variable to which you want to apply the `mean` function, the second is the grouping variable, and the third is what function you wish to apply. Try to commit this command to memory given this is a pretty common task.

If you want a data frame as output, you can use the `aggregate` function to do the same thing:

```{r}
aggregate(dat$stream_width, by = list(state = dat$state), mean)
```

## Logical/Boolean Operators

To be an efficient and capable programmer in any language, you will need to become familiar with how to implement numerical logic, i.e., the Boolean operators.  These are very useful because they always return a `TRUE` or a `FALSE`, off of which program-based decisions can be made (e.g., whether to operate a given subroutine, whether to keep certain rows, whether to print the output, etc.).

Define a simple object: `x = 5` `r x = 5`. Note that this will write over what was previously stored in the object `x`. Suppose you want to ask some questions of the new object `x` and have the answer printed to the console as a `TRUE` for "yes" and a `FALSE` for "no". Below are the common Boolean operators and their usage in R.

#### Equality {-}

To ask if `x` is exactly equal to 5, you run:

```{r}
x == 5
```

Note the use of the double `==` to denote equality as opposed to the single `=` as used in assignment (e.g., `x = 5`) or in argument specification (e.g., `mean(dat$flow, na.rm = T)`). 

#### Inequalities {-}

To ask if `x` is not equal to 5, you run:

```{r}
x != 5
```

The `!` is the logical **not** in R, and can be used to flip the direction of any `T` to a `F` or _vice versa_.

To ask if `x` is less than 5, you run:

```{r}
x < 5
```

To ask if `x` is less than _or equal to_ 5, you run:

```{r}
x <= 5
```

Greater than works the same way, though with the `>` symbol replaced.

#### In {-}

Suppose you want to ask which elements of one vector are also found within another vector:

```{r}
y = c(1,2,3)
x %in% y
# or
y = c(4,5,6)
x %in% y
# or
y %in% x
```

`%in%` is a very useful operator in R that is not one of the traditional Boolean operators.

#### And {-}

Suppose you have two conditions, and you want to know if **both are met**. For this you would use **and** by running:

```{r}
x > 4 & x < 6
```

which asks if `x` is between 4 and 6. 

#### Or {-}

Suppose you have two conditions, and you want to know if **either are met**. For this you would use **or** by running:

```{r}
x <= 5 | x > 5
```

which asks if `x` is less than or equal to 5 **or** greater than 5 - you would be hard-pressed to find a real number that did not meet these conditions!

## Logical Subsetting {#logsub}

A critical use of logical/Boolean operators is in the subsetting of data objects. You can use a logical vector (i.e., one made of only `TRUE` and `FALSE` elements) to tell R to extract only those elements corresponding to the `TRUE` records. For example:

```{r}
# here's logical vector: TRUE everywhere condition met
dat$stream_width > 60

# insert it to see only the flows for the TRUE elmements
dat$flow[dat$stream_width > 60]
```

gives all of the `flow` values for which `stream_width` is greater than 60.

To extract all of the data from Alabama, you would run:

```{r}
dat[dat$state == "Alabama",]
```

To exclude all of the data from Alabama, you would run:

```{r, eval = F}
dat[dat$state != "Alabama",]
```

You will be frequently revisiting this skill throughout the workshop.

## `if()`, `else`, and `ifelse()`

You can tell R to do something if the result of a question is `TRUE`. This is a typical if-then statement:

```{r}
if (x == 5) print("x is equal to 5")
```

This says "if `x` equals 5, then print the phrase 'x is equal to 5' to the console". If the logical returns a `FALSE`, then this command does nothing. To see this, change the `==` to a `!=` and re-run:

```{r}
if (x != 5) print("x is equal to 5")
```

Notice that because `x` does equal 5, running this line has no effect.

You can tell R to do multiple things if the logical is `TRUE` by using curly braces:

```{r}
if (x == 5) {
  print("x is equal to 5")
  print("you dummy, x is supposed to be 6")
}
```

You can always use curly braces to extend code across multiple lines whereas it may have been intended to go on one line.
 
If you want R to do something if the logical is `FALSE`, you would use the `else` command:

```{r}
if (x > 5) print("x is greater than 5") else print("x is not greater than 5")
```

Or extend this same thing to multiple lines:

```{r, eval = F}
if (x > 5) {
  print("x is greater than 5")
} else {
  print("x is not greater than 5")
} 
```

The `if()` function is useful, but it can only respond to one question at a time. If you supply it with a vector of length greater than 1, it will give a warning:

```{r}
# vector from -5 to 5, excluding zero
xs = c(-5:-1, 1:5)

# attempt a logical decision
if (xs < 0) print("negative") else print("positive")
```

Warnings are different than errors in that something still happens, but it tells you that it might not be what you wanted, whereas an error stops R altogether. In short, this warning is telling you that you passed `if()` a logical vector with more than 1 element, and that it can only use one element so it's picking the first one. Because the first element of `xs` is -5, `xs < 0` evaluated to `TRUE`, and you got a `"negative"` printed along with the warning.

To respond to multiple questions at once, you must use `ifelse()`. This function is similar, but it combines the `if()` and `else` syntax into one useful function function:

```{r}
ifelse(xs > 0, "positive", "negative")
```

The syntax is `ifelse(condition, do_if_TRUE, do_if_FALSE)`. You can `cbind()` the output with `xs` to verify it worked:

```{r}
cbind(
  xs,
  ifelse(xs > 0, "positive", "negative")
)
```

Use `ifelse()` to create a new variable in `dat` that indicates whether a stream is big or small depending on whether `stream_width` is greater or less than 50:

```{r}
dat$size_cat = ifelse(dat$stream_width > 50, "big", "small"); head(dat)
```

This says "make a new variable in the data frame `dat` called `size_cat` and assign each row a 'big' if `stream_width` is greater than 50 and a 'small' if less than 50". 

One neat thing about `ifelse()` is that you can nest multiple statements inside another^[You can nest **ALL** R functions, by the way.]. What if you wanted three categories: 'small', 'medium', and 'large'?

```{r}
dat$size_cat_fine = ifelse(dat$stream_width <= 40, "small",
                           ifelse(dat$stream_width > 40 & dat$stream_width <= 70, "medium", "big")); head(dat)
```

If the first condition is `TRUE`, then it will give that row a "small". If not, it will start another `ifelse()` to ask if the `stream_width` is greater than 40 _and_ less than or equal to 70. If so, it will give it a "medium", if not it will get a "big". Not all function nesting examples are this complex, but this is a neat example.  Without `ifelse()`, you would have to use as many `if()` statements as there are elements in `dat$stream_width`.

## Writing Output Files

### `.csv` Files
Now that you have made some new variables in your data frame, you may want to save this work in the form of a new `.csv` file. To do this, you can use the `write.csv` function:

```{r, eval = F}
write.csv(dat, "updated_streams.csv", row.names = F)
```

The first argument is the data frame (or matrix) to write, the second is what you want to call it (don't forget the `.csv`!), and `row.names = F` tells R to not include the row names (because they are just numbers in this case). R puts the file in your working directory unless you tell it otherwise. To put it somewhere else, type in the path with the new file name at the end (e.g., `C:/Users/YOU/Documents/R-Book/Data/updated_streams.csv`. 

### Saving R Objects

If all you care about is the data frame `dat` as interpreted by R (not a share-able file like the `.csv` method), then you can save the object `dat` (in its current state) then load it in to a future R session. You can save the new data frame using:

```{r, eval = F}
save(dat, file = "updated_streams")
```

Then try removing the `dat` object from your current session (`rm(dat)`) and loading it back in using:

```{r, eval = F}
rm(dat); head(dat)  # should give error
load(file = "updated_streams")
head(dat) # should show first 6 rows
```

## User-Defined Functions {#user-funcs}

Sometimes you may want R to carry out a specific task, but there is no built-in function to do it. In these cases, you can write your own functions. Function writing makes R incredibly flexible, though you will only get a small taste of this topic here. You will see more examples in later Chapters, particularly in Chapter \@ref(ch4).

First, you must think of a name for your function (e.g., `myfun`). Then, you specify that you want the object `myfun()` to be a function by using using the `function()` function. Then, in parentheses, you specify any arguments that you want to use within the function to carry out the specific task. Open and closed curly braces specify the start and end of your function body, i.e., the code that specifies how it uses the arguments to do its job.

Here's the general syntax for specifying your own function:

```{r}
myfun = function(arg1) {
  # function body goes here
    # use arg1 to do something
  
  # return something as last step
}
```

As an example, write a general function to take any number `x` to any power `y`:

```{r}
power = function(x, y){
  x^y
}

```

After typing and running the function code (`power()` is an object that must be assigned), try using it:

```{r}
power(x = 5, y = 3)
```

Remember, you can nest or embed functions:
```{r}
power(power(5,2),2)
```

This is the equivalent of $(5^2)^2$.

---

## Exercise 1B {-#ex1b}

In this exercise, you will be using what you learned in Chapter 1 to summarize data from a hypothetical pond experiment. 

Pretend that you added nutrients to mesocosoms and counted the densities of four different zooplankton taxa. In this experiment, there were two ponds, two treatments per pond, and five replicates of each treatment. The data are located in the data file `ponds.csv` (see the [instructions](#data-sets) on acquiring and organizing the data files for this book. There is one error in the data set. After you download the data and place it in the appropriate directory, make sure you open this file and fix it _before_ you bring it into R. Refer back to the information about reading in data (Section \@ref(read)) to make sure you find the error.

Create a new R script in your working directory for this chapter called `Ex1B.R` and use that script to complete this exercise.

_The solutions to this exercise are found at the end of this book ([here](#ex1b-answers)). You are **strongly recommended** to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped._

1.  Read in the data to R and assign it to an object.
2.	Calculate some basic summary statistics of your data using the `summary()` function.
3.	Calculate the mean chlorophyll _a_ for each pond (_Hint: pond is a grouping variable_)
4.	Calculate the mean number of _Chaoborus_ for each treatment in each pond using `tapply()`. (_Hint: You can group by two variables with:_ `tapply(dat$var, list(dat$grp1, dat$grp2), fun)`.
5.	Use the more general `apply()` function to calculate the variance for each zooplankton taxa found only in pond S-28.
6.	Create a new variable called `prod` in the data frame that represents the quantity of chlorophyll _a_ in each replicate. If the chlorophyll _a_ in the replicate is greater than 30 give it a "high", otherwise give it a "low". (_Hint: are you asking R to respond to one question or multiple questions? How should this change the strategy you use?_)

### Exercise 1B Bonus {-}
1.  Use `?table` to figure out how you can use `table()` to count how many observations of high and low there were in each treatment (_Hint: `table` will have only two arguments._).
2.	Create a new function called `product()` that multiplies any two numbers you specify.
3.	Modify your function to print a message to the console and return the value `if()` it meets a condition and to print another message and not return the value if it doesn't.

<!--chapter:end:01-intro-to-R.Rmd-->

# Base R Plotting Basics {#ch2}

```{r include = F}
rm(list = ls(all = T))
knitr::opts_chunk$set(fig.align = "center",
                      global.par = T)

op = par(no.readonly = T)
par(mar = c(4,4,2,1))
sp = par(no.readonly = T)
```

```{r, echo = F, warning = F, message = F}
library(dplyr)
library(kableExtra)
library(knitr)
```

## Chapter Overview{-}

In this chapter, you will get familiar with the basics of using R for making plots and figures. You will learn:

*  how to make various plots including:
    *  scatterplots
    *  line plots
    *  bar plots
    *  box-and-whisker plots
    *  histograms
*  the basics of how to change plot features like:
    *  the text displayed on axes labels
    *  the size and type of point symbols
    *  the color of features
*  the basics of multi-panel plotting
*  the basics of the `par()` function
*  how to save your plot to an external file

R's base `{graphics}` plotting package is incredibly versatile, and as you will see, it doesn't take much to get started making professional-looking graphs. It is worth mentioning that there are other R packages^[An **R package** is a bunch of code that somebody has written and placed on the web for you to install, their use is first introduced in Chapter \@ref(ch5)] for plotting that have nice features. The R packages `{ggplot2}` [@R-ggplot2] and `{lattice}` [@R-lattice] are good examples. However, they can be more complex to learn at first than the base R plotting capabilities and look a bit different.

**IMPORTANT NOTE**: If you did not attend the sessions corresponding to Chapter \@ref(ch1), you are recommended to walk through the material found in that chapter before proceeding to this material. Also note that if you are confused about a topic, you can use **CTRL + F** to find previous cases where that topic has been discussed in this book. 

## Before You Begin {-}

You should create a new directory and R script for your work in this Chapter. Create a new R script called `Ch2.R` and save it in the directory `C:/Users/YOU/Documents/R-Book/Chapter2`. Set your working directory to that location. Revisit Sections \@ref(scripts) and \@ref(working-dir) for more details on these steps.  

## R Plotting Lingo

Learning some terminology will help you get used to the base R graphics system:

*  A **high-level plotting function** is one that is used to make a new graph. Examples of higher level plotting functions are the general `plot()` and the `barplot()` functions. When-a high level plotting function is executed, the currently-displayed plot (if any) is written over.
*  A **low-level plotting function** is one that is used to modify a plot that has already been created. You must already have made a plot using a high-level plotting function before you use low-level plotting functions. Examples include `text()`, `points()`, and `lines()`. When a low-level plotting function is executed, the output is simply added to an existing plot. 
*  The **graphics device** is the area in which a plot is displayed. RStudio has a built in graphics device in its interface (lower right by default in the "Plots" tab), or you can create a new device. R will plot on the active device, which is the most recently created device. There can only be one active device at a time.

Here is an example of a basic R plot:

```{r, echo = F}
par(sp)
x = 0:10
y = x^2
plot(x = x, y = y,
     xlab = "x-axis label goes here",
     ylab = "y-axis label goes here",
     main = "Main Plot Title Goes Here")
```

There are a few components: 

*  The **plotting region**: all data information is displayed here.
*  The **margin**: where axis labels, tick marks, and main plot titles are located
*  The **outer margin**: by default, there is no outer margin. You can add one if you want to add text here or make more room around the edges.

You can change just about everything there is about this plot to suit your tastes. Duplicate this plot, but make the x-axis, y-axis, and main titles something other than the placeholders shown here:

```{r, eval = F}
# plot dummy data
x = 0:10
y = x^2
plot(x = x, y = y,
     xlab = "x-axis label goes here", 
     ylab = "y-axis label goes here",
     main = "Main Plot Title Goes Here")
```

Note that the first two arguments, `x` and `y`, specify the coordinates of the points (i.e., the first point is placed at coordinates `x[1]`,`y[1]`). `plot()` has _tons_ of arguments (or _graphical parameters_ as the help file found using `?plot` or `?par` calls them) that change how the plot looks. Note that when you want something displayed verbatim on the plotting device, you must wrap that code in `" "`, i.e., the arguments `xlab`, `ylab`, and `main` all receive a character vector of length 1 as input. 

Table `r if(is_html_output()) "\\@ref(tab:plot-arg-table-html)" else "\\@ref(tab:plot-arg-table-pdf)"` shows information on just a handful of the plotting settings to get you started.

```{r plot-arg-table-html, echo = F, results = "asis", eval = is_html_output()}
pars = c("`xlab`", "`ylab`", "`main`", "`cex`", "`pch`", "`xlim`", "`ylim`", "`type`",
         "`lty`", "`lwd`", "`col`")

usage = c("`xlab = 'X-AXIS'`", "`ylab = 'Y-AXIS'`",
          "`main = 'TITLE'`", '`cex = 1.5`', '`pch = 17`', 
          "`xlim = range(x)`", '`ylim = c(0,1)`',
          "`type = 'l'`", '`lty = 2`', '`lwd = 2`', "`col = 'blue'`")

desc = c("changes the x-axis label text",
         "changes the y-axis label text",
         "changes the main title text",
         "changes the size of symbols in the plotting region^[`cex` is a multiplier: `cex = 1.5` says make the points 1.5 times as large as they would be by default.]",
         "changes the symbol type^[There are approximately 20 different `pch` settings: `pch = 1` is empty circles, `pch = 16` is filled circles, etc.]",
         "changes the endpoints (limits) of the x-axis^[`xlim` and `ylim` both require a numeric vector of length 2 where neither of the elements may be an `NA`.]",
         "same as `xlim`, but for the y-axis",
         "changes the way points are connected by lines^[The default is points only, `type = 'l'` is for lines only, `type = 'o'` is for points connected with lines, and `type = 'b'` is for points and lines but with a small amount of separation between them.]",
         "changes the line type^[`lty = 1` is solid, `lty = 2` is dashed, `lty = 3` is dotted, etc. You can also specific it like `lty = 'solid'`, `lty = 'dotted'`, or `lty = 'dotdash'`.]", 
         "changes the line width^[works just like `cex`: `lwd = 3` codes for a line that is 3 times as thick as it would normally be]",
         "changes the color of plotted objects^[there is a whole host of colors you can pass R by name, run `colors()` to see for yourself]")
df = data.frame(Arg. = pars, Usage = usage, Description = desc)
# pander::pandoc.table(df, justify = "lll", split.cells = c("10%", "30%", "60%"))
knitr::kable(df,
             caption = "Several of the key arguments to high- and low-level plotting functions")
```

```{r plot-arg-table-pdf, echo = F, eval = is_latex_output()}
pars = c("xlab", "ylab", "main", "cex", "pch", "xlim", "ylim", "type",
         "lty", "lwd", "col")

usage = c('xlab = "X-AXIS"', 'ylab = "Y-AXIS"',
          'main = "TITLE"', 'cex = 1.5', 'pch = 17', 
          'xlim = range(x)', 'ylim = c(0,1)',
          'type = "l"', 'lty = 2', 'lwd = 2', 'col = "blue"')

desc = c("changes the x-axis label text",
         "changes the y-axis label text",
         "changes the main title text",
         "changes the size of symbols in the plotting region",
         "changes the symbol type",
         "changes the endpoints (limits) of the x-axis",
         "same as xlim, but for the y-axis",
         "changes the way points are connected by lines",
         'changes the line type', 
         "changes the line width",
         "changes the color of plotted objects")
df = data.frame(Argument = pars, Usage = usage, Description = desc)
# pander::pandoc.table(df, justify = "lll", split.cells = c("10%", "30%", "60%"))

kable(
  df, "latex", booktabs = T,
  caption = "Several of the key arguments to high- and low-level plotting functions",
  escape = F) %>%
  footnote(
    alphabet = c(
      "cex is a multiplier: cex = 1.5 says make the points 1.5 times as large as they would be by default.",
      "There are approximately 20 different pch settings: pch = 1 is empty circles, pch = 16 is filled circles, etc.",
      "xlim and ylim both require a numeric vector of length 2 where neither of the elements may be an NA.",
      'The default is points only, type = "l" is for lines only, type = "o" is for points connected with lines, and type = "b" is for points and lines but with a small amount of separation between them.',
      'lty = 1 is solid, lty = 2 is dashed, lty = 3 is dotted, etc. You can also specific it like lty = "solid", lty = "dotted", or lty = "dotdash".',
      "works just like cex: lwd = 3 codes for a line that is 3 times as thick as it would normally be",
      "there is a whole host of colors you can pass R by name, run colors() to see for yourself"
    ),
    footnote_as_chunk = F, threeparttable = T
  ) %>%
  kable_styling(full_width = F) %>%
  column_spec(3, width = "25em") %>%
  column_spec(1:2, monospace = T)
```

You are advised to try at least some of the arguments in Table `r if(is_html_output()) "\\@ref(tab:plot-arg-table-html)" else "\\@ref(tab:plot-arg-table-pdf)"` out for yourself with your `plot(x, y)` code from above - notice how every time you run `plot()`, a whole new plot is created, not just the thing you changed. There are definitely other options: check out `?plot` or `?par` for more details. 

## Lower Level Plotting Functions

Now that you have a base plot designed to your liking, you might want to add some additional "layers" to it to represent more data or other kind of information like an additional label or text. Add some more points to your plot by putting this line right beneath your `plot(x,y)` code and run just the `points()` line (make sure your device is showing a plot first): 

```{r, eval = F}
# rev() reverses a vector: so the old x[1] is x[11] now
points(x = rev(x), y = y, col = "blue", pch = 16)
```

```{r, echo = F}
par(sp)
plot(x = x, y = y, xlab = "x-axis label goes here", ylab = "y-axis label goes here",
      main = "Main Plot Title Goes Here")
points(x = rev(x), y = y, col = "blue", pch = 16)
```

Here, `points()` acted like a low-level plotting function because it added points to a plot you already made. Many of the arguments shown in Table `r if(is_html_output()) "\\@ref(tab:plot-arg-table-html)" else "\\@ref(tab:plot-arg-table-pdf)"` can be used in both high-level and low-level plotting functions (notice how `col` and `pch` were used in `points()`). Just like `points()`, there is also `lines()`:

```{r, eval = F}
lines(x = x, y = x, lty = 2, col = "red")
```

```{r, eval = T, echo = F}
par(sp)
plot(x = x, y = y, xlab = "x-axis label goes here",
     ylab = "y-axis label goes here", main = "Main Plot Title Goes Here")
points(x = rev(x), y = y, col = "blue", pch = 16)
lines(x = x, y = y, lty = 2, col = "red")
```

You can add text to the plotting region:

```{r, eval = F}
text(x = 5, y = 80, "This is Text", cex = 1.5, col = "grey", font = 4)
```

```{r, eval = T, echo = F}
par(sp)
plot(x = x, y = y, xlab = "x-axis label goes here", ylab = "y-axis label goes here",
      main = "Main Plot Title Goes Here")
points(x = rev(x), y = y, col = "blue", pch = 16)
lines(x = x, y = y, lty = 2, col = "red")
text(x = 5, y = 80, "This is Text", cex = 1.5, col = "grey", font = 4)
```

The text is centered on the coordinates you provide. You can also provide vectors of coordinates and text to write different things at once.

The easiest way to add a straight line to a plot is with `abline()`. By default it takes two arguments: `a` and `b` which are the y-intercept and slope, respectively, e.g., `abline(0,1)` will draw a 1:1 line, as will `abline(c(0,1))`. You can also do `abline(h = 5)` to draw a horizontal line at 5 or `abline(v = 5)` to draw a vertical line at `5`.

You can see that the text is centered on the coordinates `x = 5` and `y = 80` using `abline()`:

```{r, eval = F}
abline(h = 80, col = "grey")
abline(v = 5, col = "grey")
```

```{r, eval = T, echo = F}
par(sp)
plot(x = x, y = y, xlab = "x-axis label goes here", ylab = "y-axis label goes here",
      main = "Main Plot Title Goes Here")
points(x = rev(x), y = y, col = "blue", pch = 16)
lines(x = x, y = y, lty = 2, col = "red")
text(x = 5, y = 80, "This is Text", cex = 1.5, col = "grey", font = 4)
abline(h = 80, col = "grey"); abline(v = 5, col = "grey")
```

If you accidentally add a plot element that you don't want using a low-level plotting function, the only way to remove it is by re-running the high-level plotting function to start a new plot and adding only the objects you want. Try removing the "This is Text" text and the straight lines you drew with `abline()` from the plot displayed in your device.

## Other High-Level Plotting Functions

You have just seen the basics of making two-dimensional scatter plots and line plots. You will now explore other types of graphs you can make. 

### The Bar Graph
Another very common graph is a bar graph.  R has a `barplot()` function, and again, it has lots of arguments. Here you will just make two common variations: single bars per group and multiple bars per group. Create a vector and plot it:

```{r, eval = F}
x1 = c(2,4,6)
barplot(x1)
```

```{r, echo = F}
par(sp)
x1 = c(2,4,6)
barplot(x1)
```

Notice that there are no group names on the bars (if `x1` had names, there would be). You can add names by using the argument `names.arg`:

```{r, eval = F}
barplot(x1, names.arg = c("a", "b", "c"))
```

Add some more information by including two bars per group. Create another vector and combine it with the old data:

```{r, eval = F}
x2 = c(3,5,7)
x3 = rbind(x1, x2)
barplot(x3, names.arg = c("a", "b", "c"), beside = T)
```

```{r, echo = F}
par(sp)
x2 = c(3,5,7)
x3 = rbind(x1, x2)
barplot(x3, names.arg = c("a", "b", "c"), beside = T)
```

To add multiple bars per group, R needs a matrix like you just made.  The **columns** store the heights of the bars that will be placed together in a group. Including the `beside = T` argument tells R to plot all groups as different bars as opposed to using a stacked bar graph.

Oftentimes, you will want to add error bars to a bar graph like this. To avoid digressing too much here, creating error bars is covered as a bonus topic (Section \@ref(error-bars)).

## Box-and-Whisker Plots {#box-whisker}

Box-and-whisker plots are a great way to visualize the spread of your data. All you need to make a box-and-whisker plot is a grouping variable (a factor, revisit Section \@ref(factors) if you don't remember what these are) and some continuous (i.e., numeric) data for each level of the factor. You will be using the `creel.csv` data set (see the [instructions](#data-sets) on acquiring and placing the data files in the appropriate location).

Read the data in and print a summary:

```{r, eval = F}
dat = read.csv("../Data/creel.csv")
summary(dat)
```

```{r, echo = F}
dat = read.csv("Data/creel.csv")
summary(dat)
```

This data set contains some simulated (i.e., fake) continuous and categorical data that represent 300 anglers who were creel surveyed^[A creel survey is a sampling program where fishers are asked questions about their fishing behavior in order to estimate effort and harvest.]. In the data set, there are three categories (levels to the factor `fishery`) and the continuous variable is how many hours each angler fished this year. If you supply the generic `plot()` function with a continuous response (`y`) variable and a categorical predictor (`x`) variable, it will automatically assume you want to make a box-and-whisker plot:

```{r, eval = F}
plot(x = dat$fishery, y = dat$hours)
```

```{r, echo = F}
par(sp)
plot(x = dat$fishery, y = dat$hours)
```

In the box-and-whisker plot above, the heavy line is the median, the ends of the boxes are the 25^th^ and 75^th^ percentiles and the "whiskers" are the 2.5^th^ and 97.5^th^ percentiles. Any points that are outliers (i.e., fall outside of the whiskers) will be shown as points^[Outliers can be turned off using the `outline = F` argument to the `plot()` function].

It is worth introducing a shorthand syntax of typing the same command:

```{r, eval = F}
plot(hours ~ fishery, data = dat)
```

Instead of saying `plot(x = x.var, y = y.var)`, this expression says `plot(y.var ~ x.var)`. The `~` reads "as a function of". By specifying the `data` argument, you no longer need to indicate where the variables `hours` and `fishery` are found. Many R functions have a `data` argument that works this same way. It is sometimes preferable to plot variables with this syntax because it is often less code and is also the format of R's statistical equations^[Which allows you to easily copy and paste the code between the model and plot functions, see Chapter \@ref(ch3)]. 

## Histograms

Another way to show the distribution of a variable is with histograms. These figures show the relative frequencies of observations in different discrete bins. Make a histogram for the hours the surveyed tournament anglers fished this year:

```{r, eval = F}
hist(dat$hours[dat$fishery == "Tournament"])
```

```{r, echo = F}
par(sp)
hist(dat$hours[dat$fishery == "Tournament"])
```

Notice the subset that extracts hours fished for tournament anglers only before plotting.

`hist()` automatically selects the number of bins based on the range and resolution of the data. You can specify how many evenly-sized bins you want to plot:

```{r, eval = F}
# extract the hours for tournament anglers
t_hrs = dat$hours[dat$fishery == "Tournament"]

# create the bin endpoints
nbins = 20
breaks = seq(from = min(t_hrs), to = max(t_hrs), length = nbins + 1)
hist(t_hrs, breaks = breaks, main = "Tournament", col = "grey")
```

```{r, echo = F}
# extract the hours for tournament anglers
t_hrs = dat$hours[dat$fishery == "Tournament"]

# create the bin endpoints
nbins = 20
breaks = seq(from = min(t_hrs), to = max(t_hrs), length = nbins + 1)
par(sp)
hist(t_hrs, breaks = breaks, main = "Tournament", col = "grey")
```

## The `par()` Function

If it bothers you that the axes are "floating", you can fix this using this command:

```{r}
par(xaxs = "i", yaxs = "i", mar = c(4,4,2,1))
hist(t_hrs, breaks = breaks, main = "Tournament", col = "grey")
```

Here, you changed the graphical parameters of the graphics device by using the `par()` function. Once you change the settings in `par()`, they will remain that way until you start a new device.

The `par()` function is central to fine-tuning your graphics. Here, the `xaxs = "i"` and `yaxs = "i"` arguments essentially removed the buffer between the data and the axes. `par()` has options to change the size of the margins, add outer margins, change colors, etc. Some of the graphical parameters that can be passed to high- and low-level plotting functions (like those in Table `r if(is_html_output()) "\\@ref(tab:plot-arg-table-html)" else "\\@ref(tab:plot-arg-table-pdf)"`) can also be passed `par()`. Check out the help file (`?par`) to see everything it can do. If you want to start over with fresh `par()` settings, start a new device.

The `mar` argument controls how many lines of margin text are on each side of the plotting region: `mar[1]` is the bottom margin, `mar[2]` is left, `mar[3]` is top, and `mar[4]` is right. There is also `par(oma = c())` for outer margins. You can specify these in inch units instead of margin lines using the `mai` or `omi` arguments.

## New Temporary Devices

If you are using RStudio, then likely all of the plots you have made thus far have shown up in the lower right-hand corner or your RStudio window. You have been using RStudio's built-in plotting device. If you want to open a new plotting device (maybe to put it on a separate monitor), you can use the following commands, depending on your operating system:

*  **Windows Users** -- just run `windows()` to open up a new plotting device. It will become the active device.
*  **Mac Users** -- similarly, you can run `quartz()` to open a new device.
*  **Linux Users** -- similarly, just run `x11()`. 

## Multi-panel Plots 

Sometimes you want to display more than one plot at a time. You can make a multi-panel plot which allows for multiple plotting regions to show up simultaneously within the same plotting device. First, you need to change the layout of the plotting region. The easiest way to set up the device for multi-panel plotting is by using the `mfrow` argument in the `par()` function.

Below, the code says "set up the graphical parameters so that there is 1 row and 3 columns of plotting regions within the device". Every time you make a new plot, it will go in the next available plotting region (regions fill from left to right, and from top to bottom; use `mfcol` if you wish to fill columns before rows). Make 3 histograms, each that represents a different sector of the fishery:

```{r, eval = T, fig.height = 3, results = "hide"}
par(mfrow = c(1,3), mar = c(4,1,2,1), oma = c(0,3,0,0))
sapply(levels(dat$fishery), function(f) {
  hist(dat$hours[dat$fishery == f], main = f, xlab = "")
})
mtext(side = 2, outer = T, line = 1.5, "Frequency", cex = 0.8)
```

Here, `sapply()` applied a user-defined function that plots a histogram for a given fishery type to each of the fishery types separately^[`sapply()` works like `apply()`, except on vectors only so you don't need to supply it with a `1` or `2` for rows or columns], which allowed you to only need to type the `hist()` code once. Notice the `par(oma)` setting to add outer margins on the left and the `mtext()` function to add a common y-axis label to all the figures.

There are other ways to make multi-panel plots, however, they are beyond the scope of this introductory material. See `?layout` for details. With this function you can change the size of certain plots and make them have different shapes (i.e., some squares, some rectangles, etc.), but it takes some pretty involved (though not impossible, by any means) specification of how you want the device to be split up into regions.

## Legends

Oftentimes you will want to add a legend to a plot to help people interpret what it is showing. You can add legends to R plots using the low-level plotting function `legend()`. Add a legend to the bar plot you made earlier with two groups. First, re-make the plot by running the high-level `barplot()` function, but change the colors of the bars to be shades of blue and red. Once you have the plot made, add the legend:

```{r}
par(mar = c(4,4,2,1))
barplot(x3, beside = T, 
        names.arg = c("a", "b", "c"),
        col = c("skyblue2", "tomato2"))
legend("topleft", legend = c("Group 1", "Group 2"),
       fill = c("skyblue2", "tomato2"))
```

The box can be removed using the `bty = "n"` argument and the size can be changed using `cex`. The position can be specified either with words (like above) or by using x-y coordinates.

Here is a more complex example:

```{r}
# 1) extract and sort the hours for two fisheries from fewest to most hours fished
t_hrs = sort(dat$hours[dat$fishery == "Tournament"])
n_hrs = sort(dat$hours[dat$fishery == "Non.Tournament"])

# 2) make the plot: plot for t_hrs only, but ensure xlim covers both groups
# set the margins: 
  # 4 lines of margin space on bottom and left,
  # 1 on top and right
par(mar = c(4,4,1,1))  
plot(x = t_hrs, y = 1:length(t_hrs),
     type = "o", lty = 2, xlim = range(c(t_hrs, n_hrs)),
     xlab = "Hours Fished/Year", ylab = "Rank within Fishery Samples",
     las = 1)  # las = 1 says "turn the y-axis tick labels to be horizontal"
# 3) add info for the other fishery
points(x = n_hrs, y = 1:length(n_hrs), type = "o", lty = 1, pch = 16)
# 4) add the legend
legend("topleft", legend = c("Tournament", "Non-Tournament"), 
       lty = c(2,1), pch = c(1, 16), bty = "n")
```

Notice that you need to be careful about the order of how you specify which `lty` and `pch` settings match up with the elements of the `legend` argument. In the `plot()` code, you specified that the `lty = 2` but didn't specify what `pch` should be (it defaults to `1`). So when you put the "Tournament" group first in the `legend` argument vector, you must be sure to use the corresponding plotting codes. The first element of the `lty` argument matches up with the first element of `legend`, and so on. Note the other new plotting trick used in the code above: the rotation of y-axis tick mark labels using `las = 1`.

## Exporting Plots

There are two main ways to save static permanent copies of your R plots.  The first is a quick-and-dirty point-and-click method that saves the plots in low-resolution. Because you have to point and click, this cannot be automated. The second method produces cleaner-looking high-resolution plots with code that can be embedded in your script, ensuring the same exact plot will be created each time the code is executed.

### Click Save

*  If your plot is in the RStudio built-in graphics device: Right above the plot, click _Export > Save as Image_.  Change the name, dimensions and file type.
*  If your plot is in a plotting device window (opened with `windows()` or `quartz()`: Simply go to _File > Save_. 

All plots will be saved in the working directory by default. You can also just copy the plot to your clipboard (_File > Copy to the clipboard > bitmap_) and paste it where you want. You should save one of the plots you made in this Chapter using this approach.

### Use a function to place plot in a new file {#file-devices}

If you are producing plots for a final report or publication, you want the output to be as clean-looking as possible and you want them to be fully reproducible so when something changes with your data or analysis in review, you can reproduce the same figure with the new results. You can save high-resolution plots using the following steps:

```{r, eval = F}
# step 1: Make a pixels per inch object
ppi = 600

# step 2: Call the figure file creation function
png("TestFigure.png", h = 8 * ppi, w = 8 * ppi, res = ppi)

# step 3: Run the plot 
# put all of your plotting code here (without windows())

# step 4: Close the device
dev.off()
```

A file will be saved in your working directory containing the plot made by the code in step 3 above. The `ppi` object is pixels-per-inch. When you specify `h = 8 * ppi`, you are saying "make a plot with height equal to 8 inches". There are similar functions to make PDFs, tiff files, jpegs, etc. If you use `pdf()`, simply specify `h` and `w` without the `ppi` and no `res` argument. You should save one of the plots you made in this Chapter using this approach. 

## Bonus Topic: Error Bars {#error-bars}

Rarely should you ever present estimates without some measure of uncertainty. The most common way for visualizing the uncertainty in an estimate is by using error bars, which can be added to an R plot using the lower-level function `arrows()`. To use `arrows()`, you need:

*  Vectors of the `x` and `y` coordinates of the lower bound of the error bars
*  Vectors of the `x` and `y` coordinates of the upper bound of the error bars

The syntax for `arrows()` is as follows: `arrows(x0, y0, x1, y1, ...)`, where `x0` and `y0` are the coordinates you are drawing "from" (e.g., lower limits) and the `x1` and `y1` are the coordinates you are drawing "to" (e.g., upper limits). The `...` represents other arguments to change how the error bars look. Calculate the means of the hours fished in each fishery sector and plot them. (If you don't remember how `tapply()` works, revisit Section \@ref(data-summaries)). 

```{r}
x_bar = tapply(dat$hours, dat$fishery, mean)
par(mar = c(4,4,1,1))
barplot(x_bar)
```

Say you want to add error bars that represent 95% confidence intervals on the mean. You can create a 95% confidence interval using this basic formula:

\begin{equation}
  \bar{x} \pm 1.96 * SE(\bar{x}),
(\#eq:ci)
\end{equation}

where

\begin{equation}
  \bar{x}=\frac{1}{n}\sum_i^n{x_i},
(\#eq:ci-mean)
\end{equation}

and

\begin{equation}
  SE(\bar{x})=\frac{\sqrt{\frac{\sum_i^n{(x_i - \bar{x})^2}}{n-1}}}{\sqrt{n}}
(\#eq:ci-se)
\end{equation}

Begin by creating a function to calculate the standard error ($SE(\bar{x})$):

```{r}
calc_se = function(x) {
  sqrt(sum((x - mean(x))^2)/(length(x)-1))/sqrt(length(x))
}
```

Then calculate the standard errors for each fishery sector:

```{r}
se = tapply(dat$hours, dat$fishery, calc_se)
```

Then calculate the lower and upper limits of your error bars:

```{r}
lwr = x_bar - 1.96 * se
upr = x_bar + 1.96 * se
```

Then draw them on using the `arrows()` function:

```{r}
par(mar = c(4,4,1,1))
mp = barplot(x_bar, ylim = range(c(0, upr)))
arrows(x0 = mp, y0 = lwr, x1 = mp, y1 = upr, length = 0.1, angle = 90, code = 3)
```

Notice four things:

1.  The use of `mp` to specify the `x` coordinates. If you do `mp = barplot(...)`, `mp` will contain the `x` coordinates of the midpoint of each bar.
2.  `x0` and `x1` are the same: you want to have vertical bars, so these must be the same while `y1` and `y2` differ.
3.  The use of `ylim = range(c(0, upr))`: you want the y-axis to show the full range of all the error bars.
4.  The three arguments at the end of `arrows()`:
    - `length = 0.1`: the length of the arrow heads, fiddle with this until you like it.
    - `angle = 90`: the angle of the arrow heads, you want 90 here for the error bars.
    - `code = 3`: indicates that arrow heads should be drawn on both ends of the arrow.

---

## Exercise 2 {-#ex2}

For this exercise, you will be making a few plots and changing how they look to suit your taste. You will use a real data set (`sockeye.csv`, see the [instructions](#data-sets) on how to acquire the data files) from a sockeye salmon (_Oncorhynchus nerka_) population from the Columbia/Snake River system, obtained from @sockeye-cite. This population spawns in Redfish Lake in Idaho, which feeds into the Salmon River, which is a tributary of the Snake River. In order to reach the lake, the sockeye salmon must successfully pass through a total of eight dams that have fish passage mechanisms in place. The Redfish Lake population is one of the most endangered sockeye populations in the U.S. and travels farther (1,448 km), higher (1,996 m), and is the southernmost breeding population of all sockeye populations in the world [@sockeye-cite]. Given this uniqueness, a captive breeding program was initiated in 1991 to conserve the genes from this population. These data came from both hatchery-raised and wild fish and include average female spawner weight (g), fecundity (number of eggs), egg size (eggs/g), and % survival to the eyed-egg stage.

_The solutions to this exercise are found at the end of this book ([here](#ex2-answers)). You are **strongly recommended** to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped._

1.  Create a new R script called `Ex2.R` and save it in the `Chapter2` directory. Read in the data set `sockeye.csv`. Produce a basic summary of the data and take note of the data classes, missing values (`NA`), and the relative ranges for each variable.
2.	Make a histogram of fish weights for only hatchery-origin fish. Set `breaks = 10` so you can see the distribution more clearly.
3.	Make a scatter plot of the fecundity of females as a function of their body weight for wild fish only. Use whichever plotting character (`pch`) and color (`col`) you want. Change the main title and axes labels to reflect what they mean. Change the x-axis limits to be 600 to 3000 and the y-axis limits to be 0 to 3500. (_Hint: The `NAs` will not cause a problem. R will only use points where there are paired records for both `x` and `y` and ignore otherwise_).
4.	Add points for the same variables, but from hatchery fish. Use a different plotting character and a different color.
5.	Add a legend to the plot to differentiate between the two types of fish.  
6.	Make a multi-panel plot in a new window with box-and-whisker plots that compare (1) spawner weight, (2) fecundity, and (3) egg size between hatchery and wild fish. (_Hint: each comparison will be on its own panel_). Change the titles of each plot to reflect what you are comparing.
7.	Save the plot as a `.png` file in your working directory with a file name of your choosing.

### EXERCISE 2 BONUS {-}

1.  Make a bar plot comparing the mean survival to eyed-egg stage for each type of fish (hatchery and wild). Add error bars that represent 95% confidence intervals.
2.  Change the names of each bar, the main plot title, and the y-axis title.  
3.  Adjust the margins so there are 2 lines on the bottom, 5 on the left, 2 on the top, and 1 on the right.

<!--chapter:end:02-plotting-basics.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Basic Statistics {#ch3}

```{r, include = F}
rm(list = ls(all = T))
knitr::opts_chunk$set(fig.align = "center")

op = par(no.readonly = T)
par(mar = c(4,4,2,1))
sp = par(no.readonly = T)
```

```{r, echo = F, warning = F, message = F}
library(dplyr)
library(kableExtra)
library(knitr)
```

## Chapter Overview{-}

In this chapter, you will get familiar with the basics of using R for the purpose it was designed: statistical analysis. You will learn how to:

*  fit and interpret the output from various general linear models:
    - simple linear regression models
    - ANOVA (same as $t$-test but with more than two groups)
    - ANCOVA models
    - Interactions
*  conduct basic model selection
*  fit basic GLMs: the logistic regression model
*  Bonus topic: fitting non-linear regression models using `nls()`

R has gained popularity as a statistics software and is commonly used both in academia and governmental resource agencies. This popularity is likely a result of its power, flexibility, intuitive nature, and price (free!). For many students, this chapter may be the one that is most immediately useful for applying R to their own work. 

**IMPORTANT NOTE**: If you did not attend the sessions corresponding to Chapters \@ref(ch1) or \@ref(ch2), you are recommended to walk through the material found in those chapters before proceeding to this material. Also note that if you are confused about a topic, you can use **CTRL + F** to find previous cases where that topic has been discussed in this book. 

## Before You Begin {-}

You should create a new directory and R script for your work in this Chapter. Create a new R script called `Ch3.R` and save it in the directory `C:/Users/YOU/Documents/R-Book/Chapter3`. Set your working directory to that location. Revisit the material in Sections \@ref(scripts) and \@ref(working-dir) for more details on these steps. 

## The General Linear Model {#lm}

Much of this chapter will focus on the general linear model, so it is important to become familiar with it. The general linear model is a family of models that allows you to determine the relationship (if any) between some continuous response variable ($y$) and some predictor variable(s) ($x_n$) and is often written as:

\begin{equation}
  y_i=\beta_0 + \beta_1 x_{i1} + ... + \beta_j x_{ij}+ ... + \beta_n x_{in} + \varepsilon_i, \varepsilon_i \sim N(0,\sigma),
(\#eq:lin-mod)
\end{equation}

where the subscript $i$ represents an individual observation. The predictor variable(s) can be either categorical (i.e., grouping variables used in ANOVA, $t$-test, etc.), continuous (regression), or a combination of categorical and continuous (ANCOVA). The main purpose of fitting a linear model is to estimate the coefficients ($\beta$), and in some cases to determine if their values are "significantly" different from the value assumed by some null hypothesis.

The model makes several assumptions about the residuals^[The residuals ($\varepsilon_i$) are the difference between the data point $y_i$ and the model prediction $\hat{y}_i$: $\varepsilon_i=y_i-\hat{y}_i$] to obtain estimates of the coefficients. For reliable inference, the residuals must:

  * be independent
  * be normally-distributed
  * have constant variance across the range that $x_n$ was observed

In R, the general linear model is fitted using the `lm()` function. The basic syntax is `lm(y ~ x, data = dat)`^[This should look familiar from Section \@ref(box-whisker)]; it says: "fit a model with `y` as the response variable and `x` as the sole predictor variable, and look for those variables  in a data frame called `dat`". 

### Simple Linear Regression {#regression}

Read the data found in `sockeye.csv` (see in the [instructions](#data-sets) for help with acquiring the data) into R. This is the same data set you used in [Exercise 2](#ex2) - revisit this section for more details on the meaning of the variables. These are real data and are presented in @sockeye-cite.

```{r, eval = F}
dat = read.csv("../Data/sockeye.csv")
head(dat)
```

```{r, echo = F}
dat = read.csv("Data/sockeye.csv")
head(dat)
```

To fit a regression model using `lm()`, both `x` and `y` must be continuous (numeric) variables. In the data set `dat`, two such variables are called `weight` and `fecund`. Fit a regression model where you link the average fecundity (number of eggs) of fish sampled in an individual year to the average weight (in grams) for that year. Ignore for now that the fish come from two sources: hatchery and wild origin. 

```{r}
fit1 = lm(fecund ~ weight, data = dat)
```

If you run just the `fit1` object, you will see the model you ran along with the coefficient estimates of the intercept ($\beta_0$) and the slope ($\beta_1$):

```{r}
fit1
```

The mathematical formula for this model looks like this:

\begin{equation}
  y_i=\beta_0 + \beta_1 x_{i1} + \varepsilon_i, \varepsilon_i \sim N(0,\sigma),
(\#eq:lin-reg)
\end{equation}

where $x_{i1}$ is `weight`. The coefficients are interpreted as:

*  $\beta_0$: the y-intercept (mean `fecund` at zero `weight`)
*  $\beta_1$: the slope (change in `fecund` for one unit increase in `weight`)

For more information about the model fit, you can use the `summary()` function:

```{r}
summary(fit1)
```

Again the coefficient estimates are shown, but now you see the uncertainty on the parameter estimates (standard errors), the test statistic, and the p-value testing the null hypothesis that each coefficient has a zero value. Here you can see that the p-value does not support rejection of the null hypothesis that the slope is zero. You can see the residual standard error (variability of data around the fitted line and the estimate of $\sigma$), the $R^2$ value (the proportion of variation in `fecund` explained by variation in `weight`), and the p-value of the overall model.

You can easily see the model fit by using the `abline()` function. Make a new plot and add the fitted regression line:

```{r, eval = F}
plot(fecund ~ weight, data = dat, col = "grey", pch = 16, cex = 1.5)
abline(fit1)
```

```{r, echo = F}
par(sp)
plot(fecund ~ weight, data = dat, col = "grey", pch = 16, cex = 1.5)
abline(fit1)
```

It fits, but not very well. It seems there are two groups: one with data points mostly above the line and one with data points mostly below the line. You'll now run a new model to test whether those clusters of points are random or due to actual differences between groups.

### ANOVA: Categorical predictors {#anova}

ANOVA models attempt to determine if the means of different groups are different. You can fit them in the same basic `lm()` framework. But first, notice that:

```{r}
class(dat$type); levels(dat$type)
```

tells you the `type` variable is a factor. It has levels of `"hatch"` and `"wild"` which indicate the origin of the adult spawning fish sampled each year. If you pass `lm()` a predictor variable with a factor class, R will automatically fit it as an ANOVA model. See Section \@ref(factors) for more details on factors. Factors have levels that are explicitly ordered. By default, this ordering happens alphabetically: if your factor has levels `"a"`, `"b"`, and `"c"`, they will be assigned the order of `1`, `2` and `3`, respectively. You can always see how R is ordering your factor by doing something similar to this:

```{r}
pairs = cbind(
  as.character(dat$type),
  as.numeric(dat$type)
)

head(pairs); tail(pairs)
```

The functions `as.character()` and `as.numeric()` are coercion functions: they attempt to change the way something is interpreted. Notice that the level `"hatch"` is assigned the order `1` because it comes before `"wild"` alphabetically. The first level is termed the **reference level** because it is the group that all other levels are compared to when fitting a model. You can change the reference level using `dat$type_rlvl = relevel(dat$type, ref = "wild")`.

You are now ready to fit the ANOVA model, which will measure the size of the difference in the mean `fecund` between different levels of the factor `type`: 

```{r}
fit2 = lm(fecund ~ type, data = dat)
```

Think of this model as being written as:

\begin{equation}
  y_i=\beta_0 + \beta_1 x_{i1} + \varepsilon_i
(\#eq:anova)
\end{equation}

and assume that $x_{i1} = 0$ if observation $i$ is a fish from the `"hatch"` level and $x_{i1} = 1$ if observation $i$ is a fish from the `"wild"` level. Note that Equations \@ref(eq:lin-reg) and  \@ref(eq:anova) are the same, the only thing that differs is the coding of the variable $x_{i1}$. In the ANOVA case:

*  $\beta_0$ (the intercept) is the mean `fecund` for the `"hatch"` level and
*  $\beta_1$ is the difference in mean `fecund`: `"wild"` - `"hatch"`. 

So when you run `coef(fit2)` to extract the coefficient estimates and get:

```{r, echo = F}
coef(fit2)
```

you see that the mean fecundity of hatchery fish is about `r round(coef(fit2)[1], 0)` eggs and that the average wild fish has about `r round(coef(fit2)[2], 0)` more eggs than the average hatchery fish across all years. The fact that the p-value associated with the `typewild` coefficient when you run `summary(fit2)` is less than 0.05 indicates that there is statistical evidence that the difference in means is not zero.

Verify your interpretation of the coefficients:

```{r}
m = tapply(dat$fecund, dat$type, mean, na.rm = T)

# b0:
m[1]

# b1:
m[2] - m[1]
```

### ANCOVA: Continuous and categorical predictors

Now that you have seen that hatchery and wild fish tend to separate along the fecundity axis (as evidenced by the ANOVA results above), you would like to include this in your original regression model. You will fit two regression lines within the same model: one for hatchery fish and one for wild fish. This model is called an ANCOVA model and looks like this:

\begin{equation}
  y_i=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \varepsilon_i
(\#eq:ancova)
\end{equation}

If $x_{i1}$ is `type` coded with 0's and 1's as in Section \@ref(anova) and $x_{i2}$ is `weight`, then the coefficients are interpreted as:

*  $\beta_0$: the y-intercept of the `"hatch"` level (the reference level)
*  $\beta_1$: the difference in mean `fecund` at the same weight: `"wild"` - `"hatch"`
*  $\beta_2$: the slope of the `fecund` _versus_ `weight` relationship (this model assumes the lines have common slopes, i.e., that the lines are parallel)

You can fit this model and extract the coefficients table from the summary:

```{r}
fit3 = lm(fecund ~ type + weight, data = dat)
summary(fit3)$coef
```

And you can plot the fit:

```{r, eval = F}
plot(fecund ~ weight, data = dat, col = "grey",
     pch = ifelse(dat$type == "hatch", 1, 16), cex = 1.5)
abline(coef(fit3)[c(1,3)], lty = 2)
abline(sum(coef(fit3)[c(1,2)]), coef(fit3)[3])
legend("bottom", legend = c("Hatchery", "Wild"), pch = c(1,16), lty = c(2,1),
       col = "grey", pt.cex = 1.5, bty = "n", horiz = T)
```

```{r, echo = F}
par(sp)
plot(fecund ~ weight, data = dat, col = "grey",
     pch = ifelse(dat$type == "hatch", 1, 16), cex = 1.5)
abline(coef(fit3)[c(1,3)], lty = 2)
abline(sum(coef(fit3)[c(1,2)]), coef(fit3)[3])
legend("bottom", legend = c("Hatchery", "Wild"), pch = c(1,16), lty = c(2,1),
       col = "grey", pt.cex = 1.5, bty = "n", horiz = T)
```

Study this code to make sure you know what each line is doing. Use what you know about the meanings of the three coefficients to decipher the two `abline()` commands. Remember that `abline()` takes takes two arguments: `a` is the intercept and `b` is the slope.

### Interactions

Above, you have included an additional predictor variable (and parameter) in your model to help explain variation in the `fecund` variable. However, you have assumed that the effect of weight on fecundity is common between hatchery and wild fish (note the parallel lines in the figure above). You may have reason to believe that the effect of weight depends on the origin of the fish, e.g., wild fish may tend to accumulate more eggs than hatchery fish for the same increase in weight. Cases where the magnitude of the effect depends on the value of another predictor variable are known as "interactions". You can write the interactive ANCOVA model like this:

\begin{equation}
  y_i=\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1} x_{i2} + \varepsilon_i
(\#eq:ancova-interact)
\end{equation}

If $x_{i1}$ is `type` coded with 0's and 1's as in Section \@ref(anova) and $x_{i2}$ is `weight`, then the coefficients are interpreted as:

*  $\beta_0$: the y-intercept of the `"hatch"` level (the reference level)
*  $\beta_1$: the difference in y-intercept between the `"wild"` level and the `"hatch"` level. 
*  $\beta_2$: the slope of the `"hatch"` level
*  $\beta_3$: the difference in slope between the `"wild"` level and the `"hatch"` level.

You can fit this model:

```{r}
fit4 = lm(fecund ~ type + weight + type:weight, data = dat)

# or
# fit4 = lm(fecund ~ type * weight, data = dat)
```

The first option above is more clear in its statement, but both do the same thing. 

Plot the fit. Study these lines to make sure you know what each is doing. Use what you know about the meanings of the four coefficients to decipher the two `abline()` commands.

```{r, eval = F}
plot(fecund ~ weight, data = dat, col = "grey",
     pch = ifelse(dat$type == "hatch", 1, 16), cex = 1.5)
abline(coef(fit4)[c(1,3)], lty = 2)
abline(sum(coef(fit4)[c(1,2)]), sum(coef(fit4)[c(3,4)]))
legend("bottom", legend = c("Hatchery", "Wild"), pch = c(1,16), lty = c(2,1),
       col = "grey", pt.cex = 1.5, bty = "n", horiz = T)
```

```{r, echo = F}
par(sp)
plot(fecund ~ weight, data = dat, col = "grey",
     pch = ifelse(dat$type == "hatch", 1, 16), cex = 1.5)
abline(coef(fit4)[c(1,3)], lty = 2)
abline(sum(coef(fit4)[c(1,2)]), sum(coef(fit4)[c(3,4)]))
legend("bottom", legend = c("Hatchery", "Wild"), pch = c(1,16), lty = c(2,1),
       col = "grey", pt.cex = 1.5, bty = "n", horiz = T)
```

Based on the coefficients table:

```{r}
summary(fit4)$coef
```

It seems that fish of the different origins have approximately the same intercept, but that their slopes are quite different. 

### AIC Model Selection

You have now fitted four different models, each that makes different claims about how you can predict the fecundity of a given sockeye salmon at Redfish Lake. If you are interested in determining _which_ of these models you should use for prediction, you need to use **model selection**. Model selection attempts to find the model that is likely to have the smallest out-of-sample prediction error (i.e., future predictions will be close to what actually happens). One model selection metric is Akaike's Information Criterion (AIC), which is excellently described with ecological examples in @aic-cite. Lower AIC values mean the model should have better predictive performance. Obtain a simple AIC table from your fitted model objects and sort the table by increasing values of AIC:

```{r}
tab = AIC(fit1, fit2, fit3, fit4)
tab[order(tab$AIC),]
```

In general, AIC values that are different by more than 2 units are interpreted as having importantly different predictive performance [@aic-cite]. Based on this very quick-and-dirty analysis, it seems that in predicting future fecundity, you would want to use the interactive ANCOVA model.

## The Generalized Linear Model {#glms}

The models you fitted above are called "general linear models". They all make the assumption that the residuals ($\varepsilon_i$) are normally-distributed and that the response variable and the predictor variables are linearly-related. Oftentimes data and analyses do not follow this assumption. For such cases you should use the broader family of statistical models known as **generalized linear models**^[General linear models are a member of this family].

### Logistic Regression {#logis-regression}

One example is in the case of **binary** data. Binary data have two opposite outcomes, e.g., success/failure, lived/died, male/female, spawned/gravid, happy/sad, etc. If you want to predict how the probability of one outcome over its opposite changes depending on some other variable, then you need to use the **logistic regression model**, which is written as:

\begin{equation}
  logit(p_i)=\beta_0 + \beta_1 x_{i1} + ... + \beta_j x_{ij}+ ... + \beta_n x_{in}, y_i \sim Bernoulli(p_i)
(\#eq:logis-reg)
\end{equation}

Where $p_i$ is the probability of success for trial $i$ ($y_i = 1$) at the values of the predictor variables $x_{ij}$. The $logit(p_i)$ is the **link function** that links the linear parameter scale to the data scale. It constrains the value of $p_i$ to be between 0 and 1 regardless of the values of the $\beta$ coefficients. The logit link function can be expressed as:

\begin{equation}
  logit(p_i) = log\left(\frac{p_i}{1-p_i}\right)
(\#eq:logit)
\end{equation}

which is the **log odds** - the natural logarithm of the **odds**, which is a measure of how likely the event is to happen relative to it not happening^[Odds are commonly expressed as a ratio. For example, if there is a 75% chance of rain, the odds of it raining are 3 to 1. In other words, the outcome of rain is three times as likely as its opposite: not raining]. Make an R function to calculate the transformation performed by the link function:

```{r}
logit = function(p) {
  log(p/(1 - p))
}
```

The generalized linear model calculates the log odds of an outcome: `logit(p[i])` (which is given by the $\beta$ coefficients and the $x_{ij}$ data in Equation \@ref(eq:logis-reg)), so if you want to know the odds ratio of the outcome, you can simply take the inverse log. But, if you want to know the probability of the outcome `p[i]`, you have to apply the inverse logit function:

\begin{equation}
  expit(\eta_i)=\frac{e^{\eta_i}}{1 + e^{\eta_i}}
(\#eq:expit)
\end{equation}

where $\eta_i = logit(p_i)$. Make an R function to perform the inverse logit transformation to the probability scale:

```{r}
expit = function(eta) {  # lp stands for logit(p)
  exp(eta)/(1 + exp(eta))
}
```

Because all of the fitted $\beta$ coefficients are on the log scale, you cannot make easy interpretations about the relationships between the predictor variables and the observed outcomes without making these transformations. The following example will take you through the steps to fit a logistic regression model and how to interpret the model outputs using these functions.

Fit a logistic regression model to the sockeye salmon data. None of the variables of interest are binary, but you can create one. Look at the variable `dat$survival`. This is the average % survival of all eggs laid that make it to the "eyed-egg" stage. Suppose any year with over 70% egg survival is considered a successful year, so create a new variable `binary` which takes on a 0 if `dat$survival` is less than 70% and a 1 otherwise. 

```{r}
dat$binary = ifelse(dat$survival < 70, 0, 1)
```

This will be your response variable ($y$) and your model will estimate how the probability ($p$) of `binary` being a `1` changes (or doesn't) depending on the value of other variables ($x_{n}$). 

Analogous to the simple linear regression model (Section \@ref(regression)), estimate how $p$ changes with `weight`: (How does the probability of having a successful clutch relate to the weight of the fish?) 

```{r}
fit1 = glm(binary ~ weight, data = dat, family = binomial)
summary(fit1)$coef
```

The coefficients are interpreted as (remember, "success" is defined as having at least 70% egg survival to the stage of interest):

*  $\beta_0$: the log odds of success for a year in which average fish weight was 0 (which is not all that important, let alone difficult to interpret). It can be transformed into more interpretable quantities: 
    *  $e^{\beta_0}$ is the odds of success for a year in which average fish weight was 0 and 
    *  $expit(e^{\beta_0})$ is the probability of success for a year in which average fish weight was 0. 
*  $\beta_1$: the additive effect of fish weight on the log odds of success. More interpretable expressions are:
    *  $e^{\beta_1}$ is the ratio of the odds of success at two consective weights (e.g., 1500 and 1501) and Claims about $e^{\beta_1}$ are made as "for every one gram increase in average weight, success became $e^{\beta_1}$ times as likely to happen".
*  You can predict the probability of success at any weight using $expit(\beta_0 + \beta_1 weight)$:

```{r}
# create a sequence of weights to predict at
wt_seq = seq(min(dat$weight, na.rm = T),
             max(dat$weight, na.rm = t),
             length = 100)

# extract the coefficients and get p
p = expit(coef(fit1)[1] + coef(fit1)[2] * wt_seq)

```

You can plot the fitted model:

```{r, eval = F}
plot(p ~ wt_seq, type = "l", lwd = 3, ylim = c(0,1), las = 1)
```

```{r, echo = F}
par(sp)
plot(p ~ wt_seq, type = "l", lwd = 3, ylim = c(0,1), las = 1)
```

Fit another model comparing the probability of success between hatchery and wild fish (analogous to the ANOVA model in Section \@ref(anova)):

```{r}
fit2 = glm(binary ~ type, data = dat, family = binomial)
summary(fit2)$coef
```

An easier way to obtain the predicted probability is by using the `predict` function:

```{r}
predict(fit2,
        newdata = data.frame(type = c("hatch", "wild")),
        type = "response")
```

This plugs in the two possible values of the predictor variable and asks for the fitted probabilities.

Incorporate the origin type into your original model:

```{r}
fit3 = glm(binary ~ type + weight, data = dat, family = binomial)
```

and obtain the fitted probabilities for each group at each weight:

```{r}
p_hatch = predict(
  fit3, newdata = data.frame(type = "hatch", weight = wt_seq),
  type = "response"
)
p_wild = predict(
  fit3, newdata = data.frame(type = "wild", weight = wt_seq),
  type = "response"
)
```

and plot them:

```{r, eval = F}
plot(p_wild ~ wt_seq, type = "l", lwd = 3, lty = 1,
     ylim = c(0,1), las =1,
     xlab = "Weight (g)", ylab = "Pr(>70% Egg Survival)"
     )
lines(p_hatch ~ wt_seq, lwd = 3, lty = 2)
legend("topright", legend = c("Hatchery", "Wild"),
       lty = c(2,1), lwd = 3, bty = "n")
```

```{r, echo = F}
par(sp)
plot(p_wild ~ wt_seq, type = "l", lwd = 3, lty = 1,
     ylim = c(0,1), las =1,
     xlab = "Weight (g)", ylab = "Pr(>70% Egg Survival)"
     )
lines(p_hatch ~ wt_seq, lwd = 3, lty = 2)
legend("topright", legend = c("Hatchery", "Wild"),
       lty = c(2,1), lwd = 3, bty = "n")
```

Look for an interaction (all the code is the same except use `glm(binary ~ type * weight)` instead of `glm(binary ~ type + weight)` and change everything to `fit4` instead of `fit3`). 

```{r, echo = F}
fit4 = glm(binary ~ type * weight, data = dat, family = binomial)
p_hatch = predict(
  fit4, newdata = data.frame(type = "hatch", weight = wt_seq),
  type = "response"
)
p_wild = predict(
  fit4, newdata = data.frame(type = "wild", weight = wt_seq),
  type = "response"
)
```

```{r, eval = F}
plot(p_wild ~ wt_seq, type = "l", lwd = 3, lty = 1,
     ylim = c(0,1), las =1,
     xlab = "Weight (g)", ylab = "Pr(>70% Egg Survival)"
     )
lines(p_hatch ~ wt_seq, lwd = 3, lty = 2)
legend("topright", legend = c("Hatchery", "Wild"),
       lty = c(2,1), lwd = 3, bty = "n")
```

```{r, echo = F}
par(sp)
plot(p_wild ~ wt_seq, type = "l", lwd = 3, lty = 1,
     ylim = c(0,1), las =1,
     xlab = "Weight (g)", ylab = "Pr(>70% Egg Survival)"
     )
lines(p_hatch ~ wt_seq, lwd = 3, lty = 2)
legend("topright", legend = c("Hatchery", "Wild"),
       lty = c(2,1), lwd = 3, bty = "n")
```

### AIC Model Selection

You may have noticed that you just did the same analysis with `binary` as the response instead of `fecund`. Perform an AIC analysis to determine which model is likely to be best for prediction:

```{r}
tab = AIC(fit1, fit2, fit3, fit4)
tab[order(tab$AIC),]
```

The best model includes `weight` only, although there is not much confidence in this conclusion (based on how similar the AIC values are).  

## Probability Distributions {#dists}

A probability distribution is a way of representing the probability of an event or value of a parameter and they are central to statistical theory. Some of the most commonly used distributions are summarized in Table `r if(is_html_output()) "\\@ref(tab:dist-table-html)" else "\\@ref(tab:dist-table-pdf)"`, along with the suffixes of the functions in R that correspond to each distribution. For an excellent and ecologically-focused description of probability distributions, checkout Chapter 4 in @emdbook-cite on them^[There is a free proof version online: <https://ms.mcmaster.ca/~bolker/emdbook/book.pdf>]. 

```{r dist-table-html, results = "asis", echo = F, eval = is_html_output()}

tab = data.frame(
  type = c(rep("Continuous", 4), rep("Discrete", 3)),
  dist = c("Normal", "Lognormal", "Uniform", "Beta", "Binomial", "Multinomial", "Poisson"),
  desc = c("Models the relative frequency of outcomes that are symmetric around a mean, can be negative",
           "Models the relative frequency of outcomes that are normally-distributed on the log-scale",
           "Models values that are between two endpoints and that all occur with the same frequency",
           "Models values that are between 0 and 1",
           "Models the number of successes from a given number of trials when there are only two possible outcomes and all trials have the same probability of success",
           "The same as the binomial distribution, but when there are more than two possible outcomes",
           "Used for count data in cases where the variance and mean are roughly equal"),
  suffix = c("`-norm()`", "`-lnorm()`", "`-unif()`", "`-beta()`", "`-binom()`", "`-multinom()`", "`-pois()`")
)

colnames(tab) = c("Type", "Distribution", "Common Uses", "R Suffix")

kable(tab, align = "lll", caption = "A brief description of probability distributions commonly used in ecological problems, including the function suffix in R.") %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T) %>%
  collapse_rows(columns = 1, valign = "middle")

```

```{r dist-table-pdf, echo = F, eval = is_latex_output()}
tab = data.frame(
  dist = c("Normal", "Lognormal", "Uniform", "Beta", "Binomial", "Multinomial", "Poisson"),
  desc = c("Models the relative frequency of outcomes that are symmetric around a mean, can be negative",
           "Models the relative frequency of outcomes that are normally-distributed on the log-scale",
           "Models values that are between two endpoints and that all occur with the same frequency",
           "Models values that are between 0 and 1",
           "Models the number of successes from a given number of trials when there are only two possible outcomes and all trials have the same probability of success",
           "The same as the binomial distribution, but when there are more than two possible outcomes",
           "Used for count data in cases where the variance and mean are roughly equal"),
  suffix = c("-norm()", "-lnorm()", "-unif()", "-beta()", "-binom()", "-multinom()", "-pois()")
)

colnames(tab) = c("Distribution", "Description", "R Suffix")

kable(tab, "latex", booktabs = T,
      caption = "A brief description of probability distributions commonly used in ecological problems, including the function suffix in R.") %>%
  kable_styling(full_width = F) %>%
  row_spec(0, bold = T) %>%
  column_spec(2, width = "20em") %>%
  column_spec(3, monospace = T) %>%
  group_rows("Continuous", 1,4, hline_after = T) %>%
  group_rows("Discrete", 5, 7, hline_before = F, hline_after = T)

```

In R, there are four different ways to use each of these distribution functions (each has a separate prefix):

* **The probability density (or mass) function** (`d-`): the height of the probability distribution function at some given value of the random variable. 
* **The cumulative density function** (`p-`): what is the sum of the probability densities for all random variables below the input argument `q`.
*  **The quantile function** (`-q`): what value of the random variable do `p`% fall below?
*  **The random deviates function** (`-r`): generates random variables from the distribution in proportion to their probability density. 

Suppose that $x$ represents the length of individual age 6 largemouth bass in your private fishing pond. Assume that $x \sim N(\mu=500, \sigma=50)$^[English: $x$ is a normal random variable with mean equal to 500 and standard deviation equal to 50]. Here is the usage of each of the distribution functions and a plot illustrating them:

```{r norm-plots, fig.height = 6.5, fig.width = 6.5, fig.cap="The four `-norm` functions with input (x-axis) and output (y-axis) displayed."}
# parameters
mu = 500; sig = 50
# a sequence of possible random variables (fish lengths)
lengths = seq(200, 700, length = 100)
# a sequence of possible cumulative probabilities
cprobs = seq(0, 1, length = 100)
# use the four functions
densty = dnorm(x = lengths, mean = mu, sd = sig)  # takes specific lengths
cuprob = pnorm(q = lengths, mean = mu, sd = sig)  # takes specific lengths
quants = qnorm(p = cprobs, mean = mu, sd = sig)   # takes specific probabilities
random = rnorm(n = 1e4, mean = mu, sd = sig)      # takes a number of random deviates to make

# set up plotting region: see ?par for more details
# notice the tricks to clean up the plot
par(
  mfrow = c(2,2),    # set up 2x2 regions
  mar = c(3,3,3,1),  # set narrower margins
  xaxs = "i",        # remove "x-buffer"
  yaxs = "i",        # remove "y-buffer"
  mgp = c(2,0.4,0),  # bring in axis titles ([1]) and tick labels ([2])
  tcl = -0.25        # shorten tick marks
)
plot(densty ~ lengths, type = "l", lwd = 3, main = "dnorm()",
     xlab = "Fish Length (mm)", ylab = "Density", las = 1,
     yaxt = "n") # turns off y-axis
axis(side = 2, at = c(0.002, 0.006), labels = c(0.002, 0.006), las = 2)
plot(cuprob ~ lengths, type = "l", lwd = 3, main = "pnorm()",
     xlab = "Fish Length (mm)", ylab = "Cumulative Probability", las = 1)
plot(quants ~ cprobs, type = "l", lwd = 3, main = "qnorm()",
     xlab = "P", ylab = "P Quantile Length (mm)", las = 1)
hist(random, breaks = 50, col = "grey", main = "rnorm()",
     xlab = "Fish Length (mm)", ylab = "Frequency", las = 1)
box() # add borders to the histogram
```

Notice that `pnorm()` and `qnorm()` are inverses of one another: if you put the output of one into the input of the other, you get the original input back:

```{r}
qnorm(pnorm(0))
```

`pnorm(0)` asks R to find the probability that $x$ is less than zero for the standard normal distribution ($N(0,1)$ - this is the default if you don't specify `mean` and `sd`). `qnorm(pnorm(0))` asks R to find the value of $x$ that `pnorm(0)` * 100% of the possible values fall below. If the nesting is confusing, this line is the same as:

```{r, eval = F}
p = pnorm(0)
qnorm(p)
```

## Bonus Topic: Non-linear Regression {#nls}

You fitted linear and logistic regression models in Sections \@ref(regression) and \@ref(logis-regression), however, R allows you to fit non-linear regression models as well.

First, read the data into R: 

```{r, eval = F}
dat = read.csv("../Data/feeding.csv"); summary(dat)
```

```{r, echo = F}
dat = read.csv("Data/feeding.csv")
summary(dat)
```

These are hypothetical data from an experiment in which you were interested in quantifying the functional feeding response^[A functional response is the number of prey consumed by a predator at various prey densities] of a fish predator on zooplankton in an aquarium. You experimentally manipulated the prey density (`prey`) and counted how many prey items were consumed (`cons`).

Plot the data:

```{r, eval = F}
plot(cons ~ prey, data = dat, cex = 1.5, pch = 16, col = "grey")
```

```{r, echo = F}
par(sp)
plot(cons ~ prey, data = dat, cex = 1.5, pch = 16, col = "grey")
```

You can see a distinct non-linearity to the relationship. The Holling Type II functional response^[This function rises quickly at low prey densities, but saturates at high densities] has this functional form:

\begin{equation}
  y_i=\frac{ax_i}{1+ahx_i} + \varepsilon_i, \varepsilon_i \sim N(0, \sigma)
(\#eq:func-resp)
\end{equation}

where $x_i$ is `prey` and $y_i$ is `cons`. 

You can fit this model in R using the `nls()` function: 

```{r}
fit = nls(cons ~ (a * prey)/(1 + a * h * prey), data = dat,
          start = c(a = 3, h = 0.1))
```

In general, it behaves very similarly to the `lm()` function, however there are a few differences:

*  You need to specify the functional form of the curve you are attempting to fit. In using `lm()`, the terms are all additive (e.g., `type + weight`), but in using `nls()`, this is not the case. For example, note the use of division. 
*  You may need to provide starting values for the parameters (coefficients) you are estimating. This is because `nls()` will use a search algorithm to find the parameters of the best fit line, and it may need to have a reasonable idea of where to start looking for it to work properly.
*  You cannot plot the fit using `abline()` anymore, because you have more parameters than just a slope and intercept, and the relationship between $x$ and $y$ is no longer linear.

Despite these differences, you can obtain similar output as from `lm()` by using the `summary()`, `coef()`, and `predict()` functions. 

```{r}
prey_seq = seq(min(dat$prey), max(dat$prey), length = 100)
cons_seq = predict(fit, newdata = data.frame(prey = prey_seq))
```

Draw the fitted line over top of the data:

```{r, eval = F}
plot(cons ~ prey, data = dat, cex = 1.5, pch = 16, col = "grey")
lines(cons_seq ~ prey_seq, lwd = 3)
```

```{r, echo = F}
par(sp)
plot(cons ~ prey, data = dat, cex = 1.5, pch = 16, col = "grey")
lines(cons_seq ~ prey_seq, lwd = 3)
```


---

## Exercise 3 {-}

You should create a new R script called `Ex3.R` in your working directory for this chapter. You will again be using the `sockeye.csv` data found in @sockeye-cite. 

_The solutions to this exercise are found at the end of this book ([here](#ex3-answers)). You are **strongly recommended** to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped._

1.  Perform the same analyses as conducted in Section \@ref(lm) (simple linear regression, ANOVA, ANCOVA, ANCOVA with interaction), using `egg_size` as the response variable. The predictor variables you should use are `type` (categorical) and `year`. You should plot the fit for each model separately and perform an AIC analysis. Practice interpreting the coefficient estimates.
2.  Perform the same analyses as conducted in Section \@ref(glms), this time using a success threshold of 80% survival to the eyed-egg stage (instead of 70%). Use `egg_size` and `type` as the predictor variables. You should plot the fitted lines for each model separately and perform an AIC analysis. Practice interpreting the coefficient estimates.
3.  Make the same graphic as in Figure \@ref(fig:norm-plots) with at least one of the other distributions listed in Table `r if(is_html_output()) "\\@ref(tab:dist-table-html)" else "\\@ref(tab:dist-table-pdf)"` (other than the multinomial - being a multivariate distribution, it wouldn't work well with this code). Try thinking of a variable from your work that meets the uses of each distribution in Table `r if(is_html_output()) "\\@ref(tab:dist-table-html)" else "\\@ref(tab:dist-table-pdf)"` (or one that's not listed). If you run into trouble, check out the help file for that distribution^[Executing `?rnorm` or any other of the `-norm()` functions will take you to a page with info on all four function types for that distribution]. 

### Exercise 3 Bonus {-}

1.  Fit a von Bertalanffy growth model to the data found in the `growth.csv` data file. Visit Section \@ref(boot-test-ex) (particularly Equation \@ref(eq:vonB)) for details on this model. Use the initial values: `linf = 600`, `k = 0.3`, `t0 = -0.2`. Plot the fitted line over top of the data. 

<!--chapter:end:03-basic-statistics.Rmd-->

# Monte Carlo Methods {#ch4}

```{r, include = F}
rm(list = ls(all = T))
knitr::opts_chunk$set(fig.align = "center")

op = par(no.readonly = T)
par(mar = c(4,4,2,1))
sp = par(no.readonly = T)
```

```{r, echo = F, warning = F, message = F}
library(dplyr)
library(kableExtra)
library(knitr)
I_power = 500
sra_n_rep = 2000
# I_power = 10
# sra_n_rep = 100
```

## Chapter Overview{-}

Simulation modeling is one of the primary reasons to move away from `spreadsheet-type` programs (like Microsoft Excel) and into a program like `R`. R allows you to replicate the same (possibly complex and detailed) calculations over and over with different random values. You can then summarize and plot the results of these replicated calculations all within the same program. Analyses of this type are called **Monte Carlo methods**: they randomly sample from a set of quantities for the purpose of generating and summarizing a distribution of some statistic related to the sampled quantities. If this concept is confusing, hopefully this chapter will clarify. 

In this chapter, you will learn the basic skills needed for simulation (i.e., Monte Carlo) modeling in R including:

*  introduce randomness to a model
*  repeat calculations many times with `replicate()` and `for()` loops
*  summarization of many values from a distribution
*  more advanced function writing
*  applying population dynamics *added by Anthony*

**IMPORTANT NOTE**: If you did not attend the sessions corresponding to Chapters \@ref(ch1) or \@ref(ch2) or \@ref(ch3), you are recommended to walk through the material found in those chapters before proceeding to this material. Remember that if you are confused about a topic, you can use **CTRL + F** to find previous cases where that topic has been discussed in this book.

## Before You Begin {-}

You should create a new directory and R script for your work in this Chapter. Create a new R script called `Ch4.R` and save it in the directory `C:/Users/YOU/Documents/R-Book/Chapter4`. Set your working directory to that location. Revisit the material in Sections \@ref(scripts) and \@ref(working-dir) for more details on these steps.

## Layout of This Chapter {-}

This chapter is divided into three main sections:

*  **Required Material** (Sections \@ref(randomness) - \@ref(mc-summaries)) which is necessary to understand the examples in this chapter and the subsequent chapters

*  **Example Cases** (Sections \@ref(sim-examples) and \@ref(resample-examples)) which apply the skills learned in the required material. In the workshop session, you will walkthrough 2-3 of these example cases at the choice of the group of the participants. If you are interested in simulation modeling, you are suggested to work through all of the example cases, as slightly different tricks will be shown in the different examples. 

* **Population dynamics** (Sections ...) - this is an extention of simulation and MCMC methods to estimate the viability of populations (PVA).

## Introducing Randomness {#randomness}

A critical part of simulation modeling is the use of random processes. A **random process** is one that generates a different outcome according to some rules each time it is executed. They are tightly linked to the concept of **uncertainty**: you are unsure about the outcome the next time the process is executed. There are two basic ways to introduce randomness in R: **random deviates** and **resampling**.

### Random deviates

In Section \@ref(dists), you learned about using probability distributions in R. One of the uses was the `r-` family of distribution functions. These functions create random numbers following a random process specified by a probability distribution. 

Consider animal survival as an example.  At the end of each year, each individual alive at the start can either live or die. There are two outcomes here, and suppose each animal has an 80% chance of surviving. The number of individuals that survive is the result of a **binomial random process** in which there were $n$ individuals alive at the start of this year and $p$ is the probability that any one individual survives to the next year. You can execute one binomial random process where $p = 0.8$ and $n = 100$ like this:

```{r}
rbinom(n = 1, size = 100, prob = 0.8)
```

The result you get will almost certainly be different from the one printed here. That is the random component.

You can execute many such binomial processes by changing the `n` argument. Plot the distribution of expected surviving individuals:

```{r, eval = F}
survivors = rbinom(1000, 100, 0.8)
hist(survivors, col = "skyblue")
```

```{r, echo = F}
par(sp)
survivors = rbinom(1000, 100, 0.8)
hist(survivors, col = "skyblue")
```

Another random process is the **lognormal process**: it generates random numbers such that the log of the values are normally-distributed with mean equal to `logmean` and standard deviation equal to `logsd`:

```{r, eval = F}
hist(rlnorm(1000, 0, 0.1), col = "skyblue")
```

```{r, echo = F}
par(sp)
hist(rlnorm(1000, 0, 0.1), col = "skyblue")
```

There are many random processes you can use in R. Checkout Table `r if(is_html_output()) "\\@ref(tab:dist-table-html)" else "\\@ref(tab:dist-table-pdf)"` for more examples as well as the help files for each individual function for more details.

### Resampling

Using random deviates works great for creating new random numbers, but what if you already have a set of numbers that you wish to introduce randomness to? For this, you can use **resampling techniques**. In R, the `sample()` function is used to sample `size` elements from the vector `x`:

```{r}
sample(x = 1:10, size = 5)
```

You can sample with replacement (where it is possible to sample the same element two or more times):

```{r}
sample(x = c("a", "b", "c"), size = 10, replace = T)
```

You can set probabilities on the sampling of different elements^[If `prob` doesn't sum to 1, then it will be rescaled: `prob = prob/sum(prob)`]:

```{r}
sample(x = c("live", "die"), size = 10, replace = T,
       prob = c(0.8, 0.2))
```

Notice that this is the same as the binomial random process above, but with only 10 trials and the printing of the outcomes rather than the number of successes.

## Reproducing Randomness 

For reproducibility purposes, you may wish to get the same exact random numbers each time you run your script. To do this, you need to set the **random seed**, which is the starting point of the random number generator your computer uses. If you run these two lines of code, you should get the same result as printed here:

```{r}
set.seed(1234)
rnorm(1)
```

## Replication

To use Monte Carlo methods, you need to be able to replicate some random process many times. There are two main ways this is commonly done: either with`replicate()` or with `for()` loops.

### `replicate()`

The `replicate()` function executes some expression many times and returns the output from each execution. Say we have a vector `x`, which represents 30 observations of fish length (mm):

```{r}
x = rnorm(30, 500, 30)
```

We wish to build the sampling distribution of the mean length "by hand". We can sample randomly from it, calculate the mean, then repeat this process many times:

```{r}
means = replicate(n = 1000, expr = {
  x_i = sample(x, length(x), replace = T)
  mean(x_i)
})
```

If we take `mean(means)` and `sd(means)`, that should be very similar to `mean(x)` and `se(x)`. Create the `se()` function (also shown in Section \@ref(error-bars)) and prove this to yourself:

```{r}
se = function(x) sd(x)/sqrt(length(x))
mean(means); mean(x)
sd(means); se(x)
```

### The `for()` loop {#for-loops}

In programming, a *loop* is a command that does something over and over until it reaches some point that you specify. R has a few types of loops: `repeat()`, `while()`, and `for()`, to name a few. `for()` loops are among the most common in simulation modeling. A `for()` loop repeats some action for however many times you tell it **for** each value in some vector. The syntax is:

```{r eval = F}
for (var in seq) {
  expression(var)
}
```

The loop calculates the expression for values of `var` for each element in the vector `seq`. For example:

```{r}
for (i in 1:5) {
  print(i^2)
}
```

The `print()` command will be executed 5 times: once for each value of `i`. It is the same as:

```{r, eval = F}
i = 1; print(i^2); i = 2; print(i^2); i = 3; print(i^2); i = 4; print(i^2); i = 5; print(i^2)
```

If you remove the `print()` function, see what happens:

```{r}
for (i in 1:5) {
  i^2
}
```

Nothing is printed to the console. R did the calculation, but did not show you or store the result. Often, you'll need to store the results of the calculation in a **container object**:

```{r}
results = numeric(5)
```

This makes an empty numeric vector of length 5 that are all 0's. You can store the output of your loop calculations in `results`:

```{r}
for (i in 1:5) {
  results[i] = i^2
}
results
```

When `i^2` is calculated, it will be placed in the element `results[i]`. This was a trivial example, because you would never use a `for()` loop to do things as simple as vectorized calculation. The expression `(1:5)^2` would give the same result with significantly less code (see Section \@ref(vector-math)). 

However, there are times where it is advantageous to use a loop. Particularly in cases where:

1.  the calculations in one element are determined from the value in previous elements, such as in time series models
2.  the calculations have multiple steps 
3.  you wish to store multiple results
4.  you wish to track the progress of your calculations

As an illustration for item (1) above, build a (very) basic population model. At the start of the first year, the population abundance is 1000 individuals and grows by an average factor of 1.1 per year (reproduction and death processes result in a growth rate of 10%) before harvest. The growth rate varies randomly, however. Each year, the 1.1 growth factor has variability introduced by small changes in survival and reproductive process. Model these variations as lognormal random variables. After production, 8% of the population is harvested. Simulate the abundance at the end of the year for 100 years:

```{r}
nt = 100       # number of years
N = NULL       # container for abundance
N[1] = 1000    # first end-of-year abundance

for (t in 2:nt) {
  # N this year is N last year * growth *
    # randomness * fraction that survive harvest
  N[t] = (N[t-1] * 1.1 * rlnorm(1, 0, 0.1)) * (1 - 0.08)
}
```

Plot the abundance time series:

```{r, eval = F}
plot(N, type = "l", pch = 15, xlab = "Year", ylab = "Abundance")
```

```{r, echo = F}
par(sp)
plot(N, type = "l", pch = 15, xlab = "Year", ylab = "Abundance")
```

Examples of the other three utilities of using `for()` loops over replicate are shown in the example cases and exercises.

## Function Writing {#adv-funcs}
In Monte Carlo analyses, it is often useful to wrap code into functions. This allows for easy replication and setting adjustment (e.g., if you wanted to compare the growth trajectories of two populations with differing growth rates). As an example, turn the population model shown above into a function:

```{r}
pop_sim = function(nt, grow, sd_grow, U, plot = F) {
  N = NULL # empty flexible vector container
  N[1] = 1000
  for (t in 2:nt) {
    N[t] = (N[t-1] * grow * rlnorm(1, 0, sd_grow)) * (1 - U)
  }
  
  if (plot) {
    plot(N, type = "l", pch = 15, xlab = "Year", ylab = "Abundance")
  }
  
  N
}
```

This function takes five inputs: 

*  `nt`: the number of years,
*  `grow`: the population growth rate,
*  `sd_grow`: the amount of annual variability in the growth rate
*  `U`: the annual exploitation rate
*  `plot`: whether you wish to have a plot created. It has a default setting of `FALSE`: if you don't specify `plot = T` when you call `pop_sim()`, you won't see a plot made.

It returns one output: the vector of population abundance.

Execute your simulation function once using the same settings as before:

```{r, results = "hide"}
pop_sim(100, 1.1, 0.1, 0.08, T)
```

Now, you wish to replicate this simulation 1000 times. Use the `replicate()` function to do this:

```{r}
out = replicate(n = 1000, expr = pop_sim(100, 1.1, 0.1, 0.08, F))
```

If you do `dim(out)`, you'll see that years are stored as rows (there are `r nrow(out)` of them) and replicates are stored as columns (there are `r ncol(out)` of them). Notice how wrapping the code in the function made the `replicate()` call easy. 

**Here are some advantages of wrapping code like this into a function**:

*  If you do the same task at multiple places in your script, you don't need to type all of the code to perform the task, just the function call. 
*  If you need to change the way the function behaves (i.e., the function body), you only need to change it in one place: in the function definition.
*  You can easily change the settings of the code (e.g., whether or not you want to see the plot) in one place
*  Function writing can lead to shorter scripts
*  Function writing can lead to more readable code (if it is easy for readers to interpret what your functions do - informative function and argument names, as well as documentation can help here)

## Summarization {#mc-summaries}

After replicating a calculation many times, you will need to summarize the results. Here are several examples using the `out` matrix from Section \@ref(adv-funcs).

### Central Tendency

You can calculate the mean abundance each year across your iterations using the `apply()` function (Section \@ref(data-summaries)):

```{r}
N_mean = apply(out, 1, mean)
N_mean[1:10]
```

You could do the same thing using `median` rather than `mean`. The mode is more difficult to calculate in R, if you need to get the mode, try to Google it^[Google is an R programmer's best friend. There is a massive online community for R, and if you have a question on something, it has almost certainly been asked somewhere on the web.].

### Variability

One of the primary reasons to conduct a Monte Carlo analysis is to obtain estimates of variability. You can summarize the variability easily using the `quantile()` function:

```{r}
# obtain the 10% and 90% quantiles each year across iterations
N_quants = apply(out, 1, function(x) quantile(x, c(0.1, 0.9)))
```

Notice how a user-defined function was passed to `apply()`. Now plot the summary of the randomized abundances as a time series like before:

```{r, eval = F}
plot(N_mean, type = "l", ylim = c(0, 10000))
lines(N_quants[1,], lty = 2)
lines(N_quants[2,], lty = 2)
```

```{r, echo = F}
par(sp)
plot(N_mean, type = "l", ylim = c(0, 10000))
lines(N_quants[1,], lty = 2)
lines(N_quants[2,], lty = 2)
```

The range within the two dashed lines represents the range that encompassed the central 80% of the random abundances each year.

### Frequencies

Often you will want to count how many times something happened. In some cases, the fraction of times something happened can be interpreted as a probability of that event occuring.

The `table()` function is useful for counting occurrences of discrete events. Suppose you are interested in how many of your iterations resulted in fewer than 1000 individuals at year 10:

```{r}
out10 = ifelse(out[10,] < 1000, "less10", "greater10")
table(out10)
```

Suppose you are also interested in how many of your iterations resulted in fewer than 1100 individuals at year 20:

```{r}
out20 = ifelse(out[20,] < 1100, "less20", "greater20")
table(out20)
```

Now suppose you are interested in how these two metrics are related:

```{r}
table(out10, out20)
```

One example of an interpretation of this output might be that populations that were greater than 1000 at year 10 were commonly greater than 1100 at year 20. Also, if a population was less than 1000 at year 10, it was more likely to be less than 1100 at year 20 than to be greater than it.

You can turn these into probabilities (if you believe your model represents reality) by dividing each cell by the total number of iterations:

```{r}
round(table(out10, out20)/1000, 2)
```

## Simulation-Based Examples {#sim-examples}

### Test `rnorm` {#rnorm-ex}

In this example, you will verify that the function `rnorm()` works the same way that `qnorm()` and `pnorm()` indicate that it should work. That is, you will verify that random deviates generated using `rnorm()` have the same properties as the true normal distribution given by `qnorm()` and `pnorm()`. Hopefully it will also reinforce the way the random, quantile, and cumulative distribution functions work in R.

First, specify the mean and standard deviation for this example:

```{r}
mu = 500; sig = 30
```

Now make up `n` (any number of your choosing, something greater than 10) random deviates from this normal distribution:

```{r}
random = rnorm(100, mu, sig)
```

Test the quantiles (obtain the values that `p` * 100% of the quantities fall below, both for random numbers and from the `qnorm()` function):

```{r, eval = F}
p = seq(0.01, 0.99, 0.01)
random_q = quantile(random, p)
normal_q = qnorm(p, mu, sig)
plot(normal_q ~ random_q); abline(c(0,1))
```

```{r, echo = F}
par(sp)
p = seq(0.01, 0.99, 0.01)
random_q = quantile(random, p)
normal_q = qnorm(p, mu, sig)
plot(normal_q ~ random_q); abline(c(0,1))
```

The fact that all the quantiles fall around the 1:1 line suggests the `n` random samples are indeed from a normal distribution. Any deviations you see are due to sampling errors. If you increase `n` to `n = 1e6` (one million), you'll see no deviations.  This is called a **q-q plot**, and is frequently used to assess the fit of data to a distribution.

Now test the random values in their agreement with the `pnorm()` function. Plot the cumulative density functions for the truly normal curve and the one approximated by the random deviates:

```{r, eval = F}
q = seq(400, 600, 10)
random_cdf = ecdf(random)
random_p = random_cdf(q)
normal_p = pnorm(q, mu, sig)
plot(normal_p ~ q, type = "l", col = "blue")
points(random_p ~ q, col = "red")
```

```{r, echo = F}
par(sp)
q = seq(400, 600, 10)
random_cdf = ecdf(random)
random_p = random_cdf(q)
normal_p = pnorm(q, mu, sig)
plot(normal_p ~ q, type = "l", col = "blue")
points(random_p ~ q, col = "red")
```

The `ecdf()` function obtains the empirical cumulative density function (which is just `pnorm()` for a sample). It allows you to plug in any random variable and obtain the probability of having one less than it.

### Stochastic Power Analysis {#power-ex}

A **power analysis** is one where the analyst wishes to determine how much power they will have to detect an effect. Power is inversely related to the probability of making a Type II Error: failing to reject a false null hypothesis^[English: concluding there is no effect when there truly is one]. In other words, having high power means that you have a high chance of detecting an effect if an effect truly exists. Power is a function of the effect size, the sample size `n`, and the variability in the data. Strong effects are easier to detect than weak ones, more samples increase the test's sensitivity (the ability to detect weak effects), and lower variability results in more power.

You can conduct a power analysis using stochastic simulation (i.e., a Monte Carlo analysis). Here, you will write a power analysis to determine how likely are you to be able to correctly identify what you deem to be a biologically-meaningful difference in survival between two tagging procedures. 

You know one tagging procedure has approximately a 10% mortality rate (10% of tagged fish die within the first 12 hours as result of the tagging process). Another cheaper, and less labor-intensive method has been proposed, but before implementing it, your agency wishes to determine if it will have a meaningful impact on the reliability of the study or on the ability of the crew to tag enough individuals that will survive long enough to be useful. You and your colleagues determine that if the mortality rate of the new tagging method reaches 25%, then gains in time and cost-efficiency would be offset by needing to tag more fish (because more will die). You have decided to perform a small-scale study to determine if using the new method could result in 25% or more mortality. The study will tag `n` individuals using both methods (new and old) and track the fraction that survived after 12 hours. Before performing the study however, you deem it important to determine how large `n` needs to be to answer this question. You decide to use a stochastic power analysis to help your research group. The small-scale study can tag a total of at most 100 fish with the currently available resources. Could you tag fewer than 100 total individuals and still have a high probability of detecting a statistically significant difference in mortality?

The stochastic power analysis approach works like this (this is called **psuedocode**):

1.  Simulate data under the reality that the difference is real with `n` observations per treatment, where `n < 100/2` 
2.  Fit the model that will be used when the real data are collected to the simulated data
3.  Determine if the difference was detected with a significant p-value
4.  Replicate steps 1 - 3 many times
5.  Replicate step 4 while varying `n` over the interval from 10 to 50
6.  Determine what fraction of the p-values were deemed significant at each `n`

Step 2 will require fitting a generalized linear model; for a review, revisit Section \@ref(glms) (specifically Section \@ref(logis-regression) on logistic regression).

First, create a function that will generate data, fit the model, and determine if the p-value is significant (steps 1-3 above):

```{r}
sim_fit = function(n, p_old = 0.10, p_new = 0.25) {
  
  ### step 1: create the data ###
  # generate random response data
  dead_old = rbinom(n, size = 1, prob = p_old)
  dead_new = rbinom(n, size = 1, prob = p_new)
  # create the predictor variable
  method = rep(c("old", "new"), each = n)
  # create a data.frame to pass to glm
  df = data.frame(dead = c(dead_old, dead_new), method = method)
  # relevel so old is the reference
  df$method = relevel(df$method, ref = "old")
  
  ### step 2: fit the model ###
  fit = glm(dead ~ method, data = df, family = binomial)
  
  ### step 3: determine if a sig. p-value was found ###
  # extract the p-value
  pval = summary(fit)$coef[2,4]
  # determine if it was found to be significant
  pval < 0.05
}
```

Next, for steps 4 and 5, set up a **nested `for` loop**. This will have two loops: one that loops over sample sizes (step 5) and one that loops over replicates of each sample size (step 4). First, create the looping objects and containers:

```{r, eval = F}
I = 500  # the number of replicates at each sample size
n_try = seq(10, 50, 10)  # the test sample sizes
N = length(n_try)        # count them
# container: 
out = matrix(NA, I, N) # matrix with I rows and N columns
```

```{r, echo = F}
I = I_power
n_try = seq(10, 50, 10)  # the test sample sizes
N = length(n_try)     
out = matrix(NA, I, N) # matrix with I rows and N columns
```

Now perform the nested loop. The inner-loop iterations will be completed for each element of `n` in the sequence `1:N`. The output (which is one element: `TRUE` or `FALSE` based on the significance of the p-value) is stored in the corresponding row and column for that iteration of that sample size.

```{r}
for (n in 1:N) {
  for (i in 1:I) {
    out[i,n] = sim_fit(n = n_try[n])
  }
}
```

You now have a matrix of `TRUE` and `FALSE` elements that indicates whether a significant difference was found at the $\alpha = 0.05$ level if the effect was truly as large as you care about. You can obtain the proportion of all the replicates at each sample size that resulted in a significant difference using the `mean()` function with `apply()`:

```{r, eval = F}
plot(apply(out, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of Finding Effect (Power)")
```

```{r, echo = F}
par(sp)
plot(apply(out, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of Finding Effect (Power)")
```

Even if you tagged `r n_try[N] * 2` fish total, you would only have a `r round(mean(out[,N]),2) * 100`% chance of saying the effect (which truly is there!) is present under the null hypothesis testing framework.

Suppose you and your colleagues aren't relying on p-values in this case, and are purely interested in how precisely the **effect size** would be estimated. Adapt your function to determine how frequently you would be able to estimate the true mortality of the new method within +/- 5% based on the point estimate only (the estimate for the tagging mortality of the new method must be between 0.2 and 0.3 for a successful study). Change your function to calculate this additional metric and re-run the analysis:

```{r, fig.height = 4}
sim_fit = function(n, p_old = 0.10, p_new = 0.25) {
  # create the data
  dead_old = rbinom(n, size = 1, prob = p_old)
  dead_new = rbinom(n, size = 1, prob = p_new)
  # create the predictor variable
  method = rep(c("old", "new"), each = n)
  # create a data.frame to pass to glm
  df = data.frame(dead = c(dead_old, dead_new), method = method)
  # relevel so old is the reference
  df$method = relevel(df$method, ref = "old")
  # fit the model
  fit = glm(dead ~ method, data = df, family = binomial)
  # extract the p-value
  pval = summary(fit)$coef[2,4]
  # determine if it was found to be significant
  sig_pval = pval < 0.05
  # obtain the estimated mortality rate for the new method
  p_new_est = predict(fit, data.frame(method = c("new")),
                      type = "response")
  
  # determine if it is +/- 5% from the true value
  prc_est = p_new_est >= (p_new - 0.05) & p_new_est <= (p_new + 0.05)
  # return a vector with these two elements
  c(sig_pval = sig_pval, prc_est = unname(prc_est))
}

# containers: 
out_sig = matrix(NA, I, N) # matrix with I rows and N columns
out_prc = matrix(NA, I, N) # matrix with I rows and N columns
for (n in 1:N) {
  for (i in 1:I) {
    tmp = sim_fit(n = n_try[n])     # run sim
    out_sig[i,n] = tmp["sig_pval"]  # extract and store significance metric
    out_prc[i,n] = tmp["prc_est"]   # extract and store precision metric
  }
}

par(mfrow = c(1,2), mar = c(4,4,1,0))
plot(apply(out_sig, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of Finding Effect (Power)")
plot(apply(out_prc, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of a Precise Estimate")
```

It seems that even if you tagged `r n_try[N]` fish per treatment, you would have a `r round(mean(out_prc[,N]),2) * 100`% chance of estimating that the mortality rate is between 0.2 and 0.3 if it was truly 0.25.

You and your colleagues consider these results and determine that you will need to somehow acquire more funds to tag more fish in the small-scale study in order to have a high level of confidence in the results.

### Harvest Policy Analysis {#harv-ex}

In this example, you will simulate population dynamics under a more realistic model than in Sections \@ref(for-loops) and \@ref(adv-funcs) for the purpose of evaluating different harvest policies. 

Suppose you are a fisheries research biologist, and a commercial fishery for pink salmon (_Oncorhynchus gorbuscha_) takes place in your district. For the past 10 years, it has been fished with an exploitation rate of 40% (40% of the fish that return each year have been harvested, exploitation rate is abbreviated by $U$), resulting in an average annual harvest of 8.5 million fish. The management plan is up for evaluation this year, and your supervisor has asked you to prepare an analysis that determines if more harvest could be sustained if a different exploitation rate were to be used in the future.

Based on historical data, your best understanding implies that the stock is driven by Ricker spawner-recruit dynamics. That is, the total number of fish that return this year (recruits) is a function of the total number of fish that spawned (spawners) in the year of their birth. The Ricker model can be written this way:

\begin{equation}
  R_t = \alpha S_{t-1} e^{-\beta S_{t-1} + \varepsilon_t} ,\varepsilon_t \sim N(0,\sigma)
(\#eq:ricker-ch4)
\end{equation}

where $\alpha$ is a parameter representing the maximum recruits per spawner (obtained at very low spawner abundances) and $\beta$ is a measure of the strength of density-dependent mortality. Notice that the error term is in the exponent, which makes $e^{\varepsilon_t}$ lognormal.

You have estimates of the parameters^[In reality, these estimates would have substantial uncertainty that you would need to propagate through your harvest policy analysis. In this example, you will ignore this complication]:

*  $\alpha = 6$
*  $\beta = 1 \times 10^{-7}$
*  $\sigma = 0.4$

You decide that you can build a policy analysis by simulating the stock forward through time under different exploitation rates. With enough iterations of the simulation, you will be able to see whether a different exploitation rate can provide more harvest than what is currently being extracted.

First, write a function for your population model. Your function must:

1.  take the parameters, dimensions (number of years), and the policy variable ($U$) as input arguments
2.  simulate the population using Ricker dynamics
3.  calculate and return the average harvest and escapement over the number of future years you simulated.

```{r}
# Step #1: name the function and give it some arguments
ricker_sim = function(ny, params, U) {
  # extract the parameters out by name:
  alpha = params["alpha"]
  beta = params["beta"]
  sigma = params["sigma"]
  # create containers
       # this is a neat trick to condense your code:
  R = S = H = NULL
  # initialize the population in the first year
    # start the population at being fished at 40%
    # with lognormal error
  R[1] = log(alpha * (1 - 0.4))/(beta * (1 - 0.4)) * exp(rnorm(1, 0, sigma))
  S[1] = R[1] * (1 - U)
  H[1] = R[1] * U
  
  # carry simulation forward through time
  for (y in 2:ny) {
    # use the ricker function with random lognormal noise
    R[y] = S[y-1] * alpha * exp(-beta * S[y-1] + rnorm(1, 0, sigma))
    #harvest and spawners are the same as before
    S[y] = R[y] * (1 - U)
    H[y] = R[y] * U
  }
  # wrap output in a list object
  list(
    mean_H = mean(H),
    mean_S = mean(S)
    )
}
```

Use the function once:

```{r}
params = c(alpha = 6, beta = 1e-7, sigma = 0.4)
out = ricker_sim(U = 0.4, ny = 20, params = params)
#average annual harvest (in millions)
round(out$mean_H/1e6, digits = 2)
```

If you completed the stochastic power analysis example (Section \@ref(power-ex)), you might see where this is going. You are going to replicate applying a fixed policy many times to a random system. This is the Monte Carlo part of the analysis. The policy part is that you will compare the output from several candidate exploitation rates to inform a decision about which is best. This time, set up your analysis using `sapply()` (to iterate over different values of $U$) and `replicate()` (to iterate over different random populations fished at each $U$) instead of performing a nested `for()` loop as in previous examples:

```{r, eval = F}
U_try = seq(0.4, 0.6, 0.01)
n_rep = 2000
H_out = sapply(U_try, function(u) {
  replicate(n = n_rep, expr = {
    ricker_sim(U = u, ny = 20, params = params)$mean_H/1e6
  })
})
```

```{r, echo = F}
U_try = seq(0.4, 0.6, 0.01)
n_rep = sra_n_rep
H_out = sapply(U_try, function(u) {
  replicate(n = n_rep, expr = {
    ricker_sim(U = u, ny = 20, params = params)$mean_H/1e6
  })
})
```

The nested `replicate()` and `sapply()` method is a bit cleaner than a nested `for()` loop, but you have less control over the format of the output.

Plot the output of your simulations using a boxplot. To make things easier, give `H_out` column names representing the exploitation rate:

```{r, eval = F}
colnames(H_out) = U_try
boxplot(H_out, outline = F,
        xlab = "U", ylab = "Harvest (Millions of Fish)",
        col = "tomato", las = 1)
```

```{r, echo = F}
par(sp)
colnames(H_out) = U_try
boxplot(H_out, outline = F,
        xlab = "U", ylab = "Harvest (Millions of Fish)",
        col = "tomato", las = 1)
```

It appears the stock could produce more harvest than its current 8.5 million fish per year if it was fished harder. However, your supervisors also do not want to see the escapement drop below three-quarters of what it has been in recent history (75% of approximately 13 million fish). They ask you to obtain the expected average annual escapement as well as harvest. You can simply re-run the code above, but extracting `S_mean` rather than `H_mean`. Call this output `S_out` and plot it just like harvest (if you're curious, this blue color is `col = "skyblue"`):

```{r, echo = F}
S_out = sapply(U_try, function(u) {
  replicate(n = n_rep, expr = {
    ricker_sim(U = u, ny = 20, params = params)$mean_S/1e6
  })
})

par(sp)
colnames(S_out) = U_try
boxplot(S_out, outline = F,
        xlab = "U", ylab = "Escapement (Millions of Fish)",
        col = "skyblue", las = 1)
```

After seeing this information, your supervisor realizes they are faced with a trade-off: the stock could produce more with high exploitation rates, but they are concerned about pushing the stock too low would be unsustainable. They tell you to determine the probability that the average escapement would not be pushed below 75% of 13 million at each exploitation rate, as well as the probability that the average annual harvests will be at least 20% greater than they are currently (approximately 8.5 million fish). Given your output, this is easy:

```{r}
# determine if each element meets escapement criterion
Smeet = S_out > (0.75 * 13)
# determine if each element meets harvest criterion
Hmeet = H_out > (1.2 * 8.5)
# calculate the probability of each occuring at a given exploitation rate
  # remember, mean of a logical vector calculate the proportion of TRUEs
p_Smeet = apply(Smeet, 2, mean)
p_Hmeet = apply(Hmeet, 2, mean)
```

You plot this for your supervisor as follows:

```{r, fig.height = 4, fig.width = 5}
# the U levels to highlight on plot
plot_U = seq(0.4, 0.6, 0.05)
# create an empty plot
par(mar = c(4,4,1,1))
plot(p_Smeet ~ p_Hmeet, type = "n",
     xlab = "Probability of Meeting Harvest Criterion",
     ylab = "Probability of Meeting Escapement Criterion")
# add gridlines
abline(v = seq(0, 1, 0.1), col = "grey")
abline(h = seq(0, 1, 0.1), col = "grey")
#draw on the tradeoff curve
lines(p_Smeet ~ p_Hmeet, type = "l", lwd = 2)
# add points and text for particular U policies
points(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U],
       pch = 16, cex = 1.5)
text(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U],
     labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2))
```

Equipped with this analysis, your supervisor plans to go to the policy-makers with the recommendation of adjusting the exploitation rate policy to use $U = 0.5$, because they think it balances the trade-off. Notice how if the status quo was maintained, your model suggests you would have complete certainty of staying where you are now: escapement will remain above 75% of its current level with a 100% chance, but you would have no chance of improving harvests to greater than 20% of their current level. Small increases in the exploitation rate (e.g., from 0.4 to 0.45) have a reasonably large gain in harvest performance, but hardly any losses for the escapement criterion. Your supervisor is willing to live with a 90% chance that the escapement will stay where they desire in order to gain a >80% chance of obtaining the desired amount of increases in harvest.

The utility of using Monte Carlo methods in this example is the ability to calculate the probability of some event you are interested in. There are analytical (i.e., not simulation-based) solutions to predict the annual harvest and escapement from a fixed $U$ from a population with parameters $\alpha$ and $\beta$, but by incorporating randomness, you were able to obtain the relative weights of outcomes other than the expectation under the deterministic Ricker model, thereby allowing the assignment of probabilities to meeting the two criteria.

## Resampling-Based Examples {#resample-examples}

### The Bootstrap {#boot-test-ex}

Say you have a fitted model from which you want to propagate the uncertainty in some derived quantity. Consider the case of the **von Bertalanffy growth model**. This is a non-linear model used to predict the size of an organism (weight or length) based on its age. The model can be written for a non-linear regression model (see Section \@ref(nls)) as:

\begin{equation}
  L_i = L_{\infty}\left(1 - e^{-k(age_i-t_0)}\right) + \varepsilon_i, \varepsilon_i \sim N(0, \sigma)
(\#eq:vonB)
\end{equation}

where $L_i$ and $age_i$ are the observed length and age of individual $i$, respectively, and $L_{\infty}$, $k$, and $t_0$ are parameters to be estimated. The interpretations of the parameters are as follows:

*  $L_{\infty}$: the maximum average length achieved
*  $k$: a growth coefficient linked to metabolic rate. It specifies the rate of increase in length as the fish ages early in life
*  $t_0$: the theoretical age when length equals zero (the x-intercept).

Use the data set `growth.csv` for this example (see the [instructions](#data-sets) on acquiring data files). Read in and plot the data:

```{r, eval = F}
dat = read.csv("../Data/growth.csv")
plot(length ~ age, data = dat, pch = 16, col = "grey")
```

```{r, echo = F}
dat = read.csv("Data/growth.csv")
par(sp)
plot(length ~ age, data = dat, pch = 16, col = "grey")
```

Due to a large amount of variability in individual growth rates, the relationship looks pretty noisy. Notice how you have mostly young fish in your sample: this is characteristic of "random" sampling of fish populations. 

Suppose you would like to obtain the probability that an average-sized fish of each age is sexually mature. You know that fish of this species mature at approximately 450 mm, and you simply need to determine the fraction of all fish at each age that are greater than 450 mm. However, you don't have any observations for some ages (e.g., age 8), so you cannot simply calculate this fraction based on your raw data. You need to fit the von Bertalanffy growth model, then carry the statistical uncertainty from the fitted model forward to the predicted length-at-age. This would be difficult to obtain using only the coefficient estimates and their standard errors, because of the non-linear relationship between the $x$ and $y$ variables.

Enter the **bootstrap**, which is a Monte Carlo analysis using an observed data set and a model. The **pseudocode** for a bootstrap analysis is:

1.  Resample from the original data (with replacement)
2.  Fit a model of interest 
3.  Derive some quantity of interest from the fitted model
4.  Repeat steps 1 - 3 many times
5.  Summarize the randomized quantities from step 4

In this example, you will apply a bootstrap approach to obtain the distribution of expected fish lengths at each age, then use these distributions to quantify the probability that an averaged-sized fish of each age is mature (i.e., greater than 450 mm).

You will write a function for each of steps 1 - 3 above. The first is to resample the data:

```{r}
randomize = function(dat) {
  # number of observed pairs
  n = nrow(dat)
  # sample the rows to determine which will be kept
  keep = sample(x = 1:n, size = n, replace = T)
  # retreive these rows from the data
  dat[keep,]
}
```

Notice the use of `replace = T` here: without this, there would be no bootstrap. You would just sample the same observations over and over, their order in the rows would just be shuffled. Next, write a function to fit the model (revisit Section \@ref(nls) for more details on `nls()`):

```{r}
fit_vonB = function(dat) {
  nls(length ~ linf * (1 - exp(-k * (age - t0))),
      data = dat,
      start = c(linf = 600, k = 0.3, t0 = -0.2)
      )
}
```

This function will return a fitted model object when executed. Next, write a function to predict mean length-at-age:

```{r}
# create a vector of ages
ages = min(dat$age):max(dat$age)
pred_vonB = function(fit) {
  # extract the coefficients
  ests = coef(fit)
  # predict length-at-age
  ests["linf"] * (1 - exp(-ests["k"] * (ages - ests["t0"])))
}
```

Notice your function will use the object `ages` even though it was not defined in the function. This has to do with **lexical scoping** and **environments**, which are beyond the scope of this introductory material. If you'd like more details, see the section in @adv-r-cite on it^[The section on **lexical scoping** is found here: <http://adv-r.had.co.nz/Functions.html#lexical-scoping>]. Basically, if an object with the same name as one defined in the function exists outside of the function, the function will use the one that is defined within the function. If there is no object defined in the function with that name, it will look outside of the function for that object. 

Now, use these three functions to perform one iteration:

```{r, eval = F}
pred_vonB(fit = fit_vonB(dat = randomize(dat = dat)))
```

You can wrap this inside of a `replicate()` call to perform step 4 above:

```{r}
set.seed(2)
out = replicate(n = 100, expr = {
  pred_vonB(fit = fit_vonB(dat = randomize(dat = dat)))
})

dim(out)
```

It appears the rows are different ages and the columns are different bootstrapped iterations. Summarize the random lengths at each age:

```{r}
summ = apply(out, 1, function(x) c(mean = mean(x), quantile(x, c(0.025, 0.975))))
```

Plot the data, the summarized ranges of mean lengths, and the length at which all fish are assumed to be mature (450 mm)

```{r, eval = F}
plot(length ~ age, data = dat, col = "grey", pch = 16,
     ylim = c(0, max(dat$length, summ["97.5%",])),
     ylab = "Length (mm)", xlab = "Age (years)")
lines(summ["mean",] ~ ages, lwd = 2)
lines(summ["2.5%",] ~ ages, col = "grey")
lines(summ["97.5%",] ~ ages, col = "grey")
abline(h = 450, col = "blue")
```

```{r, echo = F}
par(sp)
plot(length ~ age, data = dat, col = "grey", pch = 16,
     ylim = c(0, max(dat$length, summ["97.5%",])),
     ylab = "Length (mm)", xlab = "Age (years)")
lines(summ["mean",] ~ ages, lwd = 2)
lines(summ["2.5%",] ~ ages, col = "grey")
lines(summ["97.5%",] ~ ages, col = "grey")
abline(h = 450, col = "blue")
```

Obtain the fraction of iterations that resulted in the mean length-at-age being greater than 450 mm. This is interpreted as the probability that the average-sized fish of each age is mature:

```{r, eval = F}
p_mat = apply(out, 1, function(x) mean(x > 450))
plot(p_mat ~ ages, type = "b", pch = 17,
     xlab = "Age (years)", ylab = "Probability of Average Fish Mature")
```

```{r, echo = F}
par(sp)
p_mat = apply(out, 1, function(x) mean(x > 450))
plot(p_mat ~ ages, type = "b", pch = 17,
     xlab = "Age (years)", ylab = "Probability of Average Fish Mature")
```

This **maturity schedule** can be used by fishery managers in attempting to decide which ages should be allowed to be harvested and which should be allowed to grow more^[possibly in a **yield-per-recruit** analysis]. Because each age has an associated expected length, managers can use what they know about the size selectivity of various gear types to set policies that attempt to target some ages more than others.

### Permutation Test {#perm-test-ex}

In the previous example (Section \@ref(boot-test-ex)), you learned about the bootstrap. A related Monte Carlo analysis is the **permutation test**. This is a non-parametric statistical test used to determine if there is a statistically-significant difference in the mean of some quantity between two populations. It is used in cases where the assumptions of a generalized linear model may not be met, but a p-value is still required.

The **pseudocode** for the permutation test is:

1.  Calculate the difference between means based on the original data set
2.  Shuffle the group assignments randomly among the observations
3.  Calculate the difference between the randomly-assigned groups
4.  Repeat steps 2 - 3 many times. This builds the **null distribution**: the distribution of the test statistic (the difference) assuming the null hypothesis (that there is no difference) in means is true
5.  Determine what fraction of the absolute differences were larger than the original difference. This constitutes a **two-tailed** p-value. One-tailed tests can also be derived using the same steps 1 - 4, which is left as an exercise.

Use the data set `ponds.csv` for this example (see the [instructions](#data-sets) on acquiring data files). This is the same data set used for [Exercise 1B](#ex1b), revisit that exercise for details on this hypothetical data set. Read in and plot the data:

```{r, eval = F}
dat = read.csv("ponds.csv")
plot(chl.a ~ treatment, data = dat)
```

```{r, echo = F}
dat = read.csv("Data/ponds.csv")
par(sp)
plot(chl.a ~ treatment, data = dat)
```

It appears as though there is a relatively strong signal indicating a difference. Use the permutation test to determine if it is statistically significant. Step 1 from the pseudocode is to calculate the observed difference between groups:

```{r}
Dobs = mean(dat$chl.a[dat$treatment == "Add"]) - mean(dat$chl.a[dat$treatment == "Control"])
Dobs
```

Write a function to perform one iteration of steps 2 - 3 from the pseudocode:

```{r}
# x is the group: Add or Control
# y is chl.a
perm = function(x, y) {
  # turn x to a character, easier to deal with
  x = as.character(x)
  # shuffle the x values:
  x_shuff = sample(x)
  # calculate the mean of each group:
  x_bar_add = mean(y[x_shuff == "Add"])
  x_bar_ctl = mean(y[x_shuff == "Control"])
  # calculate the difference:
  x_bar_add - x_bar_ctl
}
```

Use your function once:

```{r}
perm(x = dat$treatment, y = dat$chl.a)
```

Perform step 4 from the pseudocode by replicating your `perm()` function many times:

```{r}
Dnull = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$chl.a))
```

Plot the distribution of the null test statistic and draw a line where the originally-observed difference falls:

```{r, eval = F}
hist(Dnull, col = "grey")
abline(v = Dobs, col = "blue", lwd = 3, lty = 2)
```

```{r, echo = F}
par(sp)
hist(Dnull, col = "grey")
abline(v = Dobs, col = "blue", lwd = 3, lty = 2)
```

Notice the null distribution is centered on zero: this is because the null hypothesis is that there is no difference. The observation (blue line) falls way in the upper tail of the null distribution, indicating it is unlikely that an effect that large was observed by random chance. The two-tailed p-value can be calculated as:

```{r}
mean(abs(Dnull) >= Dobs)
```

Very few (or zero) of the random data sets resulted in a difference greater than what was observed, indicating there is statistical support to the hypothesis that there is a non-zero difference between the two nutrient treatments.

## Population dynamics

```{r}
source("./R/Rcode/Final_report_Davidson2017.R", echo = TRUE)

```

### Plot steps

```{r}
## ----raw graph, echo=FALSE, message=FALSE, warning=FALSE-----------------
#plot data
ggplot(sum.dat, aes(y = mY, x = year)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = low.qt, ymax = high.qt), width = 0.1) +
  theme_bw()


# ## ----raw graph 2, echo=FALSE, fig.height=6, fig.width=6, message=FALSE, warning=FALSE----
# 
# #PLOTS
# par(mfrow=c(2,2))
# 
# plot(factor(year2010),xlim=c(0,6),ylim=c(0,40))
# title(main="a)",sub="Sample size 3", ylab="Frequency",xlab="Calving interval",
#       cex.main = 1.5,   font.main= 4, col.main= "blue",
#       cex.sub = 1, font.sub = 3, col.sub = "red")
# box()
# 
# plot(factor(year2011),xlim=c(0,6),ylim=c(0,40))
# title(main="b)",sub="Sample size 15", ylab="Frequency",xlab="Calving interval",col.main=4,cex.main = 1.5,   font.main= 4, col.main= "blue",
#       cex.sub = 1, font.sub = 3, col.sub = "red")
# box()
# 
# plot(factor(year2012),xlim=c(0,6),ylim=c(0,40))
# title(main="c)",sub="Sample size 25", ylab="Frequency",xlab="Calving interval",col.main=4,cex.main = 1.5,   font.main= 4, col.main= "blue",
#       cex.sub = 1, font.sub = 3, col.sub = "red")
# box()
# 
# plot(factor(year2013),xlim=c(0,6),ylim=c(0,40))
# title(main="d)",sub="Sample size 45", ylab="Frequency",xlab="Calving interval",col.main=4,cex.main = 1.5,   font.main= 4, col.main= "blue",
#       cex.sub = 1, font.sub = 3, col.sub = "red")
# box()
# 
# 
# 
# ## ----raw graph 3, echo=FALSE, fig.height=6, fig.width=6, message=TRUE, warning=TRUE----
# library(qpcR)
# #data in one way for plot
# rawdata <- qpcR:::cbind.na(year2010,year2011,year2012,year2013)
# rawdata <- as.data.frame(rawdata)
# 
# #in correct format for ggplot2
# year2010 <- data.frame(year2010,year = c("2010"))
# year2010 <- rename(year2010, interval = year2010, year = year )
# year2011 <- data.frame(year2011,year = c("2011"))
# year2011 <- rename(year2011, interval = year2011, year = year )
# year2012 <- data.frame(year2012,year = c("2012"))
# year2012 <- rename(year2012, interval = year2012, year = year )
# year2013 <- data.frame(year2013,year = c("2013"))
# year2013 <- rename(year2013, interval = year2013, year = year )
# ggplotraw <- rbind(year2010,year2011,year2012, year2013)
# ggplotraw$interval <- as.numeric(as.character(ggplotraw$interval))
# 
# #sort(year2013$interval) - sort(sample.true)
# 
# 
# ggplot(year2013,aes(x = interval)) +
#     geom_bar(alpha = 1, width = 0.9,fill = "black") +
#     xlab(expression("Calving"~"interval"~(italic("years")))) +
#     ylab(expression("Total"~"number"~"of"~"observations"~(italic("n")))) +
#     scale_y_continuous(breaks = c(0,5,10,15,20,25,30), limits = c(0,30)) +
#     theme(axis.line = element_line(colour = 'black', size = 0.65),
#           axis.ticks = element_line(colour = "black", size = 0.65),
#           panel.border = element_blank(),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           legend.key = element_blank(),
#           strip.background = element_rect(fill = "white", colour = "black", size = 1),
#           panel.background = element_rect(fill = "white",
#             colour = NA),
#           axis.text = element_text(size = rel(0.8),
#             colour = "black"))
# #PLOTS
# #code to store figure
# # png("Figure_2_NZSRW_calving_interval_2017_highres.png", width = 12, height = 14.8, units = 'cm', res = 1200)
# # dev.off()
# 
# 
# ## ----missing intervals, echo=FALSE, fig.height=10, message=FALSE, warning=FALSE----
# #################################Missing calving intervals################
# #Intervals modified by accounting for missed intervals
# #Bradford et al. 2008
# 
# #Raw Data
# RealCI <- as.numeric(year2013$interval)
# 
# #Confidence interval
# xlong <- RealCI
# meanlong<-sum(xlong)/length(xlong)
# slong<-sd(xlong)
# SElong<-slong/(sqrt(length(xlong)))
# nlong<-(length(xlong))
# #Standard error and confidence intervals
# #2 sided t value at the 95% level = 2.093
# lowqtlong <- meanlong-(qt(0.975,nlong)*SElong)
# highqtlong <- meanlong+(qt(0.975,nlong)*SElong)
# 
# ####################MED CI########################################
# # 2x 6's and 1x 5 replaced with 3threes
# MedCI <- c(RealCI[RealCI < 5],3,3,3,3,2,3)
# #sort(MedCI)
# xmed<-MedCI
# meanmed<-sum(xmed)/length(xmed)
# smed<-sd(xmed)
# SEmed<-smed/(sqrt(length(xmed)))
# nmed<-(length(xmed))
# 
# #Standard error and confidence intervals
# lowqtmed <- meanmed-(qt(0.975,length(xmed))*SEmed)
# highqtmed <- meanmed+(qt(0.975,length(xmed))*SEmed)
# 
# 
# ############################SHORT CI##################################
# #6,5 replaced with 2 year intervals
# 
# LowCI <- c(RealCI[RealCI < 4],3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2)
# xshort<-LowCI
# meanshort<-mean(xshort)
# sshort<-sd(xshort)
# SEshort<-sshort/(sqrt(length(xshort)))
# 
# #Standard error and confidence intervals
# lowqtshort <- meanshort-(qt(0.975,length(xshort))*SEshort)
# highqtshort <- meanshort+(qt(0.975,length(xshort))*SEshort)
# 
# bdata <-qpcR:::cbind.na(RealCI,MedCI,LowCI)
# bdata <- as.data.frame(bdata)
# 
# #Structure of data set
# #str(bdata)
# 
# 
# ## ----missing intervals plot, echo=FALSE, fig.height=3.5, fig.width=5.5, message=FALSE, warning=FALSE----
# #Basic plots
# par(mfrow=c(1,3))
# plot(factor(bdata$LowCI),main="Lowest possible interval")
# plot(factor(bdata$MedCI), main="Medium possible interval")
# plot(factor(bdata$RealCI),main="Observed interval")
# 
# 
# ## ----missing intervals plot2, fig.height=5.5, fig.width=4.5, message=FALSE, warning=FALSE, include=FALSE----
# #Density basic plots
# par(mfrow=c(3,1))
# plot(density(as.numeric(as.character(LowCI)),bw=.5), main="Lowest possible interval")
# plot(density(as.numeric(as.character(MedCI)),bw= 0.5), main="Medium possible interval")
# plot(density(as.numeric(as.character(RealCI)),bw = 0.5),main="Observed interval")
# 
# 
# ## ----missing intervals table, fig.height=8, message=FALSE, warning=FALSE, include=FALSE----
# 
# ###################################SUMMARY############################
# #Pull out important information
# Sumtable<-data.frame(variable = c("low.qt","mean","high.qt","sd", "SE"),                 short=c(lowqtshort,meanshort,highqtshort,sshort,SEshort),
#                      medium=c(lowqtmed,meanmed,highqtmed,smed,SEmed),
#                      real=c(lowqtlong,meanlong,highqtlong,slong,SElong))
# 
# #Make dataframe to plot
# n <- c(length(LowCI),length(MedCI),length(year2013$interval))
# mY <- c(mean(LowCI),mean(MedCI),mean(year2013$interval))
# interval <-c("Low", "Medium","Observed")
# low.qt <- c(lowqtshort,lowqtmed,low.qt2013)
# high.qt <- c(highqtshort,highqtmed,high.qt2013)
# sd <- c(sshort,smed,s2013)
# Sumtable <- cbind(interval,n,mY,low.qt,high.qt,sd)
# Sumtable <-  as.data.frame(Sumtable)
# 
#  Sumtable$n <- as.numeric(as.character(Sumtable$n))
#  Sumtable$mY <- as.numeric(as.character(Sumtable$mY))
#  Sumtable$low.qt <- as.numeric(as.character(Sumtable$low.qt))
#  Sumtable$high.qt <- as.numeric(as.character(Sumtable$high.qt))
#  Sumtable$sd <- as.numeric(as.character(Sumtable$sd))
#  Sumtable$interval <- as.character(Sumtable$interval)
# 
# 
# ## ----missing intervals plot3, echo=FALSE, fig.height=4, message=FALSE, warning=FALSE----
# ggplot(Sumtable, aes(y = mY, x = interval)) +
#   geom_point(size = 5) +
#   geom_errorbar(aes(ymin = low.qt, ymax = high.qt), width = 0.05,size = 1, alpha = 0.5) +
#   scale_y_continuous(breaks = round(seq(2.3, 3.6, by = 0.2),1)) +
#   labs(y = "Mean calving interval",x = "Calving interval modification" ) +
#   geom_point(size = 3) +
#   theme_classic() +
#   theme_hc() +
#   theme(legend.position="none")
# 
# 
# ## ----missing_data_table, echo=FALSE--------------------------------------
# library(knitr)
# 
# kable(Sumtable, format = "markdown",col.names = c("Interval","Sample size", "Mean", "Lower limit", "Higher limit", "SD"))
# 
# 
# 
# ## ----srw_data_table, echo=FALSE------------------------------------------
# library(knitr)
# setwd("C:/Users/s435389/R_packages/Davidson_2017_SRWrepro/Data")
# srwdat <- read.csv(file = "srw_data.csv")
# 
# #str(srwdat)
# kable(srwdat, format = "markdown",col.names = c("Sample size","Mean", "Lower limit", "Higher limit", "SE","Author", "Location"))
# 
# 
# 
# ## ----bootstrap single, echo=FALSE, fig.height=5--------------------------
# ############################NZ Simple sample##############################
# #WITH replacement
# 
# # to try and match number of intervals observed in other populations
# # find references
# SAreps <- 1500
# ARreps <- 800
# Aussiereps <- 2000
# low <- 1000
# verylow <- 100
# lowest <- 10
# 
# #Very raw plots
# par(mfrow=c(2,3))
# plot(factor(sample(year2013$interval,lowest,replace=T)),main = "3 intervals")
# plot(factor(sample(year2013$interval,verylow,replace=T)),main = "10 intervals")
# plot(factor(sample(year2013$interval,low,replace=T)),main = "30 intervals")
# plot(factor(sample(year2013$interval,Aussiereps,replace=T)),main = "500 intervals")
# plot(factor(sample(year2013$interval,ARreps,replace=T)),main = "800 intervals")
# plot(factor(sample(year2013$interval,SAreps,replace=T)),main = "1500 intervals")
# 
# 
# ## ----bootstrap_multiple, echo=FALSE--------------------------------------
# #do each one 1000 times
# boots <- 1000
# n <- c(1:1000)
# 
# 
# ###########################n10
# var10 <- paste0("n_", 1:10)
# sample10 <-matrix(data = NA, ncol = lowest, nrow = boots)
# colnames(sample10) <- as.list(var10)
# 
# for (i in 1:boots) {
#                     sample10 [i, ] <- sample(year2013$interval,lowest,replace=T)
#                         }  #i
# 
# sample10 <- as.data.frame(sample10)
# sample10 <- sample10 %>%
#             mutate(mean10 = rowMeans(sample10))
# 
# sample10t <- as.matrix(sample10)
# sample10t <-t(sample10t)
# 
# #########################verylow sample size
# #set up variable names
# var100 <- paste0("n_", 1:100)
# 
# sample100 <-matrix(data = NA, ncol = verylow, nrow = boots)
# colnames(sample100) <- as.list(var100)
# 
# for (i in 1:boots) {
#                     sample100 [i, ] <- sample(year2013$interval,verylow,replace=T)
#                         }  #i
# 
# sample100 <- as.data.frame(sample100)
# sample100 <- sample100 %>%
#             mutate(mean100 = rowMeans(sample100))
# 
# #########################middle one
# #set up variable names
# var500 <- paste0("n_", 1:500)
# 
# sample500 <-matrix(data = NA, ncol = 500, nrow = boots)
# colnames(sample500) <- as.list(var500)
# 
# for (i in 1:boots) {
#                     sample500 [i, ] <- sample(year2013$interval,500,replace=T)
#                         }  #i
# 
# sample500 <- as.data.frame(sample500)
# sample500 <- sample500 %>%
#             mutate(mean500 = rowMeans(sample500))
# 
# 
# #########################low sample size
# #set up variable names
# var1000 <- paste0("n_", 1:1000)
# 
# sample1000 <-matrix(data = NA, ncol = low, nrow = boots)
# colnames(sample1000) <- as.list(var1000)
# 
# for (i in 1:boots) {
#                     sample1000 [i, ] <- sample(year2013$interval,low,replace=T)
#                         }  #i
# 
# sample1000 <- as.data.frame(sample1000)
# sample1000 <- sample1000 %>%
#             mutate(mean1000 = rowMeans(sample1000))
# 
# #########################AUS sample size
# #set up variable names
# varA <- paste0("n_", 1:2000)
# 
# sampleA <-matrix(data = NA, ncol = Aussiereps, nrow =  boots)
# colnames(sampleA) <- as.list(varA)
# 
# for (i in 1:boots) {
#                     sampleA [i, ] <- sample(year2013$interval,Aussiereps,replace=T)
#                         }  #i
# 
# sampleA <- as.data.frame(sampleA)
# sampleA <- sampleA %>%
#             mutate(meanA = rowMeans(sampleA))
# 
# sampleAt <- t(sampleA)
# 
# for(i in c(1:ncol(sampleA))) {
#     sampleA[,i] <- as.numeric(as.character(sampleA[,i]))
# }
# 
# 
# 
# 
# #COnfidence intervals
# 
# ab <- sort(sampleA$meanA)
# nab <- length(ab)
# #low = 25/1000
# ab2.5 <- ab[25]
# #high = 975/1000
# ab0.97.5 <- ab[975]
# 
# ab <- sort(sampleA$meanA)
# nab <- length(ab)
# #low = 25/1000
# ab2.5 <- ab[25]
# #high = 975/1000
# ab0.97.5 <- ab[975]
# 
# 
# 
# ## ----bootstrap plot2, fig.height=5, message=FALSE, warning=FALSE, include=FALSE----
# #plot the data over each other to look at change in density
# par(mfrow=c(1,1))
# #plot(density(sample3$mean3,bw = .15),lwd = 3,lyt = 5, main = "", xlab = "Calving interval", box = FALSE,axis = FALSE)
# 
# plot(density(sample10$mean10,bw = .05),col ="black", lty = 1, main = "", lwd = 5,ylim = c(0,8),xlim = c(2,4.5), axes=FALSE,xlab = "Calving interval")
# lines(density(sample100$mean100,bw = .05),col ="black", lty = 2, lwd = 4)
# lines(density(sample500$mean500,bw = .05),col ="black", lty = 3, lwd = 3)
# lines(density(sample1000$mean1000,bw = .05),col ="black", lty = 4, lwd = 2)
# lines(density(sampleA$meanA,bw = .05),col ="black", lty = 5, lwd = 1)
# legend('topright',title = "Legend", c("n=10, cv=8.12 ", "n=100, cv=2.43", "n=500, c.v=1.15", "n=1000, cv=0.79", "n=2000, cv=0.56"),bty = "n",
#   lty = c(1,2,3,4,5), lwd = c(5,4,3,2,1), cex=.75)
# axis(1,lwd=2)
# axis(2,lwd=2)
# 
# 
# 
# ## ----final plot for publication1, echo=FALSE-----------------------------
# #final [plot]
# #size defined by NZJFMR
# #  195 mm (h) ? 148 mm (w).
# #ylab(expression("Total"~"number"~"of"~"observations"~(italic("n")))) +
# 
# plot(density(sample10$mean10,bw = .05),col ="black", lty = 3, main = "", lwd = 1,ylim = c(0,8),xlim = c(2.5,4.5), axes=FALSE, xlab = expression("Calving"~"interval"~(italic("years"))))
# lines(density(sample100$mean100,bw = .05),col ="black", lty = 4, lwd = 1)
# lines(density(sample500$mean500,bw = .05),col ="black", lty = 5, lwd = 1)
# lines(density(sample1000$mean1000,bw = .05),col ="black", lty = 2, lwd = 1)
# lines(density(sampleA$meanA,bw = .05),col ="black", lty = 1, lwd = 2)
# legend(y = 8, x = 3.9,title = expression(bold("Sample size (n)")), c(expression(italic("n")~"="~"10"), expression(italic("n")~"="~"100"), expression(italic("n")~"="~"500"), expression(italic("n")~"="~"1000"), expression(italic("n")~"="~"2000")),bty = "n",
#   lty = c(3,4,5,2,1), lwd = c(1,1,1,1,2), cex=1)
#  axis(1,lwd=2)
# axis(2,lwd=2)
# 
# # PLOT CODE FOR PUBLICATION
# # png("C:/Users/s435389/R_packages/Davidson_2017_SRWrepro/Figures/Figure_3_NZSRW_calving_interval_2017_lowres.png", width = 14.8, height = 14.8, units = 'cm', res = 400)
# # dev.off()
# #
# #
# # png("C:/Users/s435389/R_packages/Davidson_2017_SRWrepro/Figures/Figure_3_NZSRW_calving_interval_2017_highres.png", width = 14.8, height = 14.8, units = 'cm', res = 1200)
# #
# # plot(density(sample10$mean10,bw = .05),col ="black", lty = 3, main = "", lwd = 1,ylim = c(0,8),xlim = c(2.5,4.5), axes=FALSE,xlab = expression("Calving"~"interval"~(italic("years"))))
# # lines(density(sample100$mean100,bw = .05),col ="black", lty = 4, lwd = 1)
# # lines(density(sample500$mean500,bw = .05),col ="black", lty = 2, lwd = 1)
# # lines(density(sample1000$mean1000,bw = .05),col ="black", lty = 5, lwd = 1)
# # lines(density(sampleA$meanA,bw = .05),col ="black", lty = 1, lwd = 2)
# # legend(y = 8, x = 3.9,title = expression(bold("Sample size (n)")), c(expression(italic("n")~"="~"10"), expression(italic("n")~"="~"100"), expression(italic("n")~"="~"500"), expression(italic("n")~"="~"1000"), expression(italic("n")~"="~"2000")),bty = "n",
# #   lty = c(3,4,2,5,1), lwd = c(1,1,1,1,2), cex=1)
# # axis(1,lwd=2)
# # axis(2,lwd=2)
# #
# # dev.off()
# 
# 
# ## ----referee_comment_1, echo=TRUE----------------------------------------
# #observed sample
# rev.one <- bdata$RealCI[1:45]
# 
# #sample 45 times
# sample.true <- year2013$interval
# 
# #power analysis
# pwr.test.results <- power.t.test(n = 45,# sample size
#              delta = seq(0,0.99,0.001),  #difference between means
#              sd = sd(sample.true),      #observed variation
#              alternative = "one.sided", #observed test type
#              sig.level = 0.05)          #significance level
# 
# #additional packages are avaliable for more complex analysis
# #but have not done this as don't think it is needed
# 
# 
# 
# ## ----referee_comment_1_plot, echo=FALSE, message=FALSE, warning=FALSE----
# #sort data into ggplot format
# pwr.analysis <- as.data.frame(cbind(
#                               pwr.test.results$power,
#                               pwr.test.results$delta))
# 
# colnames(pwr.analysis) <- c("Power","Mean.difference")
# 
# #sort data into ggplot format
# pwr.analysis.1 <- pwr.analysis %>%
#   mutate(Alpha = 1- Power,
#          Mean.estimate = 3.31 + Mean.difference)
# # %>%
# #   select(Alpha,Mean.estimate)
# 
# #work out where the cut-off is
# a <- filter(pwr.analysis.1, Alpha < 0.05)
# a[1,]
# 
# #plot data
# ggplot(data = pwr.analysis.1, aes(x = Mean.estimate, y = Alpha)) +
#   geom_line(size = 1.5) +
#   geom_vline(xintercept = 3.903, col = "blue") +
#   geom_hline(yintercept = 0.05) +
#   theme(axis.line = element_line(colour = 'black', size = 0.65),
#           axis.ticks = element_line(colour = "black", size = 0.65),
#           panel.border = element_blank(),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           legend.key = element_blank(),
#           strip.background = element_rect(fill = "white", colour = "black", size = 1),
#           panel.background = element_rect(fill = "white",
#             colour = NA),
#           axis.text = element_text(size = rel(0.8),
#             colour = "black")) +
#   ggtitle("Raw data result plot (n = 45)")
# 
# 
# 
# 
# 
# ## ----referee_comment_2_plot, echo=FALSE, message=FALSE, warning=FALSE----
# #observed sample
# rev.one <- bdata$RealCI[1:45]
# 
# #sample 45 times
# sample.true <- year2013$interval
# 
# #difference
# diff <- 3.63-3.31  #observed mean of australian population
# 
# #power analysis
# pwr.test.results <- power.t.test(n = seq(1,200,1),# sample size
#              delta = diff,  #difference between means
#              sd = sd(sample.true),      #observed variation
#              alternative = "one.sided", #observed test type
#              sig.level = 0.05)          #significance level
# 
# #additional packages are avaliable for more complex analysis
# #but have not done this as don't think it is needed
# 
# #sort data into ggplot format
# pwr.analysis <- as.data.frame(cbind(
#                               pwr.test.results$power,
#                               pwr.test.results$n))
# 
# colnames(pwr.analysis) <- c("Power","Sample.size")
# 
# #sort data into ggplot format
# pwr.analysis.1 <- pwr.analysis %>%
#   mutate(Alpha = 1- Power)
# # %>%
# #   select(Alpha,Mean.estimate)
# 
# #work out where the cut-off is
# a <- filter(pwr.analysis.1, Alpha < 0.05)
# a[1,]
# 
# #plot data
# ggplot(data = pwr.analysis.1, aes(x = Sample.size, y = Alpha)) +
#   geom_line(size = 1.5) +
#   geom_vline(xintercept = 45, col = "red") +
#   geom_vline(xintercept = 153, col = "blue") +
#   geom_hline(yintercept = 0.05) +
#   scale_y_continuous(limits = c(0,1)) +
#   theme(axis.line = element_line(colour = 'black', size = 0.65),
#           axis.ticks = element_line(colour = "black", size = 0.65),
#           panel.border = element_blank(),
#           panel.grid.major = element_blank(),
#           panel.grid.minor = element_blank(),
#           legend.key = element_blank(),
#           strip.background = element_rect(fill = "white", colour = "black", size = 1),
#           panel.background = element_rect(fill = "white",
#             colour = NA),
#           axis.text = element_text(size = rel(0.8),
#             colour = "black")) +
#    ggtitle("Observed difference between Australian and NZ mean")
# 
# 
# 
# ## ----missed individuals 1, echo=FALSE------------------------------------
# dat <- read.csv("C:/Users/s435389/R_packages/Davidson_2017_SRWrepro/Data/raw_observations_2012.csv")
# #data structure
# glimpse(dat)
# head(dat)
# #And the second dataset
# dat1<- read.csv("C:/Users/s435389/R_packages/Davidson_2017_SRWrepro/Data/RawCI.csv", header=T, quote="\"")
# #data structure
# glimpse(dat1)
# 
# 
# ## ----missed individuals 2, echo=FALSE, message=FALSE, warning=FALSE------
# ##I can then modify this data to
# #restructure dataset of capture to long dataset
# dat3 <- dplyr::select(dat, ID, X2006:X2012)%>%
#                gather(year, count,X2006:X2012)
# 
# #add data on calves
# dat4 <- full_join(dat3,dat1, by = "ID")
# dat5 <- dplyr::select(dat4,ID,year,count,Yr.first.seen,Calves,Calves.1,Calves.2)
# 
# dat6 <- filter(dat5,count >0)
# glimpse(dat6)
# 
# dat7 <- mutate(dat6, year = ifelse(year == "X2006","2006", year),
#             year = ifelse(year == "X2007","2007", year),
#             year = ifelse(year == "X2008","2008", year),
#             year = ifelse(year == "X2009","2009", year),
#             year = ifelse(year == "X2010","2010", year),
#             year = ifelse(year == "X2011","2011", year),
#             year = ifelse(year == "X2012","2012", year))
# 
# a <- group_by(dat7, ID, Yr.first.seen) %>%
#   mutate(mother = ifelse(Yr.first.seen > 0, 1, 0)) %>%
#   filter(mother == 1) %>%
#   ungroup() %>%
#   dplyr::select(ID,year,Calves,Calves.1) %>%
#   filter(Calves.1<2013) %>%
#   filter(!year == Calves) %>%
# filter(!year ==Calves.1)
# 
# a
# 
# 
# ## ----referee_comment3, echo=TRUE, message=FALSE, warning=FALSE-----------
# greater.than.2 <- sample.true[sample.true>2]
# 
# #greater.than.2
# mean.2<-sum(greater.than.2)/length(greater.than.2)
# s.2<-sd(greater.than.2)
# SE.2<-s2013/(sqrt(length(greater.than.2)))
# n.2<-length(greater.than.2)
# low.qt.2<- mean.2-(qt(0.975,length(greater.than.2))*SE.2)
# high.qt.2 <- mean.2+(qt(0.975,length(greater.than.2))*SE.2)
# 
# #add it to the table from bradford data
# Sumtable[4,] <- c("miss2year",n.2,mean.2,low.qt.2,
#                   high.qt.2,sd(greater.than.2))
# 
# 
# 
# ## ----different missing intervals 1, echo=TRUE----------------------------
# ########################### 2.2%
# #parameters
# boots <- 1000
# n <- c(1:1000)
# 
# ###round all percentages upwards
# detect1 <- 44  # (45*1.02) - 45 = 0.9
# detect2 <- 42   #  (45*1.05) - 45 = 2.25
# detect3 <- 40  # (45*1.10) - 45 = 4.5
# 
# sample2 <-rep(NA, 1000)
# sample5 <-rep(NA, 1000)
# sample10 <-rep(NA, 1000)
# 
# for (i in 1:boots) {
#                     sample2[i]<-mean(sample(year2013$interval,detect1,replace=T))
#                     sample5[i]<-mean(sample(year2013$interval,detect2,replace=T))
#                     sample10[i]<-mean(sample(year2013$interval,detect3,replace=T))
#                         }  #i
# 
# ######################estimates##############
# sample2 <- sort(sample2)
# #low = 25/1000
# sample2.2.5 <- sample2[25]
# #median
# sample2.50 <- sample2[500]
# #high = 975/1000
# sample2.975 <- sample2[975]
# 
# sample5 <- sort(sample5)
# #low = 25/1000
# sample5.2.5 <- sample5[25]
# #median
# sample5.50 <- sample5[500]
# #high = 975/1000
# sample5.975 <- sample5[975]
# 
# sample10 <- sort(sample10)
# #low = 25/1000
# sample10.2.5 <- sample10[25]
# #median
# sample10.50 <- sample10[500]
# #high = 975/1000
# sample10.975 <- sample10[975]
# 
# 
# #add it to the table from bradford data
# Sumtable[5,] <- c("detect1",detect1,sample2.50,sample2.2.5,sample2.975,NA)
# Sumtable[6,] <- c("detect2",detect2,sample5.50,sample5.2.5,sample5.975,NA)
# Sumtable[7,] <- c("detect5",detect3,sample10.50,sample10.2.5,sample10.975,NA)
# 
# 
# ## ----detection sim.2-----------------------------------------------------
# 
# #be very careful as Dat is just IDS and no id of females with calves
# #BUT Data is identified females...
# length(Data$ID)
# length(dat$ID)
# 
# 
# 
# glimpse(Data)
# dat.detect <- dplyr::select(Data,ID,Calves,Calves.1, Calves.2) %>%
#                   mutate(Calves = factor(Calves),
#                          Calves.1 = factor(Calves.1),
#                          Calves.2 = factor(Calves.2))
# 
# a <- as.data.frame.matrix(table(Data$ID,Data$Calves))
# head(a)
# a[,7] <-row.names(a)
# colnames(a)[1] <- "y2006"
# colnames(a)[2] <- "y2007"
# colnames(a)[3] <- "y2008"
# colnames(a)[4] <- "y2009"
# colnames(a)[5] <- "y2010"
# colnames(a)[6] <- "y2011"
# colnames(a)[7] <- "ID"
# a[,8] <- 0
# colnames(a)[8] <- "y2012"
# a[,9] <- 0
# colnames(a)[9] <- "y2013"
# a <- dplyr::select(a,ID,y2006,y2007,y2008, y2009, y2010, y2011, y2012, y2013)
# 
# 
# b <- as.data.frame.matrix(table(Data$ID,Data$Calves.1))
# head(b)
# b[,5] <-row.names(b)
# colnames(b)[5] <- "ID"
# b[,6] <- 0
# colnames(b)[6] <- "y2006"
# b[,7] <- 0
# colnames(b)[7] <- "y2007"
# b[,8] <- 0
# colnames(b)[8] <- "y2008"
# b[,9] <- 0
# colnames(b)[9] <- "y2009"
# colnames(b)[1] <- "y2010"
# colnames(b)[2] <- "y2011"
# colnames(b)[3] <- "y2012"
# colnames(b)[4] <- "y2013"
# b <- dplyr::select(b,ID,y2006,y2007,y2008, y2009, y2010, y2011, y2012, y2013)
# 
# 
# c <- as.data.frame.matrix(table(Data$ID,Data$Calves.2))
# head(c)
# colnames(c)[1] <- "y2013"
# c[,2] <-row.names(c)
# colnames(c)[2] <- "ID"
# c[,3] <- 0
# colnames(c)[3] <- "y2006"
# c[,4] <- 0
# colnames(c)[4] <- "y2007"
# c[,5] <- 0
# colnames(c)[5] <- "y2008"
# c[,6] <- 0
# colnames(c)[6] <- "y2009"
# c[,7] <- 0
# colnames(c)[7] <- "y2010"
# c[,8] <- 0
# colnames(c)[8] <- "y2011"
# c[,9] <- 0
# colnames(c)[9] <- "y2012"
# 
# c <- dplyr::select(c,ID,y2006,y2007,y2008, y2009, y2010, y2011, y2012,y2013)
# 
# countdat <- rbind(a,b,c)
# glimpse(countdat)
# head(full.dat)
# 
# full.dat <- group_by(countdat, ID) %>%
#   summarise(y2006 = sum(y2006),
#             y2007 = sum(y2007),
#             y2008 = sum(y2008),
#             y2009 = sum(y2009),
#             y2010 = sum(y2010),
#             y2011 = sum(y2011),
#             y2012 = sum(y2012),
#             y2013 = sum(y2013))
# 
# 2012-2006
# 
# ##checking....
# 
# sort(Data$ID)
# filter(Data, ID == "AI06022")
# filter(Data, ID == "AI08340")
# filter(Data, ID == "AI08343")
# 
# head(Data)
# 
# 
# # glimpse(c)
# # Data$Calves.1,
# # # Spread and gather are complements
# # df <- data.frame(x = c("a", "b"), y = c(3, 4), z = c(5, 6))
# # df %>% spread(x, y) %>% gather(x, y, a:b, na.rm = TRUE)
# 
# 
# 
# 
# ## ----different missing intervals 2---------------------------------------
# longer5.6 <- c(sample.true,5,6,6)
# 
# #greater.than.2
# mean.56<-sum(longer5.6)/length(longer5.6)
# s.56<-sd(longer5.6)
# SE.56<-s.56/(sqrt(length(longer5.6)))
# n.56<-(length(longer5.6))
# low.qt.56<- mean.56-(qt(0.975,length(longer5.6))*SE.56)
# high.qt.56 <- mean.56+(qt(0.975,length(longer5.6))*SE.56)
# 
# #add it to the table from bradford data
# Sumtable[8,] <- c("longer.56",n.56,mean.56,low.qt.56,high.qt.56,sd(longer5.6))
# 
# ###sort out numbering in dataframe
# Sumtable <-  as.data.frame(Sumtable)
# 
#  Sumtable$n <- as.numeric(as.character(Sumtable$n))
#  Sumtable$mY <- as.numeric(as.character(Sumtable$mY))
#  Sumtable$low.qt <- as.numeric(as.character(Sumtable$low.qt))
#  Sumtable$high.qt <- as.numeric(as.character(Sumtable$high.qt))
#  Sumtable$sd <- as.numeric(as.character(Sumtable$sd))
#  Sumtable$interval <- as.character(Sumtable$interval)
# 
# 
# ## ----missing_data_table 2, echo=FALSE------------------------------------
# library(knitr)
# 
# kable(Sumtable, format = "markdown",col.names = c("Interval","Sample size", "Mean", "Lower limit", "Higher limit", "SD"))
# 
# 
# 
# ## ----referee_comment3_plot, echo=FALSE-----------------------------------
# ggplot(Sumtable, aes(y = mY, x = interval)) +
#   geom_point(size = 5) +
#   geom_errorbar(aes(ymin = low.qt, ymax = high.qt), width = 0.05,size = 1, alpha = 0.5) +
#   scale_y_continuous(breaks = round(seq(2.3, 5, by = 0.2),1)) +
#   labs(y = "Mean calving interval",x = "Calving interval modification" ) +
#   geom_point(size = 3) +
#   theme_classic() +
#   theme_hc() +
#   theme(legend.position="none")


```

---

## Exercise 4

In these exercises, you will be adapting the code written in this chapter to investigate slightly different questions. You should create a new R script `Ex4.R` in your working directory for these exercises so your chapter code is left unchanged. Exercise 4A is based solely on the required material and Exercises 4B - 4F are based on the example cases. You should work through each example before attempting each of the later exercises.

_The solutions to this exercise are found at the end of this book ([here](#ex4a-answers)). You are **strongly recommended** to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped._

### Exercise 4A: Required Material Only {-}

These questions are based on the material in Sections \@ref(randomness) - \@ref(mc-summaries) only.

1.  Simulate flipping an unfair coin (probability of heads = 0.6) 100 times using `rbinom()`. Count the number of heads and tails.
2.  Simulate flipping the same unfair coin 100 times, but using `sample()` instead. Determine what fraction of the flips resulted in heads.
3.  Simulate rolling a fair 6-sided die 100 times using `sample()`. Determine what fraction of the rolls resulted in an even number.
4.  Simulate rolling the same die 100 times, but use the function `rmultinom()` instead. Look at the help file for details on how to use this function. Determine what fraction of the rolls resulted in an odd number.

[Solutions](#ex4a-answers)

### Exercise 4B: Test `rnorm` {-}

These questions will require you to adapt the code written in Section \@ref(rnorm-ex)

1.  Adapt this example to investigate another univariate probability distribution, like `-lnorm()`, `-pois()`, or `-beta()`. See the help files (e.g., `?rpois`) for details on how to use each function.

[Solutions](#ex4b-answers)

### Exercise 4C: Stochastic Power Analysis {-}

These questions will require you to adapt the code written in Section \@ref(power-ex)

1.  What sample size `n` do you need to have a power of 0.8 of detecting a significant difference between the two tagging methods?
2.  How do the inferences from the power analysis change if you are interested in `p_new = 0.4` instead of `p_new = 0.25`? Do you need to tag more or fewer fish in this case?
3.  Your analysis takes a bit of time to run so you are interested in tracking its progress. Add a progress message to your nested `for()` loop that will print the sample size currently being analyzed: 

```{r, eval = F}
for (n in 1:N) {
  cat("\r", "Sample Size = ", n_try[n])
  for (i in 1:I) {
    ...
  }
}
```

[Solutions](#ex4c-answers)

### Exercise 4D: Harvest Policy Analysis {-}

These questions will require you to adapt the code written in Section \@ref(harv-ex)

1.  Add an argument to `ricker_sim()` that will give the user an option to create a plot that shows the time series of recruitment, harvest, and escapement all on the same plot. Set the default to be to not plot the result, in case you forget to turn it off before performing the Monte Carlo analysis. 
2.  Add an _error handler_ to `ricker_sim()` that will cause the function to return an error `if()` the names of the vector passed to the `param` argument aren't what the function is expecting. You can use `stop("Error Message Goes Here")` to have your function stop and return an error.
3.  How do the results of the trade-off analysis differ if the process error was larger (a larger value of $\sigma$)?
4.  Add implementation error to the harvest policy. That is, if the target exploitation rate is $U$, make the real exploitation rate in year $y$ be: $U_y \sim Beta(a,b)$, where $a = 100U$ and $b = 100(1-U)$. You can make there be more implementation error by inserting a smaller number other than 100 here. How does this affect the trade-off analysis?

[Solutions](#ex4d-answers)

### Exercise 4E: The Bootstrap {-}

These questions will require you to adapt the code written in Section \@ref(boot-test-ex)

1.  Replicate the bootstrap analysis, but adapt it for the linear regression example in Section \@ref(regression). Stop at the step where you summarize the 95% interval range.
2.  Compare the 95% bootstrap confidence intervals to the intervals you get by running the `predict()` function on the original data set with the argument `interval = "confidence"`.

[Solutions](#ex4e-answers)

### Exercise 4F: Permutation Tests {-}

These questions will require you to adapt the code written in Section \@ref(perm-test-ex)

1.  Adapt the code to perform a permutation test for the difference in each of the zooplankton densities between treatments. Don't forget to fix the missing value in the `chao` variable. See [Exercise 2](#ex1b) for more details on this.
2.  Adapt the code to perform a permutation test for another data set used in this book where there are observations of both a categorical variable and a continuous variable. The data sets `sockeye.csv`, `growth.csv`, or `creel.csv` should be good starting points.
3.  Add a calculation of the p-value for a one-tailed test (i.e., that the difference in means is greater or less than zero). Steps 1 - 4 are the same: all you need is `Dnull` and `Dobs`. Don't be afraid to Google this if you are confused. 

[Solutions](#ex4f-answers)

<!--chapter:end:04-simulation.Rmd-->

# Large Data Manipulation {#ch5}

```{r, include = F}
rm(list = ls(all = T))
knitr::opts_chunk$set(fig.align = "center")

op = par(no.readonly = T)
par(mar = c(4,4,2,1))
sp = par(no.readonly = T)
```

```{r, echo = F, warning = F, message = F}
library(kableExtra)
library(knitr)
```

## Chapter Overview{-}

Any data analyst will tell you that oftentimes the most difficult part of an analysis is simply getting the data in the proper format. Real data sets are messy: they have missing values, variables are often stored in multiple columns that would be better stored as rows, the same factor level may be coded as two or more different types^[E.g., `"hatch"`, "`Hatch`", and `"HATCH"` are all treated as different factor levels in R], or the required data are in several separate files. You get the point: sometimes significant data-wrangling may be required before you perform an analysis.

In this chapter, you will learn tricks to do more advanced data manipulation in R using two **R packages**:

*  `{reshape2}` [@R-reshape2]: used for changing data between formats (wide and long)
*  `{dplyr}` [@R-dplyr]: used for large data manipulations with a consistent and readable format.

You will be turning daily observations of harvest and escapement (fish that are not harvested) into annual totals for the purpose of fitting a **spawner-recruit analysis** on a hypothetical pink salmon (_Oncorhynchus gorbuscha_) data set. More details and context on these terms and topics will be provided later. 

**IMPORTANT NOTE**: If you did not attend the sessions corresponding to Chapters \@ref(ch1) or \@ref(ch2), you are recommended to walk through the material found in those chapters before proceeding to this material. Additionally, you will find the material in Section \@ref(nls) helpful for the end of this chapter. Remember that if you are confused about a topic, you can use **CTRL + F** to find previous cases where that topic has been discussed in this book.

## Before You Begin {-}

You should create a new directory and R script for your work in this Chapter. Create a new R script called `Ch5.R` and save it in the directory `C:/Users/YOU/Documents/R-Book/Chapter5`. Set your working directory to that location. Revisit the material in Sections \@ref(scripts) and \@ref(working-dir) for more details on these steps.

## Aquiring and Loading Packages

An **R package** is a bunch of code, documentation, and data that someone has written and bundled into a consistent format for other R users to install and use in their R sessions. Packages make R incredibly flexible and extensible. If you are trying to do a specialized analysis that is not included in the base R distribution, it is likely that someone has already written a package that will allow you to do it!

In this chapter, you will be using some new packages that you likely don't already have on your computer, so you will need to install them first:

```{r, eval = F}
install.packages("dplyr")
install.packages("reshape2")
```

This requires an internet connection. You will see some text display in the console telling you the packages are being installed. Once the packages are installed, you need not re-install them in future sessions. However, if you install a new version of R, you will likely need to update your packages (i.e., re-install them).

Now that the packages are on your computer, you will need to load them into the current session:

```{r}
library(dplyr)
library(reshape2)
```

These messages are telling you that there are functions in the `{dplyr}` package that have the same names as those already being used in the `{stats}` and `{base}` R packages. This is fine so long as you don't want to use the original functions. If you wish to use the `{base}` version of the `filter()` function rather than the `{dplyr}` version, use it like this: `base::filter()`. Each time you close and reopen R, you will need to load any packages you want to use using `library()` again.

## The Data

You have daily observations of catch and escapement for every other year between 1917 and 2015. Pink salmon have a simple life history where fish that spawned in an odd year return as adults to spawn in the next odd year. There is a commercial fishery in this system located at the river mouth which harvests fish as they enter the river. A counting tower is located upstream of the fishing grounds that counts the number of fish that escaped the fishery and will have a chance to spawn (this number is termed "escapement"). The ultimate goal is to obtain annual totals of spawners and total run (catch + escapement) for the purpose of fitting a spawner recruit analysis. You will perform some other analyses along the way directed at quantifying patterns in run timing.

Read in the two data sets for this chapter (see the [instructions](#data-sets) for details on acquiring data files):

```{r, eval = F}
catch = read.csv("../Data/daily_catch.csv")
esc = read.csv("../Data/daily_escape.csv")
```

```{r, echo = F}
catch = read.csv("Data/daily_catch.csv")
esc = read.csv("Data/daily_escape.csv")
```

Look at the first 6 rows and columns of the `catch` data:

```{r}
catch[1:6,1:6]
```

The column `doy` is the day of the year that each record corresponds to, and each column represents a different year. Notice the format of the `esc` data is the same:

```{r}
esc[1:6,1:6]
```

But notice the first `doy` is one day later for `esc` than for `catch`. This is because of the time lag for fish to make it from the fishery grounds to the counting tower (a one day swim: fish that were in the fishing grounds on day `d` passed the counting tower on day `d+1` if they were not harvested).

## Change format using `melt()`

These data are in what is called **wide** format: different levels of the `year` variable are stored as columns. A **long** format would have three columns: one for `doy`, `year`, and `catch` or `esc`. Turn the `catch` data frame into long format using the `melt()` function from `{reshape2}`:

```{r}
long.catch = melt(catch, id.var = "doy",
                  variable.name = "year",
                  value.name = "catch")
head(long.catch)
```

The first argument is the data frame to reformat, the second argument (`id.var`) is the variable that identifies one observation from another, here it is the `doy` that the count occurred on. In this case, it is actually all we need to run `melt` properly. The optional `variable.name` and `value.name` arguments specify the names of the other two columns.  These default to "variable" and "value" if not specified. Do the same thing for escapement:

```{r}
long.esc = melt(esc, id.var = "doy",
                variable.name = "year",
                value.name = "esc")
head(long.esc)
```

Anytime you do a large data manipulation like this, you should compare the new data set with the original to verify that it worked properly. Ask if `all` of the daily catches for a given year in the original data set match up to the new data set:

```{r}
all(catch$y_2015 == long.catch[long.catch$year == "y_2015", "catch"])
```

The single `TRUE` indicates that every element matches up for 2015 in the catch data, just like they should.

To reverse this action (i.e., to go from long format to wide format), you would use the `dcast()` function from `{reshape2}`: 

```{r, eval = F}
dcast(long.catch, doy ~ year, value.var = "catch")
```

The formula specifies which variables are ID variables on the left-hand side, and which variable should get expanded to multiple columns. The `value.var` argument indicates which variable will be placed into the columns.

## Join Data Sets with `merge()`

You now have two long format data sets. You can turn them into one data set with the `merge()` function (which is actually in the `{base}` package). `merge()` takes two data frames and joins them based on certain grouping variables. Here, you want to combine the data frames `long.catch` and `long.esc` into one data frame, and match the rows by the `doy` and `year` for each unique pair:

```{r}
dat = merge(x = long.esc, y = long.catch,
            by = c("doy", "year"), all = T)
head(dat)
```

The `all = T` argument is important to specify here. It says that you wish to keep **all** of the unique records in the columns passed to `by` from both data frames. The `doy` in the catch data ranges from day `r min(long.catch$doy)` to day `r max(long.catch$doy)`, whereas for escapement it ranges from day `r min(long.esc$doy)` to day `r max(long.esc$doy)`. In this system, the fishery opens on the 160^th^ day of every year, and the counting tower starts the 161^st^ day. If you didn't use `all = T`, you would drop all the rows in each data set that do not have a match in the other data set (you would lose day `r min(long.catch$doy)` and day `r max(long.esc$doy)` from every year). Notice that because no escapement observations were ever made on `doy` `r min(long.catch$doy)`, the `esc` value on this day is `NA`.

Notice how the data set is now ordered by `doy` instead of `year`. This is not a problem, but if you want to reorder the data frame by `year`, you can use the `arrange()` function from `{dplyr}`:

```{r}
head(arrange(dat, year))
```

## Lagged vectors

The present task is to the calculate daily cumulative run proportion^[This is the fraction of all fish that came back each year that did so on or before a given day] in 2015 for comparison to the other years. Given the information you have, this task would be very cumbersome if not for the flexibility allowed by `{dplyr}`. 

Remember the counting tower is an average one day swim for the salmon from the fishing grounds. This means that the escapement counts are **lagged** relative to the catch counts. If you want the daily run (defined as the number of fish in the fishing grounds each day), you need to account for this lag. An easy way to think about the lag is to show it graphically. First, pull out one year of data using the `filter()` function in `{dplyr}`:

```{r}
y15 = filter(dat, year == "y_2015")
head(y15)
```

The `{base}` analog to the `filter()` function is:

```{r, eval = F}
y15 = dat[dat$year == "y_2015", ]
# or
y15 = subset(dat, year == "y_2015")
```

They take about the same amount of code to write, but `filter()` has some advantages when used with other `{dplyr}` functions that will be discussed later. Now plot the daily catch and escapement (not cumulative yet) for 2015:

```{r, eval = F}
plot(catch ~ doy, data = y15, type = "b", pch = 16, col = "blue",
     xlab = "DOY", ylab = "Count", main = "Raw Data")
lines(esc ~ doy, data = y15, type = "b", pch = 16, col = "red")
legend("topright", legend = c("Catch", "Escapement"), 
       col = c("blue", "red"), pch = 16, lty = 1, bty = "n", cex = 0.8)
```

```{r, echo = F}
par(sp)
plot(catch ~ doy, data = y15, type = "b", pch = 16, col = "blue",
     xlab = "DOY", ylab = "Count", main = "Raw Data")
lines(esc ~ doy, data = y15, type = "b", pch = 16, col = "red")
legend("topright", legend = c("Catch", "Escapement"), 
       col = c("blue", "red"), pch = 16, lty = 1, bty = "n", cex = 0.8)
```

Notice how the escapement counts are shifted to the right by one day. This is the lag. To account for it, you can use the `lag()` function from `{dplyr}`. `lag()` works by lagging some vector by `n` elements (it shifts every element in the vector `n` places to the right by putting `n` `NAs` at the front and removing the last `n` elements from the original vector). One way to fix the lag problem would be to use `lag()` on `catch` to make the days match up with `esc` (don't lag `esc` because that would just lag it by `n` additional days):

```{r, eval = F}
plot(lag(catch, n = 1) ~ doy, data = y15, type = "b", pch = 16, col = "blue",
     xlab = "DOY", ylab = "Count", main = "With lag(catch)")
lines(esc ~ doy, data = y15, type = "b", pch = 16, col = "red")
legend("topright", legend = c("Catch", "Escapement"),
       col = c("blue", "red"), pch = 16, lty = 1, bty = "n", cex = 0.8)
```

```{r, echo = F}
par(sp)
plot(lag(catch, n = 1) ~ doy, data = y15, type = "b", pch = 16, col = "blue",
     xlab = "DOY", ylab = "Count", main = "With lag(catch)")
lines(esc ~ doy, data = y15, type = "b", pch = 16, col = "red")
legend("topright", legend = c("Catch", "Escapement"),
       col = c("blue", "red"), pch = 16, lty = 1, bty = "n", cex = 0.8)
```

Notice how the two curves line up better now.

## Adding columns with `mutate()`

Apply this same idea to get the total daily run (what was harvested plus what was not). Make a function to take the two vectors (catch and escapement), lag one, and then sum them. Howevery, you don't want to move catch forward a day; you need to move escapement back a day. The opposite of `lag()` is `lead()`. The way to think of this is that `lag()` lags a vector, and `lead()` unlags a vector. `lead()` shifts every element to the left by `n` elements by removing the first `n` elements of the original vector and adding `n` `NAs` to the end. This is what your function should look like:

```{r}
lag_sum = function(x, y, n = 1){
  # x is lagged from y by n days
  rowSums(cbind(lead(x, n), y), na.rm = T)
}
```

Note the use of `na.rm = T` to specify that summing a number with an `NA` should still return a number. If two `NAs` are summed, the result will be a zero.

Try out the `lag_sum()` function:

```{r, eval = F}
lag_sum(y15$esc, y15$catch)
```

Add this column to your data set. You add columns in `{dplyr}` using `mutate()`:

```{r}
y15 = mutate(y15, run = lag_sum(esc, catch))
```

The `{base}` equivalent to `mutate()` is:

```{r, eval = F}
y15$run = lag_sum(y15$esc, y15$catch)
```

`mutate()` has advantages when used with other `{dplyr}` functions that will be shown soon. Use the new variable `run` to calculate two new variables: the cumulative run by day (`crun`) and cumulative run proportion by day (`cprun`). Cumulative means that each new element is the value that occurred on that day plus all of the values that happened on days before it. You can use R's `cumsum()` for this:

```{r}
y15 = mutate(y15, crun = cumsum(run), cprun = crun/sum(run, na.rm = T))
```

Here is one advantage of `mutate()`: you can refer to a variable you just created to make a new variable within the same function. You would have to split this into two lines to do it in `{base}`. Indeed, you could have made `run`, `crun`, and `cprun` all in the same `mutate()` call.  Plot `cprun`^[It doesn't look like much, but this information is incredibly important for fishery managers. It provides information on how much of the annual run has passed on each day. You can see that the rate of change is the fastest in the middle of the run, this corresponds to the major hump in the plots you made earlier (which were raw numbers by day). Of course during the run, the managers don't know what proportion of the total run has passed because they don't know the total number of fish that will come that season, but by characterizing the mean and variability of the different run completion proportions on each day in the past, they can have some idea of how much of the run to expect yet to come on any given day.]:

```{r, eval = F}
plot(cprun ~ doy, data = y15, type = "l", ylim = c(0, 1),
     xlab = "DOY", ylab = "Cumulative Run Proportion", lwd = 2)
```

```{r, echo = F}
par(sp)
plot(cprun ~ doy, data = y15, type = "l", ylim = c(0, 1),
     xlab = "DOY", ylab = "Cumulative Run Proportion", lwd = 2)
```

## Apply to all years

You have just obtained the daily and cumulative run for one year (2015), but `{dplyr}` makes it easy to apply these same manipulations to all years. To really see the advantage of `{dplyr}`, you'll need to learn to use piping. A code **pipe** is one that takes the result of one function and inserts it into the input of another function. Here is a basic example of a pipe:

```{r}
rnorm(10) %>% length
```

The pipe took the result of `rnorm(10)` and passed it as the first argument to the `length` function. This allows you to string together commands so you aren't continuously making intermediate objects or nesting a ton of functions together. The pipe essentially does this:

```{r}
x = rnorm(10); length(x); rm(x)
```

The reason piping works so well with `{dplyr}` is because its functions are designed to take a data frame as input as the first argument and return another data frame as output. This allows you to string them together with pipes. Do a more complex pipe: take your main data frame (`dat`), group it by `year`, and pass it to a `mutate()` call that will add the new three columns to the entire data set:

```{r}
dat = dat %>%
  group_by(year) %>%
  mutate(run = lag_sum(esc, catch), 
         crun = cumsum(run), 
         cprun = crun/sum(run, na.rm = T))

arrange(dat, year)
```
 
The `group_by()` function is spectacular: you grouped the data frame by `year`, which tells the rest of the functions in the pipe to apply its commands to each year independently. With this function, you can group your data in any way you please and calculate statistics on a group-by-group basis. Note that in this example, it doesn't do much for you, because the data are so neat (all years have the same number of days, and `NAs` in all of the same places, etc.), but for data that are messier, this trick is very handy. Also note that the `dat` object looks a little different. When you print it, it only shows the first 10 rows. This is a `{dplyr}` feature that prevents you from printing a huge data set to the console.

Now plot the cumulative run proportion by day for each year, highlighting 2015:

```{r, echo = F}
# make an empty plot
plot(x = 0, y = 0, type = "n", xlim = range(dat$doy), ylim = c(0,1),
     xlab = "DOY", ylab = "Cumulative Run Proportion")

years = levels(dat$year)

# use sapply to "loop" through years, drawing lines for each
tmp = sapply(years, function(x) {
  lines(cprun ~ doy, data = filter(dat, year == x),
        col = ifelse(x == "y_2015", "blue", "grey"))
})
```

```{r, eval = F}
par(sp)
# make an empty plot
plot(x = 0, y = 0, type = "n", xlim = range(dat$doy), ylim = c(0,1),
     xlab = "DOY", ylab = "Cumulative Run Proportion")

years = levels(dat$year)

# use sapply to "loop" through years, drawing lines for each
tmp = sapply(years, function(x) {
  lines(cprun ~ doy, data = filter(dat, year == x),
        col = ifelse(x == "y_2015", "blue", "grey"))
})
```

## Calculate Daily Means with `summarize()`

Oftentimes, you want to calculate a statistic for a value across grouping variables, like year or site or day. Remember the `tapply()` function does this in `{base}`. Here you want to calculate the mean cumulative run proportion for each day averaged across years. For this, you can make use of the `summarize()` function in `{dplyr}`. Begin by ungrouping the data to remove the `year` groups and grouping the data by `doy`. This will allow you to calculate the mean proportion by day. You then pass this grouped data frame to `summarize()` which will apply the `mean()` function to the cumulative run proportions across all years on a particular day. It will do this for all days: 

```{r}
mean_cprun = dat %>% 
  ungroup %>% 
  group_by(doy) %>%
  summarize(cprun = mean(cprun))
head(mean_cprun)
```

The way to do this in `{base}` is with `tapply()`:

```{r}
tapply(dat$cprun, dat$doy, mean)[1:6]
```

Note that you get the same result, only the output from `tapply()` is a vector, and the output of `summarize()` is a data frame. 

One disadvantage of the `summarize()` function, however, is that it can only be used to apply functions that give one number as output (e.g., `mean()`, `max()`, `sd()`, `length()`, etc.). These are called **aggregate** functions: they take a bunch of numbers and aggregate them into one number. `tapply()` does not have this constraint. Illustrate this by trying to use the `range()` function (which combines the minimum and maximum values of some vector into another vector of length 2). 

```{r, eval = F, error = T}
# with summarise
dat %>% 
  ungroup %>%
  group_by(doy) %>% 
  summarize(range = range(cprun))

# with tapply
tapply(dat$cprun, dat$doy, range)[1:5]
```

You can see that `tapply()` allows you to do this (but gives a list), whereas `summarize()` returns a short and informative error. You could very easily fix the problem by making minimum and maximum columns in the same `summarize()` call:

```{r, eval = F}
dat %>%
  ungroup %>%
  group_by(jday) %>%
  summarize(min = min(cprun), max = max(cprun))
```

Add the mean cumulative run proportion to our plot (use `lines()` just like before, add it after the `sapply()` call):

```{r, eval = F}
lines(cprun ~ doy, data = mean_cprun, lwd = 3)
```

```{r, echo = F}
par(sp)
# make an empty plot
plot(x = 0, y = 0, type = "n", xlim = range(dat$doy), ylim = c(0,1),
     xlab = "DOY", ylab = "Cumulative Run Proportion")

years = levels(dat$year)

# use sapply to "loop" through years, drawing lines for each
tmp = sapply(years, function(x) {
  lines(cprun ~ doy, data = filter(dat, year == x),
        col = ifelse(x == "y_2015", "blue", "grey"))
})
lines(cprun ~ doy, data = mean_cprun, lwd = 3)
```

It looks like 2015 started like an average year but the peak of the run happened a couple days earlier than average. 

## More `summarize()`

Your colleagues have been talking lately about how the run has been getting earlier and earlier in recent years. To determine if their claims are correct, you decide to develop a method to find the day on which 50% of the run has passed (the median run date) and plot it over time. If there is a downward trend, then the run has been getting earlier. First, you need to define another function to find the day that corresponds to 50% of the run. If there were days when the cumulative run on that day equaled exactly 0.5, we could simply use `which()`:

```{r, eval = F}
ind = which(dat$cprun == 0.5)
dat$jday[ind]
```

The `which()` function is useful: it returns the indices (element places) _for which_ the condition is `TRUE`. However, you need to do a workaround here, since the median day is likely not exactly 0.5, and it must be .5 in order for `==` to return a `TRUE`). You will give the function two vectors and it will look for a value that is closest to 0.5 in one vector and pull out the corresponding value in the second vector:

```{r}
find_median_doy = function(p,doy) {
  ind = which.min(abs(p - 0.5))
  doy[ind]
}
```

This function will take every element of `p`, subtract 0.5, take the absolute value of the results (make it positive, even if it is negative), and find the element number that is smallest. This will be the element with the value closest to 0.5. Note that you could find the first quartile (0.25) or third quartile (0.75) of the run by inserting these in for 0.5 above^[This is the perfect kind of thing to make an argument for in your function!]. Try your function on the 2015 data only:

```{r}
# use function
med_doy15 = find_median_doy(p = y15$cprun, doy = y15$doy)

# pull out the day it called the median to verify it works
filter(y15, doy == med_doy15) %>% select(cprun)
```

The `select()` function in `{dplyr}` extracts the variables requested from the data frame. If there is a grouping variable set using `group_by()`, it will be returned as well. It looks like the function works. Now combine it with `summarize()` to calculate the median day for every year:

```{r}
med_doy = 
  dat %>% 
  group_by(year) %>% 
  summarize(doy = find_median_doy(cprun, doy))
head(med_doy)
```

Now, use your `{dplyr}` skills to turn the year column into a numeric variable:

```{r}
# get years as a number
med_doy$year = 
  ungroup(med_doy) %>%
  # extract only the year column
  select(year) %>%
  # turn it to a vector and extract unique values
  unlist %>% unname %>% unique %>%
  # replace "y_" with nothing
  gsub(pattern = "y_", replacement = "") %>%
  # turn to a integer vector
  as.integer
```

Now plot the median run dates as a time series:

```{r, eval = F}
plot(doy ~ year, data = med_doy, pch = 16)
```

```{r, echo = F}
par(sp)
plot(doy ~ year, data = med_doy, pch = 16)
```

It doesn't appear that there is a trend in either direction, but fit a regression line because you can (Section \@ref(regression)):

```{r, eval = F}
fit = lm(doy ~ year, data = med_doy)
summary(fit)$coef

plot(doy ~ year, data = med_doy, pch = 16)
abline(fit, lty = 2)
```

```{r, echo = F}
fit = lm(doy ~ year, data = med_doy)
summary(fit)$coef

par(sp)
plot(doy ~ year, data = med_doy, pch = 16)
abline(fit, lty = 2)
```

The results tell you that there is a `r abs(round(coef(summary(fit))[2,1], 3))` day decrease in median run date for every 1 year that has gone by and that it is not significantly different from a zero day decrease per year. The statistical evidence does not support your colleagues' speculations.

## Fit the Model

You have done some neat data exploration exercises (and hopefully learned `{dplyr}` and piping along the way!), but your ultimate task with this data set is to run a spawner-recruit analysis on all the data. A **spawner-recruit analysis** is one that links the number of total fish produced in a year (the recruits) to the total number of fish that produced them (the spawners). This is done in an attempt to describe the productivity and carrying capacity of the population and to obtain **biological reference points** off of which harvest policies can be set. You will need to summarize the daily data into annual totals of escapement and total run:

```{r}
sr_dat = 
  dat %>%
  summarize(S = sum(esc, na.rm = T),
            R = sum(run, na.rm = T))

head(sr_dat)
```

You didn't need to use `group_by()` here because the data were still grouped from an earlier task. But, there is no harm in putting it in, and it would help make your code more readable if you were to include it.

That is the main data manipulation you need to do in order to run the spawner-recruit analysis. You can fit the model using `nls()` (Section \@ref(nls)). The model you will fit is called a **Ricker spawner-recruit model** and has the form:

\begin{equation}
  R_t = \alpha S_{t-1} e^{-\beta S_{t-1} + \varepsilon_t} ,\varepsilon_t \sim N(0,\sigma)
(\#eq:ricker-ch5)
\end{equation}

where $\alpha$ is a parameter representing the maximum recruits per spawner (obtained at very low spawner abundances) and $\beta$ is a measure of the strength of **density-dependent mortality**. Notice that the error term is in the exponent, which makes $e^{\varepsilon_t}$ lognormal. To get `nls()` to fit this properly, we will need to fit to $log(R_t)$ as the response variable. Write a function that will predict log recruitment from spawners given the two parameters (ignore the error term):

```{r}
ricker = function(S, alpha, beta) {
  log(alpha * S * exp(-beta * S))
}
```

Fit the model to the data using `nls()`. Before you fit it, however, you'll need to do another lag. This is because the run that comes back in one year was spawned by the escapement the previous odd year (you only have odd years in the data). This time extract the appropriate years "by-hand" rather than using the `lag()` or `lead()` functions as before:

```{r}
# the number of years
nyrs = nrow(sr_dat)
# the spawners to fit to:
S_fit = sr_dat$S[1:(nyrs - 1)]
# the recruits to fit to:
R_fit = sr_dat$R[2:nyrs]
```

Fit the model:

```{r}
fit = nls(log(R_fit) ~ ricker(S_fit, alpha, beta),
          start = c(alpha = 6, beta = 0))
coef(fit); summary(fit)$sigma
```

Given that the true values for these data were: $\alpha = 6$, $\beta = 8\times10^{-6}$, and $\sigma = 0.4$, these estimates look pretty good.

Plot the recruits versus spawners and the fitted line:

```{r, eval = F}
# plot the S-R pairs
plot(R_fit ~ S_fit, pch = 16, col = "grey", cex = 1.5,
     xlim = c(0, max(S_fit)), ylim = c(0, max(R_fit)))
# extract the estimates
ests = coef(fit)
# obtain and draw on a fitted line
S_line = seq(0, max(S_fit), length = 100)
R_line = exp(ricker(S = S_line,
                alpha = ests["alpha"],
                beta = ests["beta"]))
lines(R_line ~ S_line, lwd =3)
# draw the 1:1 line
abline(0, 1, lty = 2)
```

```{r, echo = F}
par(sp)
# plot the S-R pairs
plot(R_fit ~ S_fit, pch = 16, col = "grey", cex = 1.5,
     xlim = c(0, max(S_fit)), ylim = c(0, max(R_fit)))
# extract the estimates
ests = coef(fit)
# obtain and draw on a fitted line
S_line = seq(0, max(S_fit), length = 100)
R_line = exp(ricker(S = S_line,
                alpha = ests["alpha"],
                beta = ests["beta"]))
lines(R_line ~ S_line, lwd =3)
# draw the 1:1 line
abline(0, 1, lty = 2)
```

The diagonal line is the **1:1 replacement line**: where 1 spawner would produce 1 recruit. The distance between this line and the curve is the theoretical **harvestable surplus** available at each spawner abundance that would keep the stock at a fixed abundance. The biological reference points that might be used in harvest management are:

\begin{eqnarray*}
&& S_{MAX}=\frac{1}{\beta},\\
&& S_{eq}=log(\alpha) S_{MAX},\\
&& S_{MSY}=S_{eq} \left(0.5-0.07*log(\alpha)\right)
(\#eq:ricker-brps)
\end{eqnarray*}

Where $S_{MAX}$ is the spawner abundance expected to produce the maximum recruits, $S_{eq}$ is the spawner abundance that should produce exactly replacement recruits, and $S_{MSY}$ is the spawner abundance that is expected to produce the maximum surplus (all under the assumption of no environmental variability). You can calculate the reference points given in Equation \@ref(eq:ricker-brps) from your parameter estimates:

```{r}
Smax = 1/ests["beta"]
Seq = log(ests["alpha"]) * Smax
Smsy = Seq * (0.5 - 0.07 * log(ests["alpha"]))
brps = round(c(Smax = unname(Smax),
               Seq = unname(Seq),
               Smsy = unname(Smsy)), -3)
```

Draw these reference points onto your plot and label them:

```{r, eval = F}
abline(v = brps, col = "blue")
text(x = brps, y = max(R_fit) * 1.08, 
     labels = names(brps), xpd = T, col = "blue")
```

```{r, echo = F}
par(sp)
plot(R_fit ~ S_fit, pch = 16, col = "grey", cex = 1.5,
     xlim = c(0, max(S_fit)), ylim = c(0, max(R_fit)))
lines(R_line ~ S_line, lwd =3)
abline(0,1, lty = 2)
abline(v = brps, col = "blue")
text(x = brps, y = max(R_fit) * 1.08, 
     labels = names(brps), xpd = T, col = "blue")
```

---

## Exercise 5 {-}

In this exercise, you will be working with another simulated salmon data set (from an age-structured population this time, like Chinook salmon _Oncorhynchus tshawytscha_). You have information on individual fish passing a **weir**. A weir is a blockade in a stream that biologists use to count fish as they pass through. Most of the day, the weir is closed and fish stack up waiting to pass. Then a couple times a day, biologists open a gate in the weir and count fish as they swim through. Each year, the biologists sample a subset of fish that pass to get age, sex, and length. There are concerns that by using large mesh gill nets, the fishery is selectively taking the larger fish and that this is causing a shift in the size-at-age, age composition, and sex composition. If this is the case, it could potentially mean a reduction in productivity since smaller fish have fewer eggs. You are tasked with analyzing these data to determine if these quantities have changed overtime. Note that documenting directional changes in these quantities over time and the co-occurrence of the use of large mesh gill nets does not imply causation.

_The solutions to this exercise are found at the end of this book ([here](#ex5-answers)). You are **strongly recommended** to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped._

Use the data found in `asl.csv` (**a**ge, **s**ex, **l**ength) (see the [instructions](#data-sets) for details on acquiring the data files for this book). Create a new R script `Ex5.R` in your working directory. Open this script and read the data file into R. If this is a new session, load the `{dplyr}` package.

1.  Count the number of females that were sampled each year using the `{dplyr}` function `n()` within a `summarize()` call (_Hint: `n()` works just like length - it counts the number of records_).
2. Calculate the proportion of females by year.
3. Plot percent females over time. Does it look like the sex composition has changed over time?
4. Calculate mean length by age, sex, and year.
5. Come up with a way to plot a time series of mean length-at-age for both sexes. Does it look like mean length-at-age has changed over time for either sex?

---

### Exercise 5 Bonus {-}

1. Calculate the age composition by sex and year (what proportion of all the males in a year were age 4, age 5, age 6, age 7, and same for females).
2. Plot the time series of age composition by sex and year. Does it look like age composition has changed over time?

<!--chapter:end:05-large-data.Rmd-->

# Mapping and Spatial Analysis {#ch6}

_This chapter was contributed by Henry Hershey_

```{r, echo = F}
rm(list = ls(all = T))
knitr::opts_chunk$set(fig.align = "center", 
                      fig.width = 5, fig.height = 5,
                      message = F, warning = F)
```

## Chapter Overview {#ch6overview} 

R is a relatively under-used tool for creating Geographic Information Systems (GIS). Most people use ArcGIS, QGIS, or Google Earth to display and analyze spatial data. However, R can do much of what you might want to do in those programs, with the added benefit of allowing you to create a reproducible script file to share. Workflows can be difficult to replicate in ArcGIS because of the point-click user interface. With R, anyone can replicate your geospatial analysis with your script file and data.

In this chapter, you will learn the basics of:
  
*  creating a GIS in R
*  mapping parts of your GIS
*  "selecting by attribute" (A.K.A.`subset()` or `dplyr::filter()` in R)
*  manipulating spatial objects

You will map and analyze data from a brown bear (_Ursus arctos_) tracking study [@bears-cite]. In Slovenia, a railroad connects the capitol Ljubljana with the coastal city of Koper, and passes right through brown bear country. You will use R to assess potential sites to build a hypothetical wildlife overpass so that bears can safely cross the railroad.

It is worth mentioning that if you are left wanting more information about geospatial analysis in R after this brief introduction, a thorough description of more advanced techniques is presented for free in @geospatR-cite.

## Before You Begin {#ch6beforeyoubegin} 

You should create a new directory and R script for your work in this chapter called `Ch6.R` and save it in the directory `C:/Users/YOU/Documents/R-Book/Chapter6`. Set your working directory to that location. Revisit the material in Sections \@ref(scripts) and \@ref(working-dir) for more details on these steps.

For this chapter, it will be helpful to have the data in your working directory. In the `Data/Ch6` folder (see the [instructions](#data-sets) on acquiring the data files), you'll find: 
  
  1.  a file named `bear.csv`,
  2.  folders named `railways`, `states`, and `SVN_adm`, and
  3.  an R script called `points_to_line.R` [@points-line-cite]. 
  
Copy (or cut if you'd like) all of these files/folders from `Data/Ch6` into your working directory. 
        
Mapping in R requires many packages. Install **all** of the following packages:
        
```{r,eval = F}
install.packages(
  c("sp","rgdal","maptools",
  "rgeos","raster","scales",
  "adehabitatHR","dismo", "prettymapr")
)
```
        
You will need `{dplyr}` [@R-dplyr] as well, if you did not install it already for Chapter \@ref(ch5). 

Rather than load all the packages at the top of the script (as is typically customary), this chapter will load and briefly describe each package immediately prior to using it for the first time.

## Geospatial Data {#intro}

There are two types of geospatial data: **vector**^[this term should not be confused with the vector data type in R] and **raster** data. R is able to handle both, but for the sake of simplicity, this chapter will mostly deal with vector data. There are three types of vector data that you should be familiar with (Figure \@ref(fig:vector-types)): 

*  A **point** is a pair of coordinates (`x` = longitude, `y` = latitude) that represent the location of some observed object or event
*  A **line** is a path between two or more ordered points. The endpoints of a line are called **vertices**.
*  A **polygon** is an area bounded by vertices that are connected in order. In a polygon, the first and last vertex are the same. 

In this first section, you will learn how to load these different types of data and create spatial objects that you can manipulate and visualize in R.

```{r vector-types, echo = F, fig.height = 3, fig.width = 9, fig.cap = "The three basic types of GIS vector data"}
par(xaxs = "i", yaxs = "i", mfrow=c(1,3) ,mar = c(2,2,2,2),
    oma = c(2,2,0,0), cex.axis = 1.4, cex.main = 2)
plot(c(2,4)~c(2,1),pch = 16, cex = 2.5, col = "grey",
     main="Points",xlim=c(0,5),ylim=c(0,5),xlab="x",ylab="y",las=1)
points(c(2,4)~c(2,1), cex = 2.5)
plot(c(4,1,2,3)~c(1,2,3.5,4),pch = 16, col = "grey", cex = 2.5, xlim=c(0,5),ylim=c(0,5),
     main="Line",xlab="x",ylab="y",las=1)
points(c(4,1,2,3)~c(1,2,3.5,4), cex = 2.5)
lines(c(4,1,2,3)~c(1,2,3.5,4))
plot(NULL,xlim=c(0,5),ylim=c(0,5),xlab="x",ylab="y",
     main="Polygon",las=1)
polygon(c(4,1,2,3.5),c(1,2,3,4),xlim=c(0,5),ylim=c(0,5),col = "grey90")
points(c(4,1,2,3.5),c(1,2,3,4), col = "grey", pch = 16, cex = 2.5)
points(c(4,1,2,3.5),c(1,2,3,4), cex = 2.5)
lines(c(4,1,2,3.5),c(1,2,3,4))

mtext(side = 1, "x (Longitude)", outer = T, line = 0.9)
mtext(side = 2, "y (Latitude)", outer = T, line = 0.9)
```

## Importing Geospatial Data {#Import}

### `.csv` files

Begin by loading in coordinate data from `bear.csv` and creating a points layer from them. The points are from a telemetry study that tracked brown bear movements in south western Slovenia for a period of about 10 years [@bears-cite]. Each observation in this data set represents what is called a **relocation event** -  meaning a bear was detected again after it was initially tagged. The variables measured at each relocation event include a location, a time, and several other variables.

```{r, eval = F}
bear = read.csv("bear.csv",stringsAsFactors = F)
colnames(bear)
```

```{r, echo = F}
bear = read.csv("Data/Ch6/bear.csv", stringsAsFactors = F)
colnames(bear)
```

There are quite a few variables in these data that are extraneous, so just keep the bare necessities: the date and time a bear was relocated, the name of the observed bear, and the x and y coordinates of its relocation. Also, omit the observations that have an `NA` value for any of those variables with `na.omit()`:

```{r}
bear = na.omit(bear[,c("timestamp","tag.local.identifier",
                        "location.long","location.lat")])
colnames(bear) = c("timestamp","ID","x","y")
head(bear)

```

Notice that the bears have names: `unique(bear$ID)`. Do some more simple data wrangling:

```{r}
# how many records of each bear are there?
table(bear$ID)

# bears that were relocated fewer than 5 times 
# cannot be used in future analysis so filter them out
library(dplyr)
bear = bear %>%
  group_by(ID) %>%
  filter(n() >= 5)

# now how many bears are there?
unique(bear$ID)
```

Now, use the `{sp}` package [@R-sp] to create a spatial object out of your standard R data frame. You can think of this object like a **layer** in GIS. `{sp}` lets you create layers with or without attribute tables, but if your spatial data have other attributes like an ID or a timestamp variable, you should always create an object with class `SpatialPointsDataFrame` to make sure those variables/attributes are stored.

In order to convert a data frame into a `SpatialPointsDataFrame`, you need to specify four arguments: 

*  `data`: the data frame being converted, 
*  `coords`: the coordinates in that dataframe, 
*  `coords.nrs`: the indices for those coordinate vectors in the `data` data.frame, and 
*  `proj4string`: the projected coordinate system of the data. You will use the WGS84 coordinate reference system. More on coordinate reference systems later in Section \@ref(crs)^[For more information on projected coordinate systems, follow this link: http://desktop.arcgis.com/en/arcmap/10.3/guide-books/map-projections/about-projected-coordinate-systems.htm].

```{r, message = F, warning = F}
library(sp)
bear = SpatialPointsDataFrame(
  data = bear,
  coords = bear[,c("x","y")],
  coords.nrs = c(3,4),
  proj4string = CRS("+init=epsg:4326")
  )

```

Without any reference data these points are essentially useless. You will need to load some more spatial data to get your bearings. You have more spatial data, but they are in a different format.

### `.shp` files

The `{rgdal}` package [@R-rgdal] facilitates loading spatial data files (i.e., shapefiles) into R. The function `readOGR()` takes a standard shapefile (`.shp`), and converts it into a spatial object of the appropriate class (e.g., points or polygons). It takes two arguments: the data source name (`dsn`, the directory), and the layer name (`layer`). When you download a shapefile from an open-source web portal, it will often have accompanying files that store the attribute data. Store all of these files in a folder with the same name as the shapefile. Now load in the shapefile that contains a polygon for the boundary of Slovenia [@svn-cite] from the directory, so you can see where in the country the brown bears were detected:

```{r,results="hide", eval = F}
#load all the shapefiles for the background map. 
#you'll learn how to add a basemap later
library(rgdal)
#border of slovenia
slovenia = readOGR(dsn = "./SVN_adm",layer = "SVN_adm0")
```

```{r,results="hide", echo = F}
library(rgdal)
slovenia = readOGR(dsn="./Data/Ch6/SVN_adm",layer="SVN_adm0")
```

There are two important differences between `readOGR()` and `read.csv()`:

1.  The directory shortening syntax is not the same. Notice the period in the data source names. This indicates your working directory.
2.  When calling the layer name, the file extension `.shp` is not required.

Notice the feature class of `slovenia` is a polygon:

```{r}
class(slovenia)
```

Now read in two other shapefiles:

*  one showing the statistical regions (aka states) of Slovenia [@svn-cite]
*  one showing the railways in Slovenia [@rail-cite]

```{r, eval = F}
#major railroads in slovenia
railways = readOGR(dsn = "./railways",
                   layer = "railways") 
#statistical areas (states)
stats = readOGR(dsn = "./SVN_adm",
                layer = "SVN_adm1", stringsAsFactors = F)  
```

```{r,results="hide", echo = F}
#major railroads in slovenia
railways = readOGR(dsn="./Data/Ch6/railways",layer="railways") 
#statistical regions (states)
stats = readOGR(dsn="./Data/Ch6/SVN_adm",layer="SVN_adm1",stringsAsFactors = F) 
```

## Plotting {#SpatPlot} 
Plotting spatial objects in R is a breeze. See what happens if you just plot the `bear` object:

```{r, eval = F}
plot(bear)
```

You can clean up your map a bit with standard `{graphics}` arguments:

```{r, fig.height = 5, fig.width = 5}
library(scales)
par(mar = c(2,2,1,1))
plot(bear, col = alpha("blue", 0.5), pch = 16, axes = T)
```

The `alpha()` function from the `{scales}` package [@R-scales] allows you to plot with transparent colors, which is helpful for seeing the high- versus low-density clusters. Now, plot your reference layers to get your bearings:

```{r, fig.width = 5, fig.height = 5}
#make a map of all the bear relocations and railways in slovenia
par(mar = c(2,2,1,1))
plot(stats, border = "grey", axes = T)
plot(slovenia, lwd = 3, add = T)     # you can draw multiple plots on top
points(bear, pch = 16, cex = 0.5,    # or use a low lvl plot function
       col = alpha("blue", 0.5))
lines(railways, col = "red", lwd = 3) 
```

### Zooming {#zoom}

You may want to zoom in on the part of Slovenia where the bears are. Spatial objects in R have a slot^[slots are components of S4 class objects. more on that here https://stackoverflow.com/questions/4713968/r-what-are-slots/4714080#4714080] called `bbox` which is the "boundary box" of the data in that object. You can use the `bbox` to specify what the `xlim` and `ylim` of your map should be:

```{r, eval = F}
plot(stats, border = "grey", axes = T,
     xlim = bear@bbox[1,],  # access the boundary box using @
     ylim = bear@bbox[2,])
plot(slovenia, lwd = 3, add = T)
points(bear, pch = 16, cex = 0.5, col = alpha("blue", 0.5))
plot(railways, add = T, col = "red", lwd = 3)
```

```{r, echo = F, fig.width = 5, fig.height = 5}
par(mar = c(2,2,1,1))
plot(stats, border = "grey", axes = T,
     xlim = bear@bbox[1,],  # access the boundary box using @
     ylim = bear@bbox[2,])
plot(slovenia, lwd = 3, add = T)
points(bear, pch = 16, cex = 0.5, col = alpha("blue", 0.5))
plot(railways, add = T, col = "red", lwd = 3)
```

### Plotting Selections {#plot-selections}

In GIS, a subset is often called a **selection**; it is a smaller subset of a larger data layer. Say you want to see the relocation events for each individual bear at a time. Use `sapply()` to apply a function to plot each bear's track on a separate map. Wrap your code inside of a new PDF device (described in Section \@ref(file-devices)) so you can scroll through the plots separately:

```{r, results = "hide", eval = F}
pdf("Relocations.pdf", h = 5, w = 5)
sapply(unique(bear$ID), function(id) {
  par(mar = c(2,2,1,1))
  plot(stats, border = "grey", axes = T,
       xlim = bear@bbox[1,],  # access the boundary box using @
       ylim = bear@bbox[2,], main = paste("Bear:", id))
  plot(slovenia, lwd = 3, add = T)
  points(bear[bear$ID == id,], type = "o", pch = 16, cex = 0.5, col = alpha("blue", 0.5))
  plot(railways, add = T, col = "red", lwd = 3)
})
dev.off()
```

When you run this code, it will look like nothing happened. Go to your working directory and open the newly created file `Relocations.pdf` to see the output. Note that if you want to make changes to the PDF file by running `pdf(...); plot(...); dev.off()`, you'll need to close the file in your PDF viewer beforehand^[This is not true of files created with `png()` or `jpeg()`].

## Manipulating Spatial Data {#ManipSpat} 

Now that you have some data and you've taken a look at it, it's time to learn a few tricks for manipulating them. Looking at your map, see that some of the bears were detected outside of Slovenia. (Bonus points if you can name the country they're in). Suppose the Slovenian government can't build wildlife crossings in other countries, so you have to clip the bear data to the boundary of Slovenia.

### Changing the CRS {#crs}

Before you can manipulate any two related layers (e.g., clipping), you have to ensure that the two layers have identical coordinate systems. This can be done easily with the `spTransform()` function in the `{sp}` package. In order to obtain the coordinate reference system of a spatial object like `bear`, all you have to do is call `proj4string(bear)`. You can pass this directly to `spTransform()` like this:

```{r}
slovenia = spTransform(slovenia, CRS(proj4string(bear))) 
```

### Clipping {#clip}

Clipping is as simple as a standard subset in R. You can select the relocations that occured only in Slovenia using:

```{r}
bear = bear[slovenia,]
```

Make the same plot as you did in Section \@ref(zoom) with the clipped bear points.

```{r, echo = F, fig.width = 5, fig.height = 5}
par(mar = c(2,2,1,1))
plot(stats, border = "grey", axes = T,
     xlim = bear@bbox[1,],  # access the boundary box using @
     ylim = bear@bbox[2,])
plot(slovenia, lwd = 3, add = T)
points(bear, pch = 16, cex = 0.5, col = alpha("blue", 0.5))
plot(railways, add = T, col = "red", lwd = 3) 
```

### Adding Attributes{#add-attr}

What if you wanted to add an attribute to a points object, like the name of the polygon it occurs in? You can find this by extracting the attribute values of one layer (a **target**) at locations of another layer (a **source**) with the `over()` function from the `{sp}` package. For example, say you wanted to know what statistical region each bear relocation happened in. In this case, the target is the `stats` layer, and the source is `bear` layer. These two layers will need to be in the same projection, and the result will be stored in the column `bearstats$NAME_1`:

```{r}
# get in same projection
stats = spTransform(stats, proj4string(bear))
# determine which polygon of stat each bear relocation occured in
bearstats = over(bear,stats)
head(bearstats)
```

Extract just the column you care about and see how many relocations occurred in each state:

```{r}
bearstats = data.frame(stat = bearstats$NAME_1)
table(bearstats$stat) 
```

Then, you can recombine your extracted attribute values (in `bearstats`) to your original layer (`bear`) with `spCbind()` from the `{maptools}` package [@R-maptools]:

```{r, error = T}
library(maptools)
bear = spCbind(bear, bearstats$stat)
head(bear@data)
```

Now determine how many times each bear was relocated in each state:

```{r, eval = F}
table(bear$ID, bear$bearstats.stat)
```

## Analysis {#Anal} 

Now that you know how to import, plot, and manipulate your data, it's time to do some analysis. The `{rgeos}` package [@R-rgeos] has a few tools for simple geometric calculations like finding the distance between two points, or the area of a polygon. However, more specialized analyses are often only available in other packages. Two common analytical tasks in animial tracking are:

*  Calculating the **home range** of an animal, i.e., defining the area where most of the relocations occurred.
*  Finding **intersections** between tracks and some other line (e.g., river, state boundary, or railroad). 

In this section, you will use the `{rgeos}` package and another specialized package to do these tasks.

### Home Range Analysis {#hr-Anal}

If you look at the plots in Section \@ref(plot-selections), it looks like some bears live very close to the railroad, but do not cross it, some live very far away, and others may have crossed it multiple times. Which bears' home ranges are intersected by the railroad? In order to determine this, you will have to calculate the home range of each animal with the `mcp` function in the `{adehabitatHR}` package [@R-adehabitatHR]. 

```{r}
library(adehabitatHR) 
#the mcp function requires coordinates 
#be in the Universal Transverse Mercator system
bear = spTransform(bear,CRS("+proj=utm +north +zone=33 +ellps=WGS84"))
# calculate the home range. mcp is one of 5 functions to do this
cp = mcp(xy=bear[,2], percent=95,unin="m",unout="km2") 
```

`mcp` calculates the minimum convex polygon bounded by the extent of the points which are within some percentile of closeness to the centroid of a group. Points that are very far away from the center are excluded. The standard percentile is 95%. `mcp` requires three other arguments:

*  `xy`: the grouping variable of the spatial points data frame (the ID of each bear),
*  `unin`: the units of the input (meters is the default), and 
*  `unout`: the units of the output.

If you run `cp`, you'll see that the areas of each polygon are stored in the object in square kilometers.

Now, plot the homeranges of each bear, label them using the `{rgeos}` package, and overlay the railroad on a map.

```{r, results = "hide", fig.width = 5, fig.height = 5}
library(rgeos) 
#match the coordinate system of the home ranges object with the railways layer 
cp = spTransform(cp, CRS(proj4string(slovenia)))
railways = spTransform(railways, CRS(proj4string(slovenia)))
#keep only the homeranges that include some part of the railway
cp = cp[railways,] 

# plot the polygons
par(mar = c(2,2,1,1))
plot(cp,col=alpha("blue", 0.5), axes = T)
#rgeos has a bunch of neat functions like this one
polygonsLabel(cp, labels = cp$id, method = "buffer",
              col = "white", doPlot=T) 
lines(railways, lwd = 3,col = "red")
```

### Finding Intersections Between Two Layers

Now that you know which bears crossed the railroad, find out where. You'll need a user-defined function called `points_to_line()`, which is stored as a script file in your working directory [@points-line-cite]. If you ever write a function, you can store it in a script file, and then bring it into your current session using the `source()` function. This prevents you from needing to paste all the function code every time you want to use it in a new script. Save as many functions as you want in a single script, and they will all be added to your Global Environment when you `source()` the file^[For projects with many user-defined functions, you may be better off creating a `./Functions` directory and housing multiple scripts there. Better yet, you can create your own package for personal use.].

```{r, eval = F}
source("points_to_line.R")
```

```{r, echo = T}
source("Data/Ch6/points_to_line.R")
```

`source()` essentially highlights all the code in a script and runs it, without you ever having to open the script. 

```{r}
# {maptools} is needed by points_to_line()
library(maptools) 
# change CRS
bear = spTransform(bear, CRS(proj4string(slovenia)))
#turn the bear relocations into tracks
bearlines = points_to_line(
  as.data.frame(bear),
  long="x",lat="y",
  id_field="ID", sort_field = "timestamp")
```

Now that your points have been turned into tracks for each bear, see where they intersect the railroad using the `gIntersection()` function from the `{rgeos}` package. Hang on though, this will take your computer a while to run (between 5 and 20 minutes). Optionally, you can start a timer and have the `{beepr}` package [@R-beepr] make a sound when the geoprocessing calculations are completed:

```{r, eval = F}
library(beepr)
start = Sys.time()
crossings = gIntersection(bearlines, railways) 
Sys.time() - start; beep(10)
```

```{r, echo = F}
# load in the object that takes forever to calculate
if(exists("crossings")) {
  save(crossings, file = "Objects/crossings")
} else {
  load(file = "Objects/crossings")
}
```

## Creating a Presentable Map {#base-maps}

Now that you have a points object with the crossings stored, make a map that you can present to policy makers.

First, you should get a nicer basemap than R's blank slate. Use the `{dismo}` [@R-dismo] and `{raster}` [@R-raster] packages to get one:

```{r, eval = F}
library(dismo); library(raster)
base = gmap(bear, type = "terrain", lonlat = T)
plot(base)
```

Now, put the rest of the layers in the same projection as the basemap:

```{r, eval = F}
# put the other layers in the same crs as the basemap
bear = spTransform(bear, basemap@crs) #note that the method for getting the crs is different for raster objects
railways = spTransform(railways, base@crs)
slovenia = spTransform(slovenia, base@crs)
proj4string(crossings) = base@crs
```

Now, plot the crossings in blue on top of the basemap with the railroad in red. You can add a scale bar and north arrow using the `{prettymapr}` [@R-prettymapr] package!

```{r, eval = F}
# plot the basemap
plot(base)

# draw on the railways and crossing events
lines(railways, lwd = 10, col = "red")
points(crossings, col = alpha("blue", 0.5), cex = 5, pch = 16)

# draw on a legend
legend("bottom", legend = c("Railway", "Crossing Events"),
       lty = c(1,NA), col = c("red", alpha("blue", 0.5)),
       pch = c(NA, 16), lwd = 10, cex = 5, horiz = T, bty = "n")

# draw on other map components: scale bar and north arrow
look at maptools
addscalebar(plotunit = "latlon", htin = 0.5,
            label.cex = 5, padin = c(0.5,0.5))
addnortharrow(pos = "topleft", scale = 5, padin = c(2, 2.5))
```

```{r, echo = F, fig.height = 5, fig.width = 5}
knitr::include_graphics("img/FinalMap.png")
```

Where are the bears crossing the railroad? It looks like there are two areas of the railroad that get the most bear activity. One in the hairpin turn, and one that's more spread out between Logatec and Unec. Perhaps there should be wildlife crossings in those areas to protect the more "adventurous" bears.

## Other R Mapping Packages

This chapter has covered many of R's basic mapping capabilities using the built-in `plot()` functionality, but there are certainly other frameworks in R. Here are a few examples, and a quick Google search should provide you with plenty of information to get up and running.

### `{ggmap}`

Just like for `{ggplot2}` [@R-ggplot2], many R users find the `{ggmap}` [@R-ggmap] package more intuitive than creating maps with `plot()` like you have done in this chapter. If you have messed around with `{ggplot2}` or would like a slightly different plotting workflow, look into it more. 

### `{leaflet}` {#leaflet}

The `{leaflet}` package [@R-leaflet] allows you to make interactive maps. This will only work if your output is HTML-based^[If you're viewing this in PDF format, go to: <https://bstaton1.github.io/au-r-workshop/ch6.html#leaflet> to view the interactive version. Better yet, install `{leaflet}`, and try it yourself!]

```{r, eval = knitr::is_html_output()}
library(leaflet)
leaflet() %>%
  #add a basemap
  addProviderTiles(providers$Esri.WorldGrayCanvas, group = "Grey") %>%
  # change the initial zoom
  fitBounds(slovenia@bbox["x","min"], slovenia@bbox["y","min"],
            slovenia@bbox["x","max"], slovenia@bbox["y","max"]) %>%
  # fill in slovenia
  addPolygons(data = slovenia, color = "grey") %>%
  # draw the railroad
  addPolylines(data = railways,opacity = 1, color = "red") %>%
  # draw the relocations
  addCircleMarkers(data = bear, color = "blue", clusterOptions = markerClusterOptions()) %>%
  # draw the crossing events
  addCircleMarkers(data = crossings, color = "yellow", clusterOptions = markerClusterOptions())
```

## Exercise 6 {-#ex6} 

1.  Load the `caves.shp` shapefile [@caves-cite] from your working directory. Add the data to one of the maps of Slovenia you created in this chapter.
2.  How many caves are there?
3.  How many caves are in each statistical area?
4.  Which bear has the most caves in its homerange?

## Exercise 6 Bonus {-}

1.  Find some data online for a system you are interested in and do similar activities as shown in this chapter. Good examples for obtaining open access spatial data are: 

*  **Administrative boundaries:** <https://gadm.org/>
*  **Animal tracking data sets:** <https://www.movebank.org/>
*  **US Geological Survey:** <https://www.usgs.gov/products/maps/gis-data>
   

<!--chapter:end:06-spatial-analysis.Rmd-->

# Exercise Solutions {-}

```{r, echo = F, message = F, warning = F}
rm(list = ls(all = T))
library(dplyr)
library(knitr)
library(kableExtra)
knitr::opts_chunk$set(fig.align = "center")
set.seed(1)
I_power = 500
sr_n_rep = 2000

sdiff = c("sdiff", "I_power", "sr_n_rep")
```

## Exercise 1 Solutions {-#ex2-answers}
### Exercise 1A Solutions {-#ex1a-answers}

**1.  Create a new file in your working directory called `Ex1A.R`.**

Go to _File > New File > R Script_. This will create an untitled R script. Go the _File > Save_, give it the appropriate name and click _Save_. If your working directory is already set to `C:/Users/YOU/Documents/R-Book/Chapter1`, then the file will be saved there by default.

You can create a new script using **CTRL + SHIFT + N** as well.

**2.  Enter these data (found in Table `r if(is_html_output()) "\\@ref(tab:ex-1-table-html)" else "\\@ref(tab:ex-1-table-pdf)"`) into vectors. Call the vectors whatever you would like. Should you enter the data as vectors by rows, or by columns? (Hint: remember the properties of vectors).**

Because you have both numeric and character data classes for a single row, you should enter them by columns:

```{r}
Lake = c("Big", "Small", "Square", "Circle")
Area = c(100, 25, 45, 30)
Time = c(1000, 1200, 1400, 1600)
Fish = c(643, 203, 109, 15)
```

**3.  Combine your vectors into a data frame. Why should you use a data frame instead of a matrix?**

You should use a data frame because, unlike matrices, they can store multiple data classes in the different columns. Refer back the sections on matrices (Section \@ref(matrices)) and data frames (Section \@ref(data-frames)) for more details.

```{r}
df = data.frame(Lake, Area, Time, Fish)
```

**4.  Subset all of the data from Small Lake.**

Refer back to Section \@ref(sub) for details on subsetting using indices and by column names, see Section \@ref(logsub) for details on logical subsetting.

```{r, eval = F}
df[df$Lake == "Small",]
# or
df[3,]
```

**5.  Subset the area for all of the lakes.**

Refer to the suggestions for question 4 for more details. 

```{r, eval = F}
df$Area
# or
df[,2]
```

**6.  Subset the number of fish for Big and Square Lakes only.**

Refer to the suggestions for question 4 for more details. 

```{r, eval = F}
df[df$Lake == "Big" | df$Lake == "Square","Fish"]
# or
df$Fish[c(1,3)]
```

**7.  You realize that you sampled 209 fish at Square Lake, not 109. Fix the mistake. There are two ways to do this, can you think of them both? Which do you think is better?**

The two methods are:

*  Fix the mistake in the first place it appears: when you made the `Fish` vector. If you change it there, all other instances in your code where you use the `Fish` object will be fixed after you re-run everything.

```{r, eval = F}
Fish = c(643, 203, 209, 15)
# re-run the rest of your code and see the error was fixed
```

*  Fix the cell in the data frame only:

```{r, eval = F}
df[df$Lake == "Square","Fish"] = 209
```

The second method would only fix the data frame, so if you wanted to use the vector `Fish` outside of the data frame, the error would still be present. For this reason, the first method is likely better.

**8.  Save your script. Close RStudio and re-open your script to see that it was saved**. 

_File > Save_ or **CTRL + S**

### Exercise 1B Solutions {-#ex1b-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

First, did you find the error? It is the `#VALUE!` entry in the `chao` column. You should have R treat this as an `NA`. The two easiest ways to do this are to either enter `NA` in that cell or delete its contents. You can do this easily by opening `ponds.csv` in Microsoft Excel or some other spreadsheet editor.

**1.  Read in the data to R and assign it to an object.**

After placing `ponds.csv` (and all of the other data files) in the location `C:/Users/YOU/Documents/R-Book/Data` and creating `Ex1B.R` in your working directory: 

```{r, echo = F}
dat = read.csv("Data/ponds.csv")
```

```{r, eval = F}
dat = read.csv("../Data/ponds.csv")
```

**2.	Calculate some basic summary statistics of your data using the `summary()` function.**

```{r, eval = F}
summary(dat)
```

**3.	Calculate the mean chlorophyll _a_ for each pond (_Hint: pond is a grouping variable_).**

Remember the `tapply()` function. The first argument is the variable you wish to calculate a statistic for (chlorophyll), the second argument is the grouping variable (pond), and the third argument is the function you wish to apply.

```{r, eval = F}
tapply(dat$chl.a, dat$pond, mean)
```

**4.	Calculate the mean number of _Chaoborus_ for each treatment in each pond using `tapply()`. (_Hint: You can group by two variables with:_ `tapply(dat$var, list(dat$grp1, dat$grp2), fun)`.**

The hint pretty much gives this one away:

```{r, eval = F}
tapply(dat$chao, list(dat$pond, dat$treatment), mean)
```

**5.	Use the more general `apply()` function to calculate the variance for each zooplankton taxa found only in pond S-28.**

First, subset only the correct pond and the zooplankton counts. Then, specify you want the `var()` function applied to the second dimension (columns). Finally, because `chao` has an `NA`, you'll need to include the `na.rm = T` argument.

```{r, eval = F}
apply(dat[dat$pond == "S.28",c("daph", "bosm", "cope", "chao")], 2, var, na.rm = T)
```

**6.	Create a new variable called `prod` in the data frame that represents the quantity of chlorophyll _a_ in each replicate. If the chlorophyll _a_ in the replicate is greater than 30 give it a "high", otherwise give it a "low". (_Hint: are you asking R to respond to one question or multiple questions? How should this change the strategy you use?_)**

Remember, you can add a new column to a data set using the `df$new_column = something()`. If the column `new_column` doesn't exist, it will be added. If it exists already, it will be written over. You can use `ifelse()` (not `if()`!) to ask if each chlorophyll measurement was greater or less than 30, and to do something differently based on the result:

```{r}
dat$prod = ifelse(dat$chl.a > 30, "high", "low")
```

**Bonus 1.  Use `?table` to figure out how you can use `table()` to count how many observations of high and low there were in each treatment (_Hint: `table()` will have only two arguments._).**

After looking through the help file, you should have seen that `table()` has a `...` as its first argument. After reading about what it takes there, you would see it is expecting:

> one or more objects which can be interpretted as factors (including character strings)...

So if you ran:

```{r}
table(dat$prod, dat$treatment)
```

You would get a table showing how many high and low chlorophyll observations were made for each treatment.

**Bonus 2.	Create a new function called `product()` that multiplies any two numbers you specify.**

See Section \@ref(user-funcs) for more details on user-defined functions. Your function might look like this:

```{r}
product = function(a,b) {
  a * b
}
product(4,5)
```

**Bonus 3.	Modify your function to print a message to the console and return the value `if()` it meets a condition and to print another message and not return the value if it doesn't.**

```{r}
product = function(a,b,z) {
  result = a * b
  
  if (result <= z) {
    cat("The result of a * b is less than", z, "so you don't care what it is")
  } else {
    cat("The result of a * b is", result, "\n")
    result
  }
}

product(4, 5, 19)

product(4, 5, 30)
```

The use of `cat()` here is similar to `print()`, but it is better for printing messages to the console.

## Exercise 2 Solutions {-#ex2-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  Create a new R script called `Ex2.R` and save it in the `Chapter2` directory. Read in the data set `sockeye.csv`. Produce a basic summary of the data and take note of the data classes, missing values (`NA`), and the relative ranges for each variable.**

_File > New File > R Script_, then _File > Save > call it Ex2.R > Save_. Then:

```{r, eval = F}
dat = read.csv("../Data/sockeye.csv")
summary(dat)
```

**2.	Make a histogram of fish weights for only hatchery-origin fish. Set `breaks = 10` so you can see the distribution more clearly.**

```{r, eval = F}
hist(dat[dat$type == "hatch","weight"], breaks = 10)
```

**3.	Make a scatter plot of the fecundity of females as a function of their body weight for wild fish only. Use whichever plotting character (`pch`) and color (`col`) you wish. Change the main title and axes labels to reflect what they mean. Change the x-axis limits to be 600 to 3000 and the y-axis limits to be 0 to 3500. (_Hint: The `NAs` will not cause a problem. R will only use points where there are paired records for both `x` and `y` and ignore otherwise_).**

```{r, eval = F}
plot(fecund ~ weight, data = dat[dat$type == "wild",],
     main = "Fecundity vs. Weight",
     pch = 17, col = "red", cex = 1.5,
     xlab = "Weight (g)", xlim = c(600, 3000),
     ylab = "Fecundity (#eggs)", ylim = c(0, 3500))
```

All of these arguments are found in Table `r if(is_html_output()) "\\@ref(tab:plot-arg-table-html)" else "\\@ref(tab:plot-arg-table-pdf)"`.

**4.	Add points that do the same thing but for hatchery fish. Use a different plotting character and a different color.**

```{r, eval = F}
points(fecund ~ weight, data = dat[dat$type == "wild",],
       pch = 15, col = "blue", cex = 1.5)
```

**5.	Add a legend to the plot to differentiate between the two types of fish.**

```{r, eval = F}
legend("bottomright",
       legend = c("Wild", "Hatchery"),
       col = c("blue", "red"),
       pch = c(15, 17),
       bty = "n",
       pt.cex = 1.5
)
```

Make sure the correct elements of the `legend`, `col`, and `pch` arguments match the way they were specified in the `plot()` and `lines()` calls!

**6.	Make a multi-panel plot in a new window with box-and-whisker plots that compare (1) spawner weight, (2) fecundity, and (3) egg size between hatchery and wild fish. (_Hint: each comparison will be on its own panel_). Change the titles of each plot to reflect what you are comparing.**

```{r, eval = F}
vars = c("weight", "fecund", "egg_size")
par(mfrow = c(1,3))
sapply(vars, function(v) {
  plot(dat[,v] ~ dat[,"type"], xlab = "", ylab = v)
})
```

**7.	Save the plot as a .png file in your working directory with a file name of your choosing.**

One way to do this:

```{r, eval = F}
ppi = 600
png("SockeyeComparisons.png", h = 5 * ppi, w = 7 * ppi, res = ppi)
par(mfrow = c(1,3))
sapply(vars, function(v) {
  plot(dat[,v] ~ dat[,"type"], xlab = "", ylab = v)
})
dev.off()
```

**Bonus 1.  Make a bar plot comparing the mean survival to eyed-egg stage for each type of fish (hatchery and wild). Add error bars that represent 95% confidence intervals.**

First, adapt the `calc_se()` function to be able to cope with `NAs`:

```{r, eval = F}
calc_se = function(x, na.rm = F) {
  # include a option to remove NAs before calculating SE
  if (na.rm) x = x[!is.na(x)]
  
  sqrt(sum((x - mean(x))^2)/(length(x)-1))/sqrt(length(x))
}
```

Then, calculate the mean and standard error for the % survival to the eyed-egg stage:

```{r, eval = F}
mean_surv = tapply(dat$survival, dat$type, mean, na.rm = T)
se_surv = tapply(dat$survival, dat$type, calc_se, na.rm = T)
```

Then, get the 95% confidence interval:

```{r, eval = F}
lwr_ci_surv = mean_surv - 1.96 * se_surv
upr_ci_surv = mean_surv + 1.96 * se_surv
```

Finally, plot the means and intervals:

```{r, eval = F}
mp = barplot(mean_surv, ylim = c(0, max(upr_ci_surv)))
arrows(mp, lwr_ci_surv, mp, upr_ci_surv, length = 0.1, code = 3, angle = 90)
```

**Bonus 2.  Change the names of each bar, the main plot title, and the y-axis title. ** 

```{r, eval = F}
mp = barplot(mean_surv, ylim = c(0, max(upr_ci_surv)),
             main = "% Survival to Eyed-Egg Stage by Origin",
             ylab = "% Survival to Eyed-Egg Stage",
             names.arg = c("Hatchery", "Wild"))
arrows(mp, lwr_ci_surv, mp, upr_ci_surv, length = 0.1, code = 3, angle = 90)

```

**Bonus 3.  Adjust the margins so there are 2 lines on the bottom, 5 on the left, 2 on the top, and 1 on the right.**

Place this line above your `barplot(...)` code:

```{r, eval = F}
par(mar = c(2,5,2,1))
```

## Exercise 3 Solutions {-#ex3-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  Perform the same analyses as conducted in Section \@ref(lm) (simple linear regression, ANOVA, ANCOVA, ANCOVA with interaction), using `egg_size` as the response variable. The predictor variables you should use are `type` (categorical) and `year`. You should plot the fit for each model separately and perform an AIC analysis. Practice interpretting the coefficient estimates.**

First, read in the data:

```{r, eval = F}
dat = read.csv("../Data/sockeye.csv")
```

```{r, echo = F}
dat = read.csv("Data/sockeye.csv")
```

```{r}
# regression
fit1 = lm(egg_size ~ year, data = dat)
# anova
fit2 = lm(egg_size ~ type, data = dat)
# ancova
fit3 = lm(egg_size ~ year + type, data = dat)
# ancova with interaction
fit4 = lm(egg_size ~ year * type, data = dat)
```

**2.  Perform the same analyses as conducted in Section \@ref(glms), this time using a success being having greater than 80% survival to the eyed-egg stage. Use `egg_size` and `type` as the predictor variables. You should plot the fitted lines for each model separately and perform an AIC analysis. Practice interpretting the coefficient estimates.**

```{r}
dat$binary = ifelse(dat$survival < 80, 0, 1)

fit1 = glm(binary ~ egg_size, data = dat, family = binomial)
fit2 = glm(binary ~ type, data = dat, family = binomial)
fit3 = glm(binary ~ egg_size + type, data = dat, family = binomial)
fit4 = glm(binary ~ egg_size * type, data = dat, family = binomial)
```

**3.  Make the same graphic as in Figure \@ref(fig:norm-plots) with at least one of the other distributions listed in Table `r if(is_html_output()) "\\@ref(tab:dist-table-html)" else "\\@ref(tab:dist-table-pdf)"` (other than the multinomial - being a multivariate distribution, it wouldn't work well with this code). Try thinking of a variable from your work that meets the uses of each distribution in Table `r if(is_html_output()) "\\@ref(tab:dist-table-html)" else "\\@ref(tab:dist-table-pdf)"` (or one that's not listed). If you run into trouble, check out the help file for that distribution.**

This example uses the lognormal distribution, with `meanlog = 10` and `sdlog = 0.25`:

```{r}
# parameters
meanlog = 10; sdlog = 0.25

# a sequence of possible random variables (fish lengths)
lengths = seq(0, 8e4, length = 100)

# a sequence of possible cumulative probabilities
cprobs = seq(0, 1, length = 100)

densty = dlnorm(x = lengths, meanlog, sdlog)  # takes specific lengths
cuprob = plnorm(q = lengths, meanlog, sdlog)  # takes specific lengths
quants = qlnorm(p = cprobs, meanlog, sdlog)   # takes specific probabilities
random = rlnorm(n = 1e4, meanlog, sdlog)      # takes a number of random deviates to make

# set up plotting region: see ?par for more details
# notice the tricks to clean up the plot
par(
  mfrow = c(2,2),    # set up 2x2 regions
  mar = c(3,3,3,1),  # set narrower margins
  xaxs = "i",        # remove "x-buffer"
  yaxs = "i",        # remove "y-buffer"
  mgp = c(2,0.4,0),  # bring in axis titles ([1]) and tick labels ([2])
  tcl = -0.25        # shorten tick marks
)

plot(densty ~ lengths, type = "l", lwd = 3, main = "dlnorm()",
     xlab = "Random Variable", ylab = "Density", las = 1)
plot(cuprob ~ lengths, type = "l", lwd = 3, main = "plnorm()",
     xlab = "Random Variable", ylab = "Cumulative Probability", las = 1)
plot(quants ~ cprobs, type = "l", lwd = 3, main = "qlnorm()",
     xlab = "P", ylab = "P Quantile Random Variable", las = 1)
hist(random, breaks = 50, col = "grey", main = "rlnorm()",
     xlab = "Fish Length (mm)", ylab = "Frequency", las = 1)
box() # add borders to the histogram
```

**Bonus 1.  Fit a von Bertalannfy growth model to the data found in the `growth.csv` data file. Visit Section \@ref(boot-test-ex) (particularly Equation \@ref(eq:vonB)) for details on this model. Use the initial values: `linf = 600`, `k = 0.3`, `t0 = -0.2`. Plot the fitted line over top of the data.** 

Read in the data:

```{r, eval = F}
dat = read.csv("../Data/growth.csv")
```

```{r, echo = F}
dat = read.csv("Data/growth.csv")
```

Fit the model:

```{r}
fit = nls(length ~ linf * (1 - exp(-k * (age - t0))),
          data = dat, start = c(linf = 600, k = 0.3, t0 = -0.2))
```

Plot the fit:

```{r}
ages = seq(min(dat$age), max(dat$age), length = 100)
pred_length = predict(fit, newdata = data.frame(age = ages))

plot(length ~ age, data = dat)
lines(pred_length ~ ages, lwd = 3)
```

## Exercise 4 Solutions {-#ex4-answers}
### Exercise 4A Solutions {-#ex4a-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  Simulate flipping an unfair coin (probability of heads = 0.6) 100 times using `rbinom()`. Count the number of heads and tails.**

```{r, eval = F}
flips = rbinom(n = 100, size = 1, prob = 0.6)
flips = ifelse(flips == 1, "heads", "tails")
table(flips)
```

**2.  Simulate flipping the same unfair coin 100 times, but using `sample()` instead. Determine what fraction of the flips resulted in heads.**

```{r, eval = F}
flips = sample(x = c("heads", "tails"), size = 100, replace = T, prob = c(0.6, 0.4))
table(flips)["heads"]/length(flips)
```

**3.  Simulate rolling a fair 6-sided die 100 times using `sample()`. Determine what fraction of the rolls resulted in an even number.**

```{r, eval = F}
rolls = sample(x = 1:6, size = 100, replace = T)
mean(rolls %in% c(2,4,6))

```

**4.  Simulate rolling the same die 100 times, but use the function `rmultinom()` instead. Look at the help file for details on how to use this function. Determine what fraction of the rolls resulted in an odd number.**

```{r, eval = F}
rolls = rmultinom(n = 100, size = 1, prob = rep(1, 6))
dim(rolls) #  rows are different outcomes, columns are iterations

# get the fraction of odd numbered outcomes
sum(rolls[c(1,3,5),])/sum(rolls)
```

### Exercise 4B Solutions {-#ex4b-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  Adapt this example to investigate another univariate probability distribution, like `-lnorm()`, `-pois()`, or `-beta()`. See the help files (e.g., `?rpois`) for details on how to use each function.**

This solution uses the `rbeta()`, `qbeta()`, and `pbeta()` functions. These are for the beta distribution, which has random variables that are between zero and one.

First, create the parameters of the distribution of interest:

```{r}
mean_p = 0.6  # the mean of the random variable
B_sum = 100   # controls the variance: bigger values are lower variance

# get the shape parameters of the beta dist
beta_shape = c(mean_p * B_sum, (1 - mean_p) * B_sum)
```

Then, generate random samples from this distribution:

```{r}
random = rbeta(100, beta_shape[1], beta_shape[2])
```

Then, test the `qbeta()` function:

```{r}
p = seq(0.01, 0.99, 0.01)
random_q = quantile(random, p)
beta_q = qbeta(p, beta_shape[1], beta_shape[2])
plot(beta_q ~ random_q); abline(c(0,1))
```

Then, test the `pbeta() function:

```{r}
q = seq(0, 1, 0.05)
random_cdf = ecdf(random)
random_p = random_cdf(q)
beta_p = pbeta(q, beta_shape[1], beta_shape[2])
plot(beta_p ~ q, type = "l", col = "blue")
points(random_p ~ q, col = "red")
```

### Exercise 4C Solutions {-#ex4c-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  What sample size `n` do you need to have a power of 0.8 of detecting a significant difference between the two tagging methods?**

Simply increase the maximum sample size considered and re-run the whole analysis:

```{r, eval = F}
n_try = seq(20, 200, 20)
```

```{r, echo = F}
sim_fit = function(n, p_old = 0.10, p_new = 0.25) {
  # create the data
  dead_old = rbinom(n, size = 1, prob = p_old)
  dead_new = rbinom(n, size = 1, prob = p_new)
  # create the predictor variable
  method = rep(c("old", "new"), each = n)
  # create a data.frame to pass to glm
  df = data.frame(dead = c(dead_old, dead_new), method = method)
  # relevel so old is the reference
  df$method = relevel(df$method, ref = "old")
  # fit the model
  fit = glm(dead ~ method, data = df, family = binomial)
  # extract the p-value
  pval = summary(fit)$coef[2,4]
  # determine if it was found to be significant
  sig_pval = pval < 0.05
  # obtain the estimated mortality rate for the new method
  p_new_est = predict(fit, data.frame(method = c("new")),
                      type = "response")
  
  # determine if it is +/- 5% from the true value
  prc_est = p_new_est >= (p_new - 0.05) & p_new_est <= (p_new + 0.05)
  # return a vector with these two elements
  c(sig_pval = sig_pval, prc_est = unname(prc_est))
}

# run the analysis
I = I_power  # the number of replicates at each sample size
n_try = seq(20, 200, 20)  # the test sample sizes
N = length(n_try)      # count them

# containers: 
out_sig = matrix(NA, I, N) # matrix with I rows and N columns
out_prc = matrix(NA, I, N) # matrix with I rows and N columns
for (n in 1:N) {
  for (i in 1:I) {
    tmp = sim_fit(n = n_try[n])     # run sim
    out_sig[i,n] = tmp["sig_pval"]  # extract and store significance metric
    out_prc[i,n] = tmp["prc_est"]   # extract and store precision metric
  }
}

par(mfrow = c(1,2))
plot(apply(out_sig, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of Finding Effect (Power)")
plot(apply(out_prc, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of A Precise Estimate")
```

It appears you need about 100 fish per treatment to be able to detect an effect of this size.

**2.  How do the inferences from the power analysis change if you are interested in `p_new = 0.4` instead of `p_new = 0.25`? Do you need to tag more or fewer fish in this case?**

This is a argument to your function, so simply change its setting when you execute the analysis:

```{r, eval = F}
#...more code above this
tmp = sim_fit(n = n_try[n], p_new = 0.4)
#more code after this...
```

```{r, echo = F}

# run the analysis
I = I_power  # the number of replicates at each sample size
n_try = seq(20, 200, 20)  # the test sample sizes
N = length(n_try)      # count them

# containers: 
out_sig = matrix(NA, I, N) # matrix with I rows and N columns
out_prc = matrix(NA, I, N) # matrix with I rows and N columns
for (n in 1:N) {
  for (i in 1:I) {
    tmp = sim_fit(n = n_try[n], p_new = 0.4)     # run sim
    out_sig[i,n] = tmp["sig_pval"]  # extract and store significance metric
    out_prc[i,n] = tmp["prc_est"]   # extract and store precision metric
  }
}

par(mfrow = c(1,2))
plot(apply(out_sig, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of Finding Effect (Power)")
plot(apply(out_prc, 2, mean) ~ n_try, type = "l",
     xlab = "Tagged Fish per Treatment",
     ylab = "Probability of A Precise Estimate")
```

Because the effect is larger, it is easier to detect with fewer observations.

**3.  Your analysis takes a bit of time to run so you are interested in tracking its progress. Add a progress message to your nested `for()` loop that will print the sample size currently being analyzed.** 

Simply insert the `cat()` line in the appropriate place in your loop. This is a handy trick for long-running simulations.

```{r, eval = F}
for (n in 1:N) {
  cat("\r", "Sample Size = ", n_try[n])
  for (i in 1:I) {
    ...
  }
}
```

### Exercise 4D Solutions {-#ex4d-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  Add an argument to `ricker_sim()` that will give the user an option to create a plot that shows the time series of recruitment, harvest, and escapement all on the same plot. Set the default to be to not plot the result, in case you forget to turn it off before performing the Monte Carlo analysis.**

Change your function to look something like this:

```{r}
ricker_sim = function(ny, params, U, plot = F) {
  # extract the parameters out by name:
  alpha = params["alpha"]
  beta = params["beta"]
  sigma = params["sigma"]
  # create containers:
  # yep, you can do this
  R = S = H = NULL
  # initialize the population in the first year
    # start the population at being fished at 40%
    # with lognormal error
  R[1] = log(alpha * (1 - 0.4))/(beta * (1 - 0.4)) * exp(rnorm(1, 0, sigma))
  S[1] = R[1] * (1 - U)
  H[1] = R[1] * U
  
  # carry simulation forward through time
  for (y in 2:ny) {
    # use the ricker function with random lognormal white noise
    R[y] = S[y-1] * alpha * exp(-beta * S[y-1] + rnorm(1, 0, sigma))
    #harvest and spawners are the same as before
    S[y] = R[y] * (1 - U)
    H[y] = R[y] * U
  }
  
  if (plot) {
    # the I() lets you calculate a quantity within the plot call
    plot(I(R/1e6) ~ seq(1,ny), type = "l", col = "black",
         xlab = "Year", ylab = "State (millions of fish)",
         ylim = range(c(R, S, H)/1e6) + c(0,0.2))
    lines(I(S/1e6) ~ seq(1,ny), col = "blue")
    lines(I(H/1e6) ~ seq(1,ny), col = "red")
    legend("top", legend = c("R", "S", "H"), lty = 1, bty = "n",
           col = c("black", "blue", "red"), horiz = T)
  }
  
  # wrap output in a list object
  list(
    mean_H = mean(H),
    mean_S = mean(S)
    )
}
```

Then use the function:

```{r}
ricker_sim(ny = 20, 
           params = c(alpha = 6,
                      beta = 1e-7,
                      sigma = 0.4),
           U = 0.4, plot = T)
```

**2.  Add an _error handler_ to `ricker_sim()` that will cause the function to return an error `if()` the names of the vector passed to the `param` argument aren't what the function is expecting. You can use stop("Error Message Goes Here") to have your function stop and return an error.**

Error handlers are useful: they catch common errors that someone might make when using your function and return and informative error. Insert this at the top of your function:

```{r, eval = F}
if (!all(names(params) %in% c("alpha", "beta", "sigma"))) {
  stop("the `params` argument must take a named vector
       with three elements: 'alpha', 'beta', and 'sigma'")
}
```

This says, if not all of the names of the `params` argument are in the specified vector, then stop the execution of the function and return the error message.

**3.  How do the results of the trade-off analysis differ if the process error was larger (a larger value of $\sigma$)?**

Simply increase the process error variance term (the `sigma` element of `params`) to be 0.6 and re-run the analysis:

```{r, echo = F}
U_try = seq(0.4, 0.6, 0.01)
params1 = c(alpha = 6, beta = 1e-7, sigma = 0.4)
params2 = c(alpha = 6, beta = 1e-7, sigma = 0.6)

H_out1 = H_out2 = S_out1 = S_out2 = matrix(NA, sr_n_rep, length(U_try))

for (u in 1:length(U_try)) {
  for (i in 1:sr_n_rep) {
    sim1 = ricker_sim(ny = 20, params = params1, U = U_try[u])
    sim2 = ricker_sim(ny = 20, params = params2, U = U_try[u])
    H_out1[i,u] = sim1$mean_H/1e6
    H_out2[i,u] = sim2$mean_H/1e6
    S_out1[i,u] = sim1$mean_S/1e6
    S_out2[i,u] = sim2$mean_S/1e6
  }
}

Smeet1 = S_out1 > (0.75 * 13)
Smeet2 = S_out2 > (0.75 * 13)
Hmeet1 = H_out1 > (1.2 * 8.5)
Hmeet2 = H_out2 > (1.2 * 8.5)
p_Smeet1 = apply(Smeet1, 2, mean)
p_Smeet2 = apply(Smeet2, 2, mean)
p_Hmeet1 = apply(Hmeet1, 2, mean)
p_Hmeet2 = apply(Hmeet2, 2, mean)

# the U levels to highlight on plot
plot_U = seq(0.4, 0.6, 0.05)
# create an empty plot
par(mar = c(4,5,1,1))
plot(p_Smeet1 ~ p_Hmeet1, type = "n",
     xlab = "Probability of Meeting Harvest Criterion",
     ylab = "Probability of Meeting Escapement Criterion")
# add gridlines
abline(v = seq(0, 1, 0.1), col = "grey")
abline(h = seq(0, 1, 0.1), col = "grey")
#draw on the tradeoff curve
lines(p_Smeet1 ~ p_Hmeet1, type = "l", lwd = 2)
lines(p_Smeet2 ~ p_Hmeet2, type = "l", lwd = 2, col = "blue")
# add points and text for particular U policies
points(p_Smeet1[U_try %in% plot_U] ~ p_Hmeet1[U_try %in% plot_U],
       pch = 16, cex = 1.5)
text(p_Smeet1[U_try %in% plot_U] ~ p_Hmeet1[U_try %in% plot_U],
     labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2))
points(p_Smeet2[U_try %in% plot_U] ~ p_Hmeet2[U_try %in% plot_U],
       pch = 16, cex = 1.5, col = "blue")
text(p_Smeet2[U_try %in% plot_U] ~ p_Hmeet2[U_try %in% plot_U],
     labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2), col = "blue")
```

The blue line is the higher process error scenario. It seems that if the process error was higher, you would need to sacrifice more in the escapement objective to obtain the same level of the harvest objective than in the case with lower process error. This makes sense: more variability means more iterations will have escapement less than the criterion. 

**4.  Add implementation error to the harvest policy. That is, if the target exploitation rate is $U$, make the real exploitation rate in year $y$ be: $U_y \sim Beta(a,b)$, where $a = 50U$ and $b = 50(1-U)$. You can make there be more implementation error by inserting a smaller number other than 50 here. How does this affect the trade-off analysis?**

For this, add a `B_sum` argument to your function (this represents the 50 value in the question) and use a different randomly generated exploitation rate each year according to the directions:

```{r, eval = F}
# ... more code above
U_real = rbeta(ny, Bsum * U, Bsum * (1 - U))
# more code below...
```

You can use this instead of a fixed exploitation rate each year, for example:

```{r, eval = F}
# ...inside a loop
S[y] = R[y] * (1 - U_real[y])
H[y] = R[y] * U_real[y]
# end of a loop...
```

```{r, echo = F}
ricker_sim = function(ny, params, U, B_sum, plot = F) {
  
  if (!all(names(params) %in% c("alpha", "beta", "sigma"))) {
    stop("the `params` argument must take a named 
         vector with three elements: 'alpha', 'beta', and 'sigma'")
  }
  
  # obtain the actual exploitation rates each year
  # managers shoot for U, but because of imperfect implementation
  # and errors in abundance estimation, it varies
  U_real = rbeta(ny, B_sum * U, B_sum * (1 - U))
  
  # extract the parameters out by name:
  alpha = params["alpha"]
  beta = params["beta"]
  sigma = params["sigma"]
  # create containers:
  # yep, you can do this
  R = S = H = NULL
  # initialize the population in the first year
    # start the population at being fished at 40%
    # with lognormal error
  R[1] = log(alpha * (1 - 0.4))/(beta * (1 - 0.4)) * exp(rnorm(1, 0, sigma))
  S[1] = R[1] * (1 - U_real[1])
  H[1] = R[1] * U_real[1]
  
  # carry simulation forward through time
  for (y in 2:ny) {
    # use the ricker function with random lognormal white noise
    R[y] = S[y-1] * alpha * exp(-beta * S[y-1] + rnorm(1, 0, sigma))
    #harvest and spawners are the same as before
    S[y] = R[y] * (1 - U_real[y])
    H[y] = R[y] * U_real[y]
  }
  
  if (plot) {
    plot(I(R/1e6) ~ seq(1,ny), type = "l", col = "black",
         xlab = "Year", ylab = "State (millions of fish)",
         ylim = range(c(R, S, H)/1e6) + c(0,0.2))
    lines(I(S/1e6) ~ seq(1,ny), col = "blue")
    lines(I(H/1e6) ~ seq(1,ny), col = "red")
    legend("top", legend = c("R", "S", "H"), lty = 1, bty = "n",
           col = c("black", "blue", "red"), horiz = T)
  }
  
  # wrap output in a list object
  list(
    mean_H = mean(H),
    mean_S = mean(S)
    )
}
```

```{r, echo = F}
U_try = seq(0.4, 0.6, 0.01)
params = c(alpha = 6, beta = 1e-7, sigma = 0.4)
H_out1 = H_out2 = S_out1 = S_out2 = matrix(NA, sr_n_rep, length(U_try))

for (u in 1:length(U_try)) {
  for (i in 1:sr_n_rep) {
    sim1 = ricker_sim(ny = 20, B_sum = 1e6, params = params, U = U_try[u])
    sim2 = ricker_sim(ny = 20, B_sum = 50, params = params, U = U_try[u])
    H_out1[i,u] = sim1$mean_H/1e6
    H_out2[i,u] = sim2$mean_H/1e6
    S_out1[i,u] = sim1$mean_S/1e6
    S_out2[i,u] = sim2$mean_S/1e6
  }
}

Smeet1 = S_out1 > (0.75 * 13)
Smeet2 = S_out2 > (0.75 * 13)
Hmeet1 = H_out1 > (1.2 * 8.5)
Hmeet2 = H_out2 > (1.2 * 8.5)
p_Smeet1 = apply(Smeet1, 2, mean)
p_Smeet2 = apply(Smeet2, 2, mean)
p_Hmeet1 = apply(Hmeet1, 2, mean)
p_Hmeet2 = apply(Hmeet2, 2, mean)

# the U levels to highlight on plot
plot_U = seq(0.4, 0.6, 0.05)
# create an empty plot
par(mar = c(4,5,1,1))
plot(p_Smeet1 ~ p_Hmeet1, type = "n",
     xlab = "Probability of Meeting Harvest Criterion",
     ylab = "Probability of Meeting Escapement Criterion")
# add gridlines
abline(v = seq(0, 1, 0.1), col = "grey")
abline(h = seq(0, 1, 0.1), col = "grey")
#draw on the tradeoff curve
lines(p_Smeet1 ~ p_Hmeet1, type = "l", lwd = 2)
lines(p_Smeet2 ~ p_Hmeet2, type = "l", lwd = 2, col = "red")
# add points and text for particular U policies
points(p_Smeet1[U_try %in% plot_U] ~ p_Hmeet1[U_try %in% plot_U],
       pch = 16, cex = 1.5)
text(p_Smeet1[U_try %in% plot_U] ~ p_Hmeet1[U_try %in% plot_U],
     labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2))
points(p_Smeet2[U_try %in% plot_U] ~ p_Hmeet2[U_try %in% plot_U],
       pch = 16, cex = 1.5, col = "red")
text(p_Smeet2[U_try %in% plot_U] ~ p_Hmeet2[U_try %in% plot_U],
     labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2), col = "red")
```

The red line has `B_sum = 50` and the black line has `B_sum = 1e6` (really large `B_sum` reduces the variance of `U_real` around `U`). It appears that introducing random implementation errors has a similar effect as increasing the process variance. To acheive the same harvest utility as the case with no implementation error, you need to be willing to sacrifice more in terms of escapement utility.

See what happens if you increase or decrease the amount of implementation error the management system has in acheiving a target exploitation rate.

**5. Visually show how variable beta random variable is with `B_sum = 50`.**

Here are two ways that come to mind:

```{r}
# boxplots at various levels of U target
B_sum = 50
out = sapply(U_try, function(x) {
  rbeta(n = 1000, x * B_sum, B_sum * (1 - x))
})
colnames(out) = U_try
boxplot(out, outline = F, col = "goldenrod1")

# a time series at one level of U target
U = 0.50
plot(rbeta(20, U * B_sum, B_sum * (1 - U)), type = "b", col = "red", pch = 15)
abline(h = U, col = "blue", lty = 2, lwd = 2)
```

### Exercise 4E Solutions {-#ex4e-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  Replicate the bootstrap analysis but adapted for the linear regression example in Section \@ref(regression). Stop at the step where you summarize the 95% interval range.**

First, read in the `sockeye.csv` data set:

```{r, eval = F}
dat = read.csv("../Data/sockeye.csv")
```

```{r, echo = F}
dat = read.csv("Data/sockeye.csv")
```

Then, copy the three functions and change the model to be a linear regression for these data:

```{r}
randomize = function(dat) {
  # number of observed pairs
  n = nrow(dat)
  # sample the rows to determine which will be kept
  keep = sample(x = 1:n, size = n, replace = T)
  # retreive these rows from the data
  dat[keep,]
}

fit_lm = function(dat) {
  lm(fecund ~ weight, data = dat)
}

# create a vector of weights
weights = seq(min(dat$weight, na.rm = T),
              max(dat$weight, na.rm = T),
              length = 100)
pred_lm = function(fit) {
  # extract the coefficients
  ests = coef(fit)
  # predict length-at-age
  ests["(Intercept)"] + ests["weight"] * weights
}
```

Then perform the bootstrap by replicating these functions many times:

```{r}
out = replicate(n = 5000, expr = {
  pred_lm(fit = fit_lm(dat = randomize(dat = dat)))
})
```

Finally, summarize and plot them:
```{r, eval = F}
summ = apply(out, 1, function(x) c(mean = mean(x), quantile(x, c(0.025, 0.975))))

plot(summ["mean",] ~ weights, type = "l", ylim = range(summ))
lines(summ["2.5%",] ~ weights, col = "grey")
lines(summ["97.5%",] ~ weights, col = "grey")
```

**2.  Compare the 95% bootstrap confidence intervals to the intervals you get by running the `predict` function on the original data set with the argument `interval = "confidence"` set.**

You can obtain these same intervals using the `predict()` function:

```{r, eval = F}
pred = predict(lm(fecund ~ weight, data = dat),
               newdata = data.frame(weight = weights),
               interval = "confidence")
```

Plot them over top of the bootstrap intervals to verify the bootstrap worked right:

```{r, echo = F, eval = F}
plot(summ["mean",] ~ weights, type = "l", ylim = range(summ))
lines(summ["2.5%",] ~ weights, col = "grey")
lines(summ["97.5%",] ~ weights, col = "grey")
lines(pred[,"lwr"] ~ weights, col = "red")
lines(pred[,"upr"] ~ weights, col = "red")
```

The intervals should look approximately correct.

### Exercise 4F Solutions {-#ex4f-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**1.  Adapt the code to perform a permutation test for the difference in each of the zooplankton densities between treatments. Don't forget to fix the missing value in the `chao` variable. See [Exercise 2](#ex1b) for more details on this.**

Read in the data:

```{r, eval = F}
dat = read.csv("../Data/ponds.csv")
```

```{r, echo = F}
dat = read.csv("Data/ponds.csv")
```

Calculate the difference in means between the treatments for each zooplankton taxon:

```{r}
Dobs_daph = mean(dat$daph[dat$treatment == "Add"]) - 
  mean(dat$daph[dat$treatment == "Control"])
Dobs_bosm = mean(dat$bosm[dat$treatment == "Add"]) - 
  mean(dat$bosm[dat$treatment == "Control"])
Dobs_cope = mean(dat$cope[dat$treatment == "Add"]) - 
  mean(dat$cope[dat$treatment == "Control"])
Dobs_chao = mean(dat$chao[dat$treatment == "Add"], na.rm = T) - 
  mean(dat$chao[dat$treatment == "Control"], na.rm = T)
```

You can use the same `perm()` function from the chapter to perform this analysis, except you should add an `na.rm` argument. All you need to change is the variables you pass to `y`:

```{r}
perm = function(x, y, na.rm = T) {
  # turn x to a character, easier to deal with
  x = as.character(x)
  # shuffle the x values:
  x_shuff = sample(x)
  # calculate the mean of each group:
  x_bar_add = mean(y[x_shuff == "Add"], na.rm = na.rm)
  x_bar_ctl = mean(y[x_shuff == "Control"], na.rm = na.rm)
  # calculate the difference:
  x_bar_add - x_bar_ctl
}
```

Then, run the permutation test on each taxon separately:

```{r}
Dnull_daph = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$daph))
Dnull_bosm = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$bosm))
Dnull_cope = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$cope))
Dnull_chao = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$chao, na.rm = T))
```

Then, create the histograms for each species showing the null distribution and the observed difference:

```{r}
par(mfrow = c(1,4), mar = c(2,0,2,0))
hist(Dnull_daph, col = "skyblue", main = "DAPH", yaxt = "n")
abline(v = Dobs_daph, col = "red", lwd = 3)

hist(Dnull_bosm, col = "skyblue", main = "BOSM", yaxt = "n")
abline(v = Dobs_bosm, col = "red", lwd = 3)

hist(Dnull_cope, col = "skyblue", main = "COPE", yaxt = "n")
abline(v = Dobs_cope, col = "red", lwd = 3)

hist(Dnull_chao, col = "skyblue", main = "CHAO", yaxt = "n")
abline(v = Dobs_chao, col = "red", lwd = 3)
```

Finally, calculate the two-tailed hypothesis tests:

```{r}
mean(abs(Dnull_daph) >= Dobs_daph)
mean(abs(Dnull_bosm) >= Dobs_bosm)
mean(abs(Dnull_cope) >= Dobs_cope)
mean(abs(Dnull_chao) >= Dobs_chao)
```

It appears that the zooplankton densities differed significantly between treatments for each taxon.

**2.  Adapt the code to perform a permutation test for another data set used in this book where there are observations of both a categorical variable and a continuous variable. The data sets `sockeye.csv`, `growth.csv`, or `creel.csv` should be good starting points.**

This example will test the difference in mean length between age 3 and 4 fish in the `growth.csv` data set.

Read in the data and extract only age 3 and 4 fish:

```{r, eval = F}
dat = read.csv("../Data/growth.csv")
dat = dat[dat$age %in% c(3,4),]
```

```{r, echo = F}
dat = read.csv("Data/growth.csv")
dat = dat[dat$age %in% c(3,4),]
```

Calculate the observed difference in means (age 4 - age 3):

```{r}
Dobs = mean(dat$length[dat$age == 4]) - mean(dat$length[dat$age == 3])
```

Adapt the `perm()` function for this example (no `na.rm` argument is needed):

```{r}
perm = function(x, y) {
  # shuffle the x values:
  x_shuff = sample(x)
  # calculate the mean of each group:
  x_bar_3 = mean(y[x_shuff == 3])
  x_bar_4 = mean(y[x_shuff == 4])
  # calculate the difference:
  x_bar_4 - x_bar_3
}

```

Then, use the function:

```{r}
Dnull = replicate(n = 5000, expr = perm(x = dat$age, y = dat$length))
```

Plot the null distribution:

```{r}
hist(Dnull, col = "skyblue")
abline(v = Dobs, col = "red", lwd = 3)
```

Obtain the two-tailed p-value:

```{r}
mean(abs(Dnull) >= Dobs)
```

Based on this, it appears there is no significant difference between the mean length of age 3 and 4 fish in this example.

**3.  Add a calculation of the p-value for a one-tailed test (i.e., that the difference in means is greater or less than zero). Steps 1 - 4 are the same: all you need is `Dnull` and `Dobs`. Don't be afraid to Google this if you are confused.**

To calculate the p-value for a one-tailed test (null hypothesis is that the mean length of age 4 is less than or equal to the mean length of age 3 fish):

```{r}
mean(Dnull >= Dobs)
```

To test the opposite hypothesis (null hypothesis is that the mean length of age 4 fish is greater than or equal to the mean length of age 3 fish):

```{r}
mean(Dnull <= Dobs)
```

## Exercise 5 Solutions {-#ex5-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

First, load `{dplyr}` and read in the data:

```{r, eval = F}
library(dplyr)
dat = read.csv("../Data/asl.csv")
head(dat)
```

```{r, echo = F, message = F, warning = F}
library(dplyr)
dat = read.csv("Data/asl.csv")
head(dat)
```

**1.  Count the number of females that were sampled each year using the `{dplyr}` function `n()` within a `summarize()` call (_Hint: `n()` works just like length - it counts the number of records_).**

Pipe `dat` to a `filter()` call to extract only females, then `group_by()` year, then count the number of records:

```{r}
n_female = dat %>%
  filter(sex == "Female") %>%
  group_by(year) %>%
  summarize(n_female = n())
```

**2. Calculate the proportion of females by year.**

Do a similar task as in the previous question, but count all individuals each year, regardless of sex:

```{r}
n_tot = dat %>%
  group_by(year) %>%
  summarize(n_tot = n())
```

Then merge these two data sets:

```{r}
samp_size = merge(n_tot, n_female, by = "year")
```

Finally, calculate the fraction that were females:

```{r}
samp_size = samp_size %>%
  mutate(p_female = n_female/n_tot)
```

**3. Plot percent females over time. Does it look like the sex composition has changed over time?**

```{r}
plot(p_female ~ year, data = samp_size, type = "b")
```

It seems that the proportion of females has varied randomly over time but has not systematically changed since the beginning of the data time series. Note that some of this variability is introduced by sampling.

**4. Calculate mean length by age, sex, and year.**

`{dplyr}` makes this relatively complex calculation easy and returns an intuitive data frame as output:

```{r}
mean_length = dat %>% 
  group_by(year, age, sex) %>%
  summarize(mean_length = mean(length))
  
```

**5. Come up with a way to plot a time series of mean length-at-age for both sexes. Does it look like mean length-at-age has changed over time for either sex?**

```{r, results = "hide", fig.align = "center"}

# set up a 2x2 plotting device with specialized margins
par(mfrow = c(2,2), mar = c(2,2,2,2), oma = c(2,2,0,0))

# extract the unique ages
ages = unique(mean_length$age)

# use sapply() to "loop" over ages
sapply(ages, function(a) {
  # create an empty plot
  plot(1,1, type = "n", ann = F, xlim = range(dat$year),
       ylim = range(filter(mean_length, age == a)$mean_length))
  
  # add a title
  title(paste("Age", a))
  
  # draw on the lines for each sex
  lines(mean_length ~ year, type = "b", pch = 16,
        data = filter(mean_length, age == a & sex == "Male"))
  lines(mean_length ~ year, type = "b", pch = 1, lty = 2,
        data = filter(mean_length, age == a & sex == "Female"))
  
  # draw a legend if the age is 4
  if (a == 4) {
    legend("topright", legend = c("Male", "Female"),
           lty = c(1,2), pch = c(16,1), bty = "n")
  }
})

# add on margin text for the shared axes
mtext(side = 1, outer = T, "Year")
mtext(side = 2, outer = T, "Mean Length (mm)")
```

**Bonus 1. Calculate the age composition by sex and year (what proportion of all the males in a year were age 4, age 5, age 6, age 7, and same for females).**

This follows a similar workflow as in the calculation of the proportion of females:

```{r}
# get the number by age and sex each year
n_age_samps = dat %>% 
  group_by(year, age, sex) %>%
  summarize(age_samps = n())

# get the number by sex each year
n_tot_samps = dat %>% 
  group_by(year, sex) %>%
  summarize(tot_samps = n())

# merge the two:
n_samps = merge(n_age_samps, n_tot_samps, by = c("year", "sex"))

# calculate the proportion at each age and sex:
n_samps = n_samps %>%
  ungroup() %>%
  mutate(age_comp = age_samps/tot_samps)
```

**Bonus 2. Plot the time series of age composition by sex and year. Does it look like age composition has changed over time?**

Create the same plots as for Question 5, but with age composition instead of sex composition:

```{r, results = "hide", fig.align = "center"}
# set up a 2x2 plotting device with specialized margins
par(mfrow = c(2,2), mar = c(2,2,2,2), oma = c(2,2,0,0))

# extract the unique ages
ages = unique(n_samps$age)

# use sapply() to "loop" over ages
sapply(ages, function(a) {
  # create an empty plot
  plot(1,1, type = "n", ann = F, xlim = range(dat$year),
       ylim = range(filter(n_samps, age == a)$age_comp) + c(0, 0.05))
  
  # add a title
  title(paste("Age", a))
  
  # draw on the lines for each sex
  lines(age_comp ~ year, type = "b", pch = 16,
        data = filter(n_samps, age == a & sex == "Male"))
  lines(age_comp ~ year, type = "b", pch = 1, lty = 2,
        data = filter(n_samps, age == a & sex == "Female"))
  
  # draw a legend if the age is 4
  if (a == 4) {
    legend("topleft", legend = c("Male", "Female"),
           lty = c(1,2), pch = c(16,1), bty = "n")
  }
})

# add on margin text for the shared axes
mtext(side = 1, outer = T, "Year")
mtext(side = 2, outer = T, "Age Composition by Sex")
```

## Exercise 6 Solutions {-#ex6-answers}

```{r, echo = F}
rm(list = setdiff(ls(), sdiff))
```

**Note**: these solutions assume you have the layers `bear`, `stat`, `slovenia`,  `railways`, and `cp` as well as all of the packages used in Chapter \@ref(ch6) already loaded into your workspace.

```{r, echo = F}
library(dplyr)
library(sp)
library(rgdal)
library(scales)
library(maptools)
library(adehabitatHR)

# read in/manipulate bear data
bear = read.csv("Data/Ch6/bear.csv", stringsAsFactors = F)
bear = na.omit(bear[,c("timestamp","tag.local.identifier",
                       "location.long","location.lat")])
colnames(bear) = c("timestamp","ID","x","y")
bear = bear %>%
  group_by(ID) %>%
  filter(n() >= 5)

bear = SpatialPointsDataFrame(
  data = bear,
  coords = bear[,c("x","y")],
  coords.nrs = c(3,4),
  proj4string = CRS("+init=epsg:4326")
)

# read in other spatial objects
slovenia = readOGR(dsn="./Data/Ch6/SVN_adm",layer="SVN_adm0")
railways = readOGR(dsn = "./Data/Ch6/railways", layer = "railways")
stats = readOGR(dsn = "./Data/Ch6/SVN_adm", layer = "SVN_adm1", stringsAsFactors = F)

slovenia = spTransform(slovenia, CRS(proj4string(bear)))

bear = bear[slovenia,]

bear = spTransform(bear,CRS("+proj=utm +north +zone=33 +ellps=WGS84"))
# calculate the home range. mcp is one of 5 functions to do this
cp = mcp(xy=bear[,2], percent=95,unin="m",unout="km2") 

bear = spTransform(bear, CRS(proj4string(bear)))
```

**1.  Load the `caves.shp` shapefile [@caves-cite] from your working directory. Add the data to one of the maps of Slovenia you created in this chapter.**

```{r, eval = F}
# load the caves file
caves = readOGR(dsn="./caves",layer="EPO_JAMEPoint")
# put all layers the same projection as bear data
caves = spTransform(caves, CRS(proj4string(bear)))
stats = spTransform(stats, CRS(proj4string(bear)))
slovenia = spTransform(slovenia, CRS(proj4string(bear)))
railways = spTransform(railways, CRS(proj4string(bear)))
# make the plot
par(mar = c(2,2,1,1))
plot(stats, border = "grey", axes = T,
     xlim = bear@bbox[1,],  # access the boundary box using @
     ylim = bear@bbox[2,])
plot(slovenia, lwd = 3, add = T)
points(bear, pch = 16, cex = 0.5, col = alpha("blue", 0.5))
plot(railways, add = T, col = "red", lwd = 3)
# add the caves layer
plot(caves, add=T)
```

```{r, echo = F}
# load the caves file
caves = readOGR(dsn="./Data/Ch6/caves",layer="EPO_JAMEPoint")
# put all layers the same projection as bear data
caves = spTransform(caves, CRS(proj4string(bear)))
stats = spTransform(stats, CRS(proj4string(bear)))
slovenia = spTransform(slovenia, CRS(proj4string(bear)))
railways = spTransform(railways, CRS(proj4string(bear)))
# make the plot
par(mar = c(2,2,1,1))
plot(stats, border = "grey", axes = T,
     xlim = bear@bbox[1,],  # access the boundary box using @
     ylim = bear@bbox[2,])
plot(slovenia, lwd = 3, add = T)
points(bear, pch = 16, cex = 0.5, col = alpha("blue", 0.5))
plot(railways, add = T, col = "red", lwd = 3)
# add the caves layer
plot(caves, add=T, pch = 16, col = "green", cex = 2)
```

**2.  How many caves are there?**

```{r}
nrow(caves)
```

**3.  How many caves are in each statistical area?**

For this, you will need to add a field to your attribute table. Look back to Section \@ref(add-attr) for details on how to do this.

```{r}
library(maptools)
caves = spTransform(caves, proj4string(stats))
cavestats = over(caves, stats)
caves = spCbind(caves,cavestats$NAME_1)
table(caves$cavestats.NAME_1)
```

**4.  Which bear has the most caves in its homerange?**

First, use the `over()` function to determine which homeranges have which caves (the `returnList = T` argument will show all of the polygons each point falls in).

```{r}
cp = spTransform(cp, CRS(proj4string(caves)))
homecaves = over(caves, cp, returnList = T)
```

Now just count how many caves were in each bear's homerange (the bear names are the rownames of `homecaves`).

```{r}
table(unlist(sapply(homecaves,rownames)))
```

<!--chapter:end:07-exercise-solutions.Rmd-->

