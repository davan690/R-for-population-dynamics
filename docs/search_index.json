[
["index.html", "Introduction to R for Natural Resource Scientists Overview What is Covered? Prerequisites Exercises Text Conventions Keyboard Shortcuts Development of this Book About the Author", " Introduction to R for Natural Resource Scientists Ben Staton with contributions by Henry Hershey Overview This book is intended to be a first course in R programming for natural resource professionals. It is by no means comprehensive (no book about R ever could be), but instead attempts to introduce the main topics needed to get a beginner up and running with applying R to their own work. It is intended to be a companion to in-person workshop sessions, in which each chapter is covered in a 2 hour session, however it can be used as “self-teach” manual as well. Although the examples shown have a natural resource/ecological theme, the general skills presented are general to R users across all scientific disciplines. What is Covered? The book is composed of six chapters intended to cover a suite of topics in introductory R programming. In general, the material builds in complexity from chapter to chapter and earlier chapters can be seen as prerequisites for later chapters. Chapter 1 covers the basics of working in R through RStudio, including the basics of the R coding language and environment. Chapter 2 covers the basics of plotting using the base R graphics functionality. Chapter 3 covers the basics of fitting statistical models using built-in functionality for generalized linear models as well as non-linear models. Chapter 4 covers the basics of simulation modeling in R. Chapter 5 covers the basics of the {dplyr} and {reshape2} packages for manipulating and summarizing large data sets using highly readable code. Chapter 6 covers the basics of producing maps and performing spatial analysis in R. This chapter was contributed by Henry Hershey Prerequisites Chapter 1 starts at the first step (installing R) and progresses by assuming no prior knowledge of programming in R or in any other language. In the later chapters, e.g., Chapters 3 and 4, an understanding of statistics at the introductory undergraduate level would be helpful but not strictly essential. There are, however, some tasks you’ll need to complete before using this book, which are described in the two sections that follow. Prepare Your Computer Installation First off, you will need to get R and RStudio1 onto your computer. Go to: https://cran.rstudio.com/ to get R and https://www.rstudio.com/products/rstudio/download/ to get RStudio Desktop. Download the appropriate installation file for your operating system and run that file. All default settings should be fine. Optional Configuration As a matter of personal preference, you are recommended to configure a few settings. Open up RStudio and go to Tools &gt; Global Options, and in the section listed “General”: Make sure Restore .RData into workspace at startup is unchecked Make sure Save workspace to .RData on exit is set to Never Make sure Always save history (even when not saving .RData) is unchecked. These settings will prevent you from getting a bunch of useless files and dialog boxes every time you open and close R. Create a Book Directory You should create a devoted folder on your computer for this book. All examples will assume this folder is located here: C:/Users/YOU/Documents/R-Book. Change YOU to be specific for your computer. Data Sets The data sets2 used in this book are hosted on a GitHub repository maintained by the author. It is located here: https://github.com/bstaton1/au-r-workshop-data. To acquire the data for this book, you should: Navigate to the GitHub repository click the green Clone or download button at the top right, click Download ZIP unzip the contents of this folder into the location: C:/Users/YOU/Documents/R-Book/Data File organization will be very important for your success in learning to use R. This book will assume your R-Book directory is organized as shown below. Notice that there is a separate folder for the data downloaded from GitHub as well as one for each chapter that will house the R code for that chapter. Do not worry about making all of these folders now, you will do this at the appropriate time as you work your way through this book. For now, just make sure there is a Data folder that contains all of the unzipped contents from the GitHub repository your main R-Book directory. ## levelName ## 1 R-Book ## 2 ¦--Data ## 3 ¦ ¦--asl.csv ## 4 ¦ ¦--... ## 5 ¦ °--Ch6 ## 6 ¦--Chapter1 ## 7 ¦ ¦--Ch1.R ## 8 ¦ ¦--Ex1A.R ## 9 ¦ °--Ex1B.R ## 10 ¦--Chapter2 ## 11 ¦ ¦--Ch2.R ## 12 ¦ °--Ex2.R ## 13 ¦--Chapter3 ## 14 ¦--Chapter4 ## 15 ¦--Chapter5 ## 16 °--Chapter6 Exercises Following each chapter, there is a set of exercises. You should attempt and complete them, as they give you an opportunity to practice what you learned while reading and typing along. Solutions are provided at the end of this book, however you are strongly recommended to attempt to figure the problems out on your own before looking to how the author would solve them. Some exercises have bonus questions. These are intended to challenge you with some of the more difficult tasks shown in the chapter or ask you to extend what you learned to a completely different problem. If you can get all of the non-bonus questions without looking at the solutions too much, you can consider yourself to have good understanding of that chapter’s material. If you can complete the bonus questions with little or no help, that means you have mastered that chapter’s material! Text Conventions Regular text: a description of what you you should do, how some code works, or a general narrative of something. monospace: references something in R this() references some function this references some other object {this} references an R package C:/This is a file path Bold is intended to provide more emphasis to a word or topic. In general, new topics are introduced this way. Links: this is a link to some other location in this book. External links are provided with a full URL. \\(Equations\\): it is sometimes useful to describe concepts mathematically before showing how to do it in R. 3: a footnote containing more information. Keyboard Shortcuts Several parts of this book in this book make reference to keyboard shortcuts. They are never necessary, but can help you be more efficient if you commit them to muscle memory. This book assumes you are using a PC for the keyboard shortcuts. If you are using a Mac, they will be different4. For a complete list of RStudio’s keyboard shortcuts specific to your operating system, go to Help &gt; Keyboard Shortcuts Help. Development of this Book This book represents the third reincarnation of the Auburn R Workshop Series. The first version was written in Fall 2014 using Microsoft Word, but the author found that making even small changes was clunky - each change to code in the document required a copy-paste of code and output from R to Word. Individual session materials (i.e., handout, exercises, solutions, data) were created in separate documents, saved as PDFs and .xlsx files, and uploaded to a wordpress webpage. The second version was written through R (R Core Team 2018) and RStudio using the R packages {rmarkdown} (Allaire et al. 2018) and {knitr} (Xie 2018b, 2015), which allowed the integration of text, code, and output all into one output file. This version was completed in Fall 2015. Like the first version, individual session materials were created in separate documents, and replaced those previously found on the wordpress site. This third version was written through R and RStudio but used the R package {bookdown} (Xie 2018a) which allowed for the individual sessions to be combined into one “book” by turning each session into a chapter. This facilitated cross-references to topics covered in previous chapters and allows the reader to only refer to one location when trying to remember how to use a skill. It also allowed for multiple formats to be published including both HTML and PDF versions. The book is hosted on GitHub Pages, and was last built on 10-31-2019. About the Author Ben Staton is a PhD candidate in the School of Fisheries at Auburn University. He studies quantitative methods for assessing fish populations for use in harvest management, with a focus on Pacific salmon in western Alaska. His interests are in population dynamics, Bayesian methods, Monte Carlo methods, and reproducible research. Ben has been using R on a daily basis since the beginning of his graduate work in 2014, and is enthusiastic about helping others learn to use R for their own work. References "],
["ch1.html", "Chapter 1 Introduction to the R Environment Chapter Overview Before You Begin 1.1 The R Studio Interface 1.2 Saving Your Code: Scripts 1.3 The Working Directory 1.4 R Object Types 1.5 Factors 1.6 Vector Math 1.7 Data Subsets/Queries Exercise 1A 1.8 Read External Data Files 1.9 Explore the Data Set 1.10 Logical/Boolean Operators 1.11 Logical Subsetting 1.12 if(), else, and ifelse() 1.13 Writing Output Files 1.14 User-Defined Functions Exercise 1B", " Chapter 1 Introduction to the R Environment Chapter Overview In this first chapter, you will get familiar with the basics of using R. You will learn: how to use R as a basic calculator some basic object types some basic data classes some basic data structures how to read in data how to produce basic data summaries how to write out data how to write your own functions Before You Begin Before you start this chapter, you should make sure you have read and done everything in the Overview pertaining to preparing your computer for working through this book. 1.1 The R Studio Interface When you open up RStudio for the first time, you will see three panes: the left-hand side is the console where results from executed commands are printed, and the two panes on the right are for additional information to help you code more efficiently - don’t worry too much about what these are at the moment. For now, focus your attention on the console. 1.1.1 Write Some Simple Code To start off, you will use R as a simple calculator. Type these commands (not the lines with ##, those are output5) one at a time and hit CTRL + ENTER to run it. The spaces don’t matter at all, they are used here for clarity and for styling. To learn more about standard R code styling, check out the section in Wickham (2015) on it6. 3 + 3 ## [1] 6 12/4 ## [1] 3 Notice that when you run each line, it prints the command and the output to the console. R is an object oriented language, which means that you fill objects with data and do things with them. Make an object called x that stores the result of the calculation 3 + 3 (type this and run using CTRL + ENTER): x = 3 + 3 Notice that running this line did not return a value as before. This is because in that line you are assigning a value to the object x. You can view the contents of x by typing its name alone and running just that: x ## [1] 6 When used this way, the = sign denotes assignment of the value on the right-hand side to an object with the name on the left-hand side. The &lt;- serves this same purpose so in this context the two are interchangeable: y &lt;- 2 + 5 You can highlight smaller sections of a line to run as well. For example after creating y above, press the up arrow to see the line you just ran, highlight just the y, and press CTRL + ENTER. From this point forward, the verb “run” means execute some code using CTRL + ENTER. You can use your objects together to make a new object: z = y - x Here are some things to note about object names: Object names can contain any of the following: letters numbers the . or _ symbols Object names must start with a letter, not a number or symbol and cannot contain spaces As a general rule, avoid naming your objects things that already have names in R, e.g., data(), mean(), sum(), sd(), etc. Capitalization matters: A and a are two different objects Keep your names short with abbreviations or shorthand 1.2 Saving Your Code: Scripts If you closed R at this moment, your work would be lost. Running code in the console like you have just done does not save a record of your work. To save R code, you must use what is called a script, which is a plain-text file with the extension .R. To create a new script file, go to File &gt; New File &gt; R Script, or use the keyboard shortcut CTRL + SHIFT + N. A new pane will open called the source pane - this is where you will edit your code and save your progress. R Scripts are a key feature of reproducible research with R, given that if they are well-written they can present a complete road map of your statistical analysis and workflow. 1.3 The Working Directory Keeping things organized is essential to efficient use of R. For this book, you should have a separate subfolder for your scripts in each chapter. Create a subfolder called C:/Users/YOU/Documents/R-Book/Chapter1 and save your Ch1.R script there. Part of keeping your work organized in R is making sure you know where R is looking for your files. One way to facilitate this is to use a working directory. This is the location (i.e., folder) on your computer that your current R session will “talk to” by default. R will read files from and write files to the working directory by default. Because you’ll likely be visiting it often, it should probably be somewhere that is easy to remember and not too deeply buried in your computer’s file system. To set the working directory to C:/Users/YOU/Documents/R-Book/Chapter1, you have three options: Go to Session &gt; Set Working Directory &gt; Source File Location. This will set the working directory to the location of the file that is currently open in your source pane. Go to Session &gt; Set Working Directory &gt; Choose Directory. This will open an interactive file selection window to allow you to navigate to the desired directory. Use code. In the console, you can type setwd(\"C:/Users/YOU/Documents/R-Book/Chapter1\"). If at any point you want to know where your current working directory is set to, you can either look at the top of the console pane, which shows the full path or by running getwd() in the console. Note the use of / rather than \\ for file paths in R. If you are using a Mac, omit C: from your directory name. The main benefits of using a working directory are: Files are read from and written to a consistent and predictable place every time Everything for your analysis is organized into one place on your computer You don’t have to continuously type file paths to your work. If file.txt is a file in your current working directory, you can reference it your R session using \"file.txt\" rather than with \"C:/Users/YOU/Documents/R-Book/Chapter1/file.txt\" each time. Note that while it is generally good practice to keep your data in the working directory, this is not recommended for this book. You will be using the same data files in multiple chapters, so it will help if they are all stored in one location. More details on this later (Section 1.8). 1.4 R Object Types R has a variety of object types that you will need to become familiar with. 1.4.1 Functions Much of your work in R will involve functions. A function is called using the syntax: fun(arg1 = value1, arg2 = value2) Here, fun() is the function name and arg1 and arg2 are called arguments. Functions take input in the form of the arguments, do some task with them, then return some output. The parentheses are a sure sign that fun() is a function. The syntax above passes the function two arguments by name: all functions have arguments, all arguments have names, and there is always a default order to the arguments. If you memorize the argument order of functions you use frequently, you don’t have to specify the argument names: fun(value1, value2) would give the same result as the command above in which the argument names were specified. Here’s a real example: print(x = z) ## [1] 1 The function is print(), the argument is x, and the value you have supplied the argument is the object z. The task that print() does is to print the value of z to the console. R has a ton of built-in documentation to help you learn how to use a function. Take a look at the help file for the mean() function. Run ?mean in the console: a window on the right-hand side of the R Studio interface should open. The help file tells you what goes into a function and what comes out. For more complex functions it also tells you what all of the options (i.e., arguments) can do. Help files can be a bit intimidating to interpret at first, but they are all organized the same and once you learn their layout you will know where to go to find the information you’re looking for. 1.4.2 Vectors Vectors are one of the most common data structures. A vector is a set of numbers going in only one dimension. Each position in a vector is called an element, and the number of elements is called the length of the vector. Here are some ways to make some vectors with different elements, all of length five: # this is a comment. R will ignore all text on a line after a # # the ; means run everything after it on a new line # count up by 1 month = 2:6; month ## [1] 2 3 4 5 6 # count up by 2 day = seq(from = 1, to = 9, by = 2); day ## [1] 1 3 5 7 9 # repeat the same number (repeat 2018 5 times) year = rep(2018, 5); year ## [1] 2018 2018 2018 2018 2018 The [1] that shows up is an element position, more on this later (see Section 1.7). If you wish to know how many elements are in a vector, use length(): length(year) ## [1] 5 You can also create a vector “by-hand” using the c() function7: # a numeric vector number = c(4, 7, 8, 10, 15); number ## [1] 4 7 8 10 15 # a character vector pond = c(&quot;F11&quot;, &quot;S28&quot;, &quot;S30&quot;, &quot;S8&quot;, &#39;S11&#39;); pond ## [1] &quot;F11&quot; &quot;S28&quot; &quot;S30&quot; &quot;S8&quot; &quot;S11&quot; Note the difference between the numeric and character vectors. The terms “numeric” and “character”\" represent data classes, which specify the type of data the vector is holding: A numeric vector stores numbers. You can do math with numeric vectors A character vector stores what are essentially letters. You can’t do math with letters. A character vector is easy to spot because the elements will be wrapped with quotes8. A vector can only hold one data class at a time: v = c(1,2,3,&quot;a&quot;); v ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;a&quot; Notice how all the elements now have quotes around them. The numbers have been coerced to characters9. If you attempt to calculate the sum of your vector: sum(v) ## Error in sum(v): invalid &#39;type&#39; (character) of argument you would find that it is impossible in its current form. 1.4.3 Matrices Matrices act just like vectors, but they have two dimensions, i.e., they have both rows and columns. An easy way to make a matrix is by combining vectors you have already made: # combine vectors by column (each vector will become a column) m1 = cbind(month, day, year, number); m1 ## month day year number ## [1,] 2 1 2018 4 ## [2,] 3 3 2018 7 ## [3,] 4 5 2018 8 ## [4,] 5 7 2018 10 ## [5,] 6 9 2018 15 # combine vectors by row (each vector will become a row) m2 = rbind(month, day, year, number); m2 ## [,1] [,2] [,3] [,4] [,5] ## month 2 3 4 5 6 ## day 1 3 5 7 9 ## year 2018 2018 2018 2018 2018 ## number 4 7 8 10 15 Just like vectors, matrices can hold only one data class (note the coercion of numbers to characters): cbind(m1, pond) ## month day year number pond ## [1,] &quot;2&quot; &quot;1&quot; &quot;2018&quot; &quot;4&quot; &quot;F11&quot; ## [2,] &quot;3&quot; &quot;3&quot; &quot;2018&quot; &quot;7&quot; &quot;S28&quot; ## [3,] &quot;4&quot; &quot;5&quot; &quot;2018&quot; &quot;8&quot; &quot;S30&quot; ## [4,] &quot;5&quot; &quot;7&quot; &quot;2018&quot; &quot;10&quot; &quot;S8&quot; ## [5,] &quot;6&quot; &quot;9&quot; &quot;2018&quot; &quot;15&quot; &quot;S11&quot; Each vector should have the same length. 1.4.4 Data Frames Many data sets you will work with require storing different data classes in different columns, which would rule out the use of a matrix. This is where data frames come in: df1 = data.frame(month, day, year, number, pond); df1 ## month day year number pond ## 1 2 1 2018 4 F11 ## 2 3 3 2018 7 S28 ## 3 4 5 2018 8 S30 ## 4 5 7 2018 10 S8 ## 5 6 9 2018 15 S11 Notice the lack of quotation marks which indicates that all variables (i.e., columns) are stored as their original data class. It is important to know what kind of object type you are using, since R treats them differently. For example, some functions can only use a certain object type. The same holds true for data classes (numeric vs. character). You can quickly determine what kind of object you are dealing with by using the class() function: class(day); class(pond); class(m1); class(df1) ## [1] &quot;numeric&quot; ## [1] &quot;character&quot; ## [1] &quot;matrix&quot; ## [1] &quot;data.frame&quot; 1.5 Factors At this point, it is worthwhile to introduce an additional data class: factors. Notice the data class of the pond variable in df1: class(df1$pond) ## [1] &quot;factor&quot; The character vector pond was coerced to a factor when you placed it in the data frame. A vector with a factor class is like a character vector in that you see letters and that you can’t do math on it. However, a factor has additional properties: in particular, it is a grouping variable. See what happens when you print the pond variable: df1$pond ## [1] F11 S28 S30 S8 S11 ## Levels: F11 S11 S28 S30 S8 A factor has levels, with each level being a subcategory of the factor. You can see the unique levels of your factor by running: levels(df1$pond) ## [1] &quot;F11&quot; &quot;S11&quot; &quot;S28&quot; &quot;S30&quot; &quot;S8&quot; Additionally, factor levels have an assigned order (even if the levels are totally nominal), which will become important in Chapter 3 when you learn how to fit linear models to groups of data, in which one level is the “reference” group that all other groups are compared to (see Section 3.1.2 for more details). If you run into errors about R expecting character vectors, it may be because they are actually stored as factors. When you make a data frame, you’ll often have the option to turn off the automatic factor coercion. For example: data.frame(month, day, year, number, pond, stringsAsFactors = F) read.csv(&quot;streams.csv&quot;, stringsAsFactors = F) # see below for details on read.csv will result in character vectors remaining that way as opposed to being coerced to factors. This can be preferable if you are doing many string manipulations, as character vectors are often easier to work with than factors. 1.6 Vector Math R does vectorized calculations. This means that if supplied with two numeric vectors of equal length and a mathematical operator, R will perform the calculation on each pair of elements. For example, if you wanted to add the two vectors day and month, then you would just run: dm = day + month; dm ## [1] 3 6 9 12 15 Look at the contents of both day and month again to make sure you see what R did. You typically should ensure that the vectors you are doing math with are of equal lengths. You could do the same calculation to each element of a vector (e.g., divide each element by 2) with: dm/2 ## [1] 1.5 3.0 4.5 6.0 7.5 1.7 Data Subsets/Queries This is perhaps the most important and versatile skill to know in R. Say you have an object with data in it and you want to use it for analysis, but you don’t want the whole data set: just a few rows or just a few columns, or perhaps you need just a single element from a vector. This section is devoted to ways you can extract certain parts of data objects (the terms query and subset are often used interchangeably to describe this task). There are three main methods: By Index – This method allows you to pull out specific rows/columns by their location in an object. However, you must know exactly where in the object the desired data are. An index is a location of an element in a data object, like the element position or the position of a specific row or column. The syntax for subsetting a vector by index is vector[element] and for a matrix it is matrix[row,column]. Here are some examples: # show all of day, then subset the third element day; day[3] ## [1] 1 3 5 7 9 ## [1] 5 # show all of m1, then subset the cell in row 1 col 4 m1; m1[1,4] ## month day year number ## [1,] 2 1 2018 4 ## [2,] 3 3 2018 7 ## [3,] 4 5 2018 8 ## [4,] 5 7 2018 10 ## [5,] 6 9 2018 15 ## number ## 4 # show all of df1, then subset the entire first column df1; df1[,1] ## month day year number pond ## 1 2 1 2018 4 F11 ## 2 3 3 2018 7 S28 ## 3 4 5 2018 8 S30 ## 4 5 7 2018 10 S8 ## 5 6 9 2018 15 S11 ## [1] 2 3 4 5 6 Note this last line: the [,1] says “keep all the rows, but take only the first column”. Here is another example: # show m1, then subset the 1st, 2nd, and 4th rows and every column m1; m1[c(1,2,4),] ## month day year number ## [1,] 2 1 2018 4 ## [2,] 3 3 2018 7 ## [3,] 4 5 2018 8 ## [4,] 5 7 2018 10 ## [5,] 6 9 2018 15 ## month day year number ## [1,] 2 1 2018 4 ## [2,] 3 3 2018 7 ## [3,] 5 7 2018 10 Notice how you can pass a vector of row indices here to exclude the 3rd and 5th rows. 2. By name – This method allows you to pull out a specific column of data based on what the column name is. Of course, the column must have a name first. The name method uses the $ operator: df1$month ## [1] 2 3 4 5 6 You can combine these two methods: df1$month[3] ## [1] 4 # or df1[,c(&quot;year&quot;, &quot;month&quot;)] ## year month ## 1 2018 2 ## 2 2018 3 ## 3 2018 4 ## 4 2018 5 ## 5 2018 6 The $ method is useful because it can be used to add columns to a data frame: df1$dm = df1$day + df1$month; df1 ## month day year number pond dm ## 1 2 1 2018 4 F11 3 ## 2 3 3 2018 7 S28 6 ## 3 4 5 2018 8 S30 9 ## 4 5 7 2018 10 S8 12 ## 5 6 9 2018 15 S11 15 Logical Subsetting – This is perhaps the most flexible method, but requires more explaining. It is described in Section 1.11. Exercise 1A Take a break to apply what you’ve learned so far to enter the data found in Table 1.1 into R by hand and do some basic data subsets. The solutions to this exercise are found at the end of this book (here). You are strongly recommended to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped. Table 1.1: The table you should enter in to R by hand for Exercise 1A Lake Area Time Fish Big 100 1000 643 Small 25 1200 203 Square 45 1400 109 Circle 30 1600 15 Create a new file in your working directory called Ex_1A.R. Enter these data into vectors. Call the vectors whatever you would like. Should you enter the data as vectors by rows, or by columns? (Hint: remember the properties of vectors). Combine your vectors into a data frame. Why should you use a data frame instead of a matrix? Subset all of the data from Small Lake. Subset the area for all of the lakes. Subset the number of fish for Big and Square lakes. You realize that you sampled 209 fish at Square Lake, not 109. Fix the mistake. There are two ways to do this, can you think of them both? Which do you think is better? Save your script. Close R and re-open your script to see that it was saved. 1.8 Read External Data Files It is rare that you will enter data by hand as you did in Exercise 1A. Often, you have a data set that you wish to analyze or manipulate that is stored in a spreadsheet. R has several ways to read information from data files and in this book, you will be using a common and simple method: reading in .csv files. .csv files are data files that separate columns with commas10. If your data are in a Microsoft Excel spreadsheet, you can save your spreadsheet file as a .csv file (File &gt; Save As &gt; Save as Type&gt; CSV (Comma Delimited)). Several dialog boxes will open asking if you are sure you want to save it as a .csv file. The syntax for reading in a .csv file is: dat = read.csv(&quot;Path/To/FileName.csv&quot;) Make sure your working directory is set to the location of your current file, which should be in the location C:/Users/YOU/Documents/R-Book/Chapter 1. According to the instructions on acquiring the data, you should have placed all of the data files for this book (downloaded from the GitHub repository) in the location C:/Users/YOU/Documents/R-Book/Data. If you have done this already and your working directory is set to the location of the Ch1.R script, you can simply run: dat = read.csv(&quot;../Data/streams.csv&quot;) The ../ tells R to look up one directory from the working directory for a folder called Data, then within that, look for a file called streams.csv. If you do not get an error, congratulations! However, if you get an error that looks like this: ## Warning in file(file, &quot;rt&quot;): cannot open file &#39;streams.csv&#39;: No such file ## or directory ## Error in file(file, &quot;rt&quot;): cannot open the connection then fear not. This must be among the most common errors encountered by R users world-wide. It simply means the file you told R to look for doesn’t exist where you told R to find it. Here is a trouble-shooting guide to this error: The exact case and spelling matters, as do the quotes and .csv at the end. Ensure the file name is typed correctly. Check what files are in the path you are specifying: run dir(\"../Data\"). This will return a vector with the names of the files located in the Data directory (which is one folder up from your working directory). Is the file you told R was there truly in there? Is your working directory set to where you thought it was? A simpler method is to put the data file in your working directory, in which case it can be read into R using dat = read.csv(\"streams.csv\"). This method is not recommended for this book, because you will be using the same data files in multiple chapters, and it will help if they are all located in the same location. If you did not get any errors, then the data are in the object you named (dat) and that object is a data frame. Do not proceed until you are able to get read.csv to run successfully. A few things to note about reading in .csv files: R will assume the first row are column headers by default. If there is a space in one of the header cells, a \".\" will be inserted. For example, the column header Total Length would become Total.Length. R brings in .csv files in as data frames by default. If a record (i.e., cell) is truly missing and you want R to treat it that way (i.e., as an NA), you have three options: Hard code an NA into that cell in your .csv file Leave that cell completely empty in your .csv file Enter in some other character (e.g., \".\") alone in all cells that are meant to be coded as NA in R and use the na.strings = \".\" argument of read.csv(). If at some point you did “Clear Contents” in Microsoft Excel to delete rows or columns from your .csv file, these “deleted” rows/columns will be read in as all NAs, which can be annoying. To remove this problem, open the .csv file in Excel, then highlight and delete the rows/columns in question and save the file. Read it back into R again using read.csv(). If even a single character is found in a numeric column in FileName.csv, the entire column will be coerced to a character/factor data class after it is read in (i.e., no more math with data on that column until you remove the character). A common error is to have a #VALUE! record left over from an invalid Excel function result. You must remove all of these occurrences in order to use that column as numeric. Characters include anything other than a number ([0-9]) and a period when used as a decimal. None of these characters: !?[]\\/@#$%^&amp;*()&lt;&gt;_-+=[a-z];[A-Z] should never be found in a column you wish to do math with (e.g., take the mean of that column). This is an incredibly common problem! 1.9 Explore the Data Set Have a look at the data. You could just run dat to view the contents of the object, but it will show the whole thing, which may be undesirable if the data set is large. To view the first handful of rows, run head(dat) or the last handful of rows with tail(dat). You will now use some basic functions to explore the simulated streams data before any analysis. The summary() function is very useful for getting a coarse look at how R has interpreted the data frame: summary(dat) ## state stream_width flow ## Alabama :5 Min. :17.65 Min. : 28.75 ## Florida :5 1st Qu.:46.09 1st Qu.: 65.50 ## Georgia :5 Median :61.80 Median : 95.64 ## Tennessee:5 Mean :60.88 Mean : 91.49 ## 3rd Qu.:79.34 3rd Qu.:120.55 ## Max. :94.65 Max. :149.54 ## NA&#39;s :1 You can see the spread of the numeric data and see the different levels of the factor (state) as well as how many records belong to each level. Note that there is one NA in the variable called flow. To count the number of elements in a variable (or any vector), remember the length() function: length(dat$stream_width) ## [1] 20 Note that R counts missing values as elements as well: length(dat$flow) ## [1] 20 To get the dimensions of an object with more than one dimension (i.e., a data frame or matrix) you can use the dim() function. This returns a vector with two elements: the first number is the number of rows and the second is the number of columns. If you only want one of these, use the nrow() or ncol() functions (but remember, only for objects with more than one dimension; vectors don’t have rows or columns!). dim(dat); nrow(dat); ncol(dat) ## [1] 20 3 ## [1] 20 ## [1] 3 You can extract the names of the variables (i.e., columns) in the data frame using colnames(): colnames(dat) ## [1] &quot;state&quot; &quot;stream_width&quot; &quot;flow&quot; Calculate the mean of all the stream_width records: mean(dat$stream_width) ## [1] 60.8845 Calculate the mean of all of the flow records: mean(dat$flow) ## [1] NA mean() returned an NA because there is an NA in the data for this variable. The way to tell R to ignore this NA is by including the argument na.rm = TRUE in the mean() function (separate arguments are always separated by commas). This is a logical argument, meaning that it asks a question. It says “do you want to remove NAs before calculating the mean?” TRUE means “yes” and FALSE means “no.” TRUE and FALSE can be abbreviated as T and F, respectively. Many of R’s functions have the na.rm argument (e.g. mean(), sd(), var(), min(), max(), sum(), etc. - most anything that collapses a vector into one number). mean(dat$flow, na.rm = T) ## [1] 91.49053 which is the same as (i.e., the definition of the mean with the NA removed): sum(dat$flow, na.rm = T)/(nrow(dat) - 1) ## [1] 91.49053 What if you need to apply a function to more than one variable at a time? One of the easiest ways to do this (though as with most things in R, there are many) is by using the apply() function. This function applies the same summary function to individual subsets of a data object at a time then returns the individual summaries all at once: apply(dat[,c(&quot;stream_width&quot;, &quot;flow&quot;)], 2, FUN = var, na.rm = T) ## stream_width flow ## 581.1693 1337.3853 The first argument is the data object to which you want to apply the function. The second argument (the number 2) specifies that you want to apply the function to columns, 1 would tell R to apply it to rows. The FUN argument specifies what function you wish to apply to each of the columns; here you are calculating the variance which takes the na.rm = T argument. This use of apply() alone is very powerful and can help you get around having to write the dreaded for() loop (introduced in Chapter 4). There is a whole family of -apply() functions, the base apply() is the most basic but a more sophisticated one is tapply(), which applies a function based on some grouping variable (a factor). Calculate the mean stream width separated by state: tapply(dat$stream_width, dat$state, mean) ## Alabama Florida Georgia Tennessee ## 53.664 63.588 54.996 71.290 The first argument is the variable to which you want to apply the mean function, the second is the grouping variable, and the third is what function you wish to apply. Try to commit this command to memory given this is a pretty common task. If you want a data frame as output, you can use the aggregate function to do the same thing: aggregate(dat$stream_width, by = list(state = dat$state), mean) ## state x ## 1 Alabama 53.664 ## 2 Florida 63.588 ## 3 Georgia 54.996 ## 4 Tennessee 71.290 1.10 Logical/Boolean Operators To be an efficient and capable programmer in any language, you will need to become familiar with how to implement numerical logic, i.e., the Boolean operators. These are very useful because they always return a TRUE or a FALSE, off of which program-based decisions can be made (e.g., whether to operate a given subroutine, whether to keep certain rows, whether to print the output, etc.). Define a simple object: x = 5 . Note that this will write over what was previously stored in the object x. Suppose you want to ask some questions of the new object x and have the answer printed to the console as a TRUE for “yes” and a FALSE for “no”. Below are the common Boolean operators and their usage in R. Equality To ask if x is exactly equal to 5, you run: x == 5 ## [1] TRUE Note the use of the double == to denote equality as opposed to the single = as used in assignment (e.g., x = 5) or in argument specification (e.g., mean(dat$flow, na.rm = T)). Inequalities To ask if x is not equal to 5, you run: x != 5 ## [1] FALSE The ! is the logical not in R, and can be used to flip the direction of any T to a F or vice versa. To ask if x is less than 5, you run: x &lt; 5 ## [1] FALSE To ask if x is less than or equal to 5, you run: x &lt;= 5 ## [1] TRUE Greater than works the same way, though with the &gt; symbol replaced. In Suppose you want to ask which elements of one vector are also found within another vector: y = c(1,2,3) x %in% y ## [1] FALSE # or y = c(4,5,6) x %in% y ## [1] TRUE # or y %in% x ## [1] FALSE TRUE FALSE %in% is a very useful operator in R that is not one of the traditional Boolean operators. And Suppose you have two conditions, and you want to know if both are met. For this you would use and by running: x &gt; 4 &amp; x &lt; 6 ## [1] TRUE which asks if x is between 4 and 6. Or Suppose you have two conditions, and you want to know if either are met. For this you would use or by running: x &lt;= 5 | x &gt; 5 ## [1] TRUE which asks if x is less than or equal to 5 or greater than 5 - you would be hard-pressed to find a real number that did not meet these conditions! 1.11 Logical Subsetting A critical use of logical/Boolean operators is in the subsetting of data objects. You can use a logical vector (i.e., one made of only TRUE and FALSE elements) to tell R to extract only those elements corresponding to the TRUE records. For example: # here&#39;s logical vector: TRUE everywhere condition met dat$stream_width &gt; 60 ## [1] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE ## [12] FALSE TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE # insert it to see only the flows for the TRUE elmements dat$flow[dat$stream_width &gt; 60] ## [1] 120.48 123.78 95.64 95.82 120.06 135.63 120.61 111.34 149.54 131.22 gives all of the flow values for which stream_width is greater than 60. To extract all of the data from Alabama, you would run: dat[dat$state == &quot;Alabama&quot;,] ## state stream_width flow ## 1 Alabama 81.68 120.48 ## 2 Alabama 57.76 85.90 ## 3 Alabama 48.32 73.38 ## 4 Alabama 31.63 46.55 ## 5 Alabama 48.93 80.91 To exclude all of the data from Alabama, you would run: dat[dat$state != &quot;Alabama&quot;,] You will be frequently revisiting this skill throughout the workshop. 1.12 if(), else, and ifelse() You can tell R to do something if the result of a question is TRUE. This is a typical if-then statement: if (x == 5) print(&quot;x is equal to 5&quot;) ## [1] &quot;x is equal to 5&quot; This says “if x equals 5, then print the phrase ‘x is equal to 5’ to the console”. If the logical returns a FALSE, then this command does nothing. To see this, change the == to a != and re-run: if (x != 5) print(&quot;x is equal to 5&quot;) Notice that because x does equal 5, running this line has no effect. You can tell R to do multiple things if the logical is TRUE by using curly braces: if (x == 5) { print(&quot;x is equal to 5&quot;) print(&quot;you dummy, x is supposed to be 6&quot;) } ## [1] &quot;x is equal to 5&quot; ## [1] &quot;you dummy, x is supposed to be 6&quot; You can always use curly braces to extend code across multiple lines whereas it may have been intended to go on one line. If you want R to do something if the logical is FALSE, you would use the else command: if (x &gt; 5) print(&quot;x is greater than 5&quot;) else print(&quot;x is not greater than 5&quot;) ## [1] &quot;x is not greater than 5&quot; Or extend this same thing to multiple lines: if (x &gt; 5) { print(&quot;x is greater than 5&quot;) } else { print(&quot;x is not greater than 5&quot;) } The if() function is useful, but it can only respond to one question at a time. If you supply it with a vector of length greater than 1, it will give a warning: # vector from -5 to 5, excluding zero xs = c(-5:-1, 1:5) # attempt a logical decision if (xs &lt; 0) print(&quot;negative&quot;) else print(&quot;positive&quot;) ## Warning in if (xs &lt; 0) print(&quot;negative&quot;) else print(&quot;positive&quot;): the ## condition has length &gt; 1 and only the first element will be used ## [1] &quot;negative&quot; Warnings are different than errors in that something still happens, but it tells you that it might not be what you wanted, whereas an error stops R altogether. In short, this warning is telling you that you passed if() a logical vector with more than 1 element, and that it can only use one element so it’s picking the first one. Because the first element of xs is -5, xs &lt; 0 evaluated to TRUE, and you got a \"negative\" printed along with the warning. To respond to multiple questions at once, you must use ifelse(). This function is similar, but it combines the if() and else syntax into one useful function function: ifelse(xs &gt; 0, &quot;positive&quot;, &quot;negative&quot;) ## [1] &quot;negative&quot; &quot;negative&quot; &quot;negative&quot; &quot;negative&quot; &quot;negative&quot; &quot;positive&quot; ## [7] &quot;positive&quot; &quot;positive&quot; &quot;positive&quot; &quot;positive&quot; The syntax is ifelse(condition, do_if_TRUE, do_if_FALSE). You can cbind() the output with xs to verify it worked: cbind( xs, ifelse(xs &gt; 0, &quot;positive&quot;, &quot;negative&quot;) ) ## xs ## [1,] &quot;-5&quot; &quot;negative&quot; ## [2,] &quot;-4&quot; &quot;negative&quot; ## [3,] &quot;-3&quot; &quot;negative&quot; ## [4,] &quot;-2&quot; &quot;negative&quot; ## [5,] &quot;-1&quot; &quot;negative&quot; ## [6,] &quot;1&quot; &quot;positive&quot; ## [7,] &quot;2&quot; &quot;positive&quot; ## [8,] &quot;3&quot; &quot;positive&quot; ## [9,] &quot;4&quot; &quot;positive&quot; ## [10,] &quot;5&quot; &quot;positive&quot; Use ifelse() to create a new variable in dat that indicates whether a stream is big or small depending on whether stream_width is greater or less than 50: dat$size_cat = ifelse(dat$stream_width &gt; 50, &quot;big&quot;, &quot;small&quot;); head(dat) ## state stream_width flow size_cat ## 1 Alabama 81.68 120.48 big ## 2 Alabama 57.76 85.90 big ## 3 Alabama 48.32 73.38 small ## 4 Alabama 31.63 46.55 small ## 5 Alabama 48.93 80.91 small ## 6 Georgia 39.42 57.63 small This says “make a new variable in the data frame dat called size_cat and assign each row a ‘big’ if stream_width is greater than 50 and a ‘small’ if less than 50”. One neat thing about ifelse() is that you can nest multiple statements inside another11. What if you wanted three categories: ‘small’, ‘medium’, and ‘large’? dat$size_cat_fine = ifelse(dat$stream_width &lt;= 40, &quot;small&quot;, ifelse(dat$stream_width &gt; 40 &amp; dat$stream_width &lt;= 70, &quot;medium&quot;, &quot;big&quot;)); head(dat) ## state stream_width flow size_cat size_cat_fine ## 1 Alabama 81.68 120.48 big big ## 2 Alabama 57.76 85.90 big medium ## 3 Alabama 48.32 73.38 small medium ## 4 Alabama 31.63 46.55 small small ## 5 Alabama 48.93 80.91 small medium ## 6 Georgia 39.42 57.63 small small If the first condition is TRUE, then it will give that row a “small”. If not, it will start another ifelse() to ask if the stream_width is greater than 40 and less than or equal to 70. If so, it will give it a “medium”, if not it will get a “big”. Not all function nesting examples are this complex, but this is a neat example. Without ifelse(), you would have to use as many if() statements as there are elements in dat$stream_width. 1.13 Writing Output Files 1.13.1 .csv Files Now that you have made some new variables in your data frame, you may want to save this work in the form of a new .csv file. To do this, you can use the write.csv function: write.csv(dat, &quot;updated_streams.csv&quot;, row.names = F) The first argument is the data frame (or matrix) to write, the second is what you want to call it (don’t forget the .csv!), and row.names = F tells R to not include the row names (because they are just numbers in this case). R puts the file in your working directory unless you tell it otherwise. To put it somewhere else, type in the path with the new file name at the end (e.g., C:/Users/YOU/Documents/R-Book/Data/updated_streams.csv. 1.13.2 Saving R Objects If all you care about is the data frame dat as interpreted by R (not a share-able file like the .csv method), then you can save the object dat (in its current state) then load it in to a future R session. You can save the new data frame using: save(dat, file = &quot;updated_streams&quot;) Then try removing the dat object from your current session (rm(dat)) and loading it back in using: rm(dat); head(dat) # should give error load(file = &quot;updated_streams&quot;) head(dat) # should show first 6 rows 1.14 User-Defined Functions Sometimes you may want R to carry out a specific task, but there is no built-in function to do it. In these cases, you can write your own functions. Function writing makes R incredibly flexible, though you will only get a small taste of this topic here. You will see more examples in later Chapters, particularly in Chapter 4. First, you must think of a name for your function (e.g., myfun). Then, you specify that you want the object myfun() to be a function by using using the function() function. Then, in parentheses, you specify any arguments that you want to use within the function to carry out the specific task. Open and closed curly braces specify the start and end of your function body, i.e., the code that specifies how it uses the arguments to do its job. Here’s the general syntax for specifying your own function: myfun = function(arg1) { # function body goes here # use arg1 to do something # return something as last step } As an example, write a general function to take any number x to any power y: power = function(x, y){ x^y } After typing and running the function code (power() is an object that must be assigned), try using it: power(x = 5, y = 3) ## [1] 125 Remember, you can nest or embed functions: power(power(5,2),2) ## [1] 625 This is the equivalent of \\((5^2)^2\\). Exercise 1B In this exercise, you will be using what you learned in Chapter 1 to summarize data from a hypothetical pond experiment. Pretend that you added nutrients to mesocosoms and counted the densities of four different zooplankton taxa. In this experiment, there were two ponds, two treatments per pond, and five replicates of each treatment. The data are located in the data file ponds.csv (see the instructions on acquiring and organizing the data files for this book. There is one error in the data set. After you download the data and place it in the appropriate directory, make sure you open this file and fix it before you bring it into R. Refer back to the information about reading in data (Section 1.8) to make sure you find the error. Create a new R script in your working directory for this chapter called Ex1B.R and use that script to complete this exercise. The solutions to this exercise are found at the end of this book (here). You are strongly recommended to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped. Read in the data to R and assign it to an object. Calculate some basic summary statistics of your data using the summary() function. Calculate the mean chlorophyll a for each pond (Hint: pond is a grouping variable) Calculate the mean number of Chaoborus for each treatment in each pond using tapply(). (Hint: You can group by two variables with: tapply(dat$var, list(dat$grp1, dat$grp2), fun). Use the more general apply() function to calculate the variance for each zooplankton taxa found only in pond S-28. Create a new variable called prod in the data frame that represents the quantity of chlorophyll a in each replicate. If the chlorophyll a in the replicate is greater than 30 give it a “high”, otherwise give it a “low”. (Hint: are you asking R to respond to one question or multiple questions? How should this change the strategy you use?) Exercise 1B Bonus Use ?table to figure out how you can use table() to count how many observations of high and low there were in each treatment (Hint: table will have only two arguments.). Create a new function called product() that multiplies any two numbers you specify. Modify your function to print a message to the console and return the value if() it meets a condition and to print another message and not return the value if it doesn’t. References "],
["ch2.html", "Chapter 2 Base R Plotting Basics Chapter Overview Before You Begin 2.1 R Plotting Lingo 2.2 Lower Level Plotting Functions 2.3 Other High-Level Plotting Functions 2.4 Box-and-Whisker Plots 2.5 Histograms 2.6 The par() Function 2.7 New Temporary Devices 2.8 Multi-panel Plots 2.9 Legends 2.10 Exporting Plots 2.11 Bonus Topic: Error Bars Exercise 2", " Chapter 2 Base R Plotting Basics Chapter Overview In this chapter, you will get familiar with the basics of using R for making plots and figures. You will learn: how to make various plots including: scatterplots line plots bar plots box-and-whisker plots histograms the basics of how to change plot features like: the text displayed on axes labels the size and type of point symbols the color of features the basics of multi-panel plotting the basics of the par() function how to save your plot to an external file R’s base {graphics} plotting package is incredibly versatile, and as you will see, it doesn’t take much to get started making professional-looking graphs. It is worth mentioning that there are other R packages12 for plotting that have nice features. The R packages {ggplot2} (Wickham and Chang 2016) and {lattice} (Sarkar 2017) are good examples. However, they can be more complex to learn at first than the base R plotting capabilities and look a bit different. IMPORTANT NOTE: If you did not attend the sessions corresponding to Chapter 1, you are recommended to walk through the material found in that chapter before proceeding to this material. Also note that if you are confused about a topic, you can use CTRL + F to find previous cases where that topic has been discussed in this book. Before You Begin You should create a new directory and R script for your work in this Chapter. Create a new R script called Ch2.R and save it in the directory C:/Users/YOU/Documents/R-Book/Chapter2. Set your working directory to that location. Revisit Sections 1.2 and 1.3 for more details on these steps. 2.1 R Plotting Lingo Learning some terminology will help you get used to the base R graphics system: A high-level plotting function is one that is used to make a new graph. Examples of higher level plotting functions are the general plot() and the barplot() functions. When-a high level plotting function is executed, the currently-displayed plot (if any) is written over. A low-level plotting function is one that is used to modify a plot that has already been created. You must already have made a plot using a high-level plotting function before you use low-level plotting functions. Examples include text(), points(), and lines(). When a low-level plotting function is executed, the output is simply added to an existing plot. The graphics device is the area in which a plot is displayed. RStudio has a built in graphics device in its interface (lower right by default in the “Plots” tab), or you can create a new device. R will plot on the active device, which is the most recently created device. There can only be one active device at a time. Here is an example of a basic R plot: There are a few components: The plotting region: all data information is displayed here. The margin: where axis labels, tick marks, and main plot titles are located The outer margin: by default, there is no outer margin. You can add one if you want to add text here or make more room around the edges. You can change just about everything there is about this plot to suit your tastes. Duplicate this plot, but make the x-axis, y-axis, and main titles something other than the placeholders shown here: # plot dummy data x = 0:10 y = x^2 plot(x = x, y = y, xlab = &quot;x-axis label goes here&quot;, ylab = &quot;y-axis label goes here&quot;, main = &quot;Main Plot Title Goes Here&quot;) Note that the first two arguments, x and y, specify the coordinates of the points (i.e., the first point is placed at coordinates x[1],y[1]). plot() has tons of arguments (or graphical parameters as the help file found using ?plot or ?par calls them) that change how the plot looks. Note that when you want something displayed verbatim on the plotting device, you must wrap that code in \" \", i.e., the arguments xlab, ylab, and main all receive a character vector of length 1 as input. Table 2.1 shows information on just a handful of the plotting settings to get you started. Table 2.1: Several of the key arguments to high- and low-level plotting functions Arg. Usage Description xlab xlab = 'X-AXIS' changes the x-axis label text ylab ylab = 'Y-AXIS' changes the y-axis label text main main = 'TITLE' changes the main title text cex cex = 1.5 changes the size of symbols in the plotting region13 pch pch = 17 changes the symbol type14 xlim xlim = range(x) changes the endpoints (limits) of the x-axis15 ylim ylim = c(0,1) same as xlim, but for the y-axis type type = 'l' changes the way points are connected by lines16 lty lty = 2 changes the line type17 lwd lwd = 2 changes the line width18 col col = 'blue' changes the color of plotted objects19 You are advised to try at least some of the arguments in Table 2.1 out for yourself with your plot(x, y) code from above - notice how every time you run plot(), a whole new plot is created, not just the thing you changed. There are definitely other options: check out ?plot or ?par for more details. 2.2 Lower Level Plotting Functions Now that you have a base plot designed to your liking, you might want to add some additional “layers” to it to represent more data or other kind of information like an additional label or text. Add some more points to your plot by putting this line right beneath your plot(x,y) code and run just the points() line (make sure your device is showing a plot first): # rev() reverses a vector: so the old x[1] is x[11] now points(x = rev(x), y = y, col = &quot;blue&quot;, pch = 16) Here, points() acted like a low-level plotting function because it added points to a plot you already made. Many of the arguments shown in Table 2.1 can be used in both high-level and low-level plotting functions (notice how col and pch were used in points()). Just like points(), there is also lines(): lines(x = x, y = x, lty = 2, col = &quot;red&quot;) You can add text to the plotting region: text(x = 5, y = 80, &quot;This is Text&quot;, cex = 1.5, col = &quot;grey&quot;, font = 4) The text is centered on the coordinates you provide. You can also provide vectors of coordinates and text to write different things at once. The easiest way to add a straight line to a plot is with abline(). By default it takes two arguments: a and b which are the y-intercept and slope, respectively, e.g., abline(0,1) will draw a 1:1 line, as will abline(c(0,1)). You can also do abline(h = 5) to draw a horizontal line at 5 or abline(v = 5) to draw a vertical line at 5. You can see that the text is centered on the coordinates x = 5 and y = 80 using abline(): abline(h = 80, col = &quot;grey&quot;) abline(v = 5, col = &quot;grey&quot;) If you accidentally add a plot element that you don’t want using a low-level plotting function, the only way to remove it is by re-running the high-level plotting function to start a new plot and adding only the objects you want. Try removing the “This is Text” text and the straight lines you drew with abline() from the plot displayed in your device. 2.3 Other High-Level Plotting Functions You have just seen the basics of making two-dimensional scatter plots and line plots. You will now explore other types of graphs you can make. 2.3.1 The Bar Graph Another very common graph is a bar graph. R has a barplot() function, and again, it has lots of arguments. Here you will just make two common variations: single bars per group and multiple bars per group. Create a vector and plot it: x1 = c(2,4,6) barplot(x1) Notice that there are no group names on the bars (if x1 had names, there would be). You can add names by using the argument names.arg: barplot(x1, names.arg = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) Add some more information by including two bars per group. Create another vector and combine it with the old data: x2 = c(3,5,7) x3 = rbind(x1, x2) barplot(x3, names.arg = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), beside = T) To add multiple bars per group, R needs a matrix like you just made. The columns store the heights of the bars that will be placed together in a group. Including the beside = T argument tells R to plot all groups as different bars as opposed to using a stacked bar graph. Oftentimes, you will want to add error bars to a bar graph like this. To avoid digressing too much here, creating error bars is covered as a bonus topic (Section 2.11). 2.4 Box-and-Whisker Plots Box-and-whisker plots are a great way to visualize the spread of your data. All you need to make a box-and-whisker plot is a grouping variable (a factor, revisit Section 1.5 if you don’t remember what these are) and some continuous (i.e., numeric) data for each level of the factor. You will be using the creel.csv data set (see the instructions on acquiring and placing the data files in the appropriate location). Read the data in and print a summary: dat = read.csv(&quot;../Data/creel.csv&quot;) summary(dat) ## fishery hours ## Harvest :100 Min. :-1.150 ## Non.Tournament:100 1st Qu.: 9.936 ## Tournament :100 Median :20.758 ## Mean :26.050 ## 3rd Qu.:43.896 ## Max. :62.649 This data set contains some simulated (i.e., fake) continuous and categorical data that represent 300 anglers who were creel surveyed20. In the data set, there are three categories (levels to the factor fishery) and the continuous variable is how many hours each angler fished this year. If you supply the generic plot() function with a continuous response (y) variable and a categorical predictor (x) variable, it will automatically assume you want to make a box-and-whisker plot: plot(x = dat$fishery, y = dat$hours) In the box-and-whisker plot above, the heavy line is the median, the ends of the boxes are the 25th and 75th percentiles and the “whiskers” are the 2.5th and 97.5th percentiles. Any points that are outliers (i.e., fall outside of the whiskers) will be shown as points21. It is worth introducing a shorthand syntax of typing the same command: plot(hours ~ fishery, data = dat) Instead of saying plot(x = x.var, y = y.var), this expression says plot(y.var ~ x.var). The ~ reads “as a function of”. By specifying the data argument, you no longer need to indicate where the variables hours and fishery are found. Many R functions have a data argument that works this same way. It is sometimes preferable to plot variables with this syntax because it is often less code and is also the format of R’s statistical equations22. 2.5 Histograms Another way to show the distribution of a variable is with histograms. These figures show the relative frequencies of observations in different discrete bins. Make a histogram for the hours the surveyed tournament anglers fished this year: hist(dat$hours[dat$fishery == &quot;Tournament&quot;]) Notice the subset that extracts hours fished for tournament anglers only before plotting. hist() automatically selects the number of bins based on the range and resolution of the data. You can specify how many evenly-sized bins you want to plot: # extract the hours for tournament anglers t_hrs = dat$hours[dat$fishery == &quot;Tournament&quot;] # create the bin endpoints nbins = 20 breaks = seq(from = min(t_hrs), to = max(t_hrs), length = nbins + 1) hist(t_hrs, breaks = breaks, main = &quot;Tournament&quot;, col = &quot;grey&quot;) 2.6 The par() Function If it bothers you that the axes are “floating”, you can fix this using this command: par(xaxs = &quot;i&quot;, yaxs = &quot;i&quot;, mar = c(4,4,2,1)) hist(t_hrs, breaks = breaks, main = &quot;Tournament&quot;, col = &quot;grey&quot;) Here, you changed the graphical parameters of the graphics device by using the par() function. Once you change the settings in par(), they will remain that way until you start a new device. The par() function is central to fine-tuning your graphics. Here, the xaxs = \"i\" and yaxs = \"i\" arguments essentially removed the buffer between the data and the axes. par() has options to change the size of the margins, add outer margins, change colors, etc. Some of the graphical parameters that can be passed to high- and low-level plotting functions (like those in Table 2.1) can also be passed par(). Check out the help file (?par) to see everything it can do. If you want to start over with fresh par() settings, start a new device. The mar argument controls how many lines of margin text are on each side of the plotting region: mar[1] is the bottom margin, mar[2] is left, mar[3] is top, and mar[4] is right. There is also par(oma = c()) for outer margins. You can specify these in inch units instead of margin lines using the mai or omi arguments. 2.7 New Temporary Devices If you are using RStudio, then likely all of the plots you have made thus far have shown up in the lower right-hand corner or your RStudio window. You have been using RStudio’s built-in plotting device. If you want to open a new plotting device (maybe to put it on a separate monitor), you can use the following commands, depending on your operating system: Windows Users – just run windows() to open up a new plotting device. It will become the active device. Mac Users – similarly, you can run quartz() to open a new device. Linux Users – similarly, just run x11(). 2.8 Multi-panel Plots Sometimes you want to display more than one plot at a time. You can make a multi-panel plot which allows for multiple plotting regions to show up simultaneously within the same plotting device. First, you need to change the layout of the plotting region. The easiest way to set up the device for multi-panel plotting is by using the mfrow argument in the par() function. Below, the code says “set up the graphical parameters so that there is 1 row and 3 columns of plotting regions within the device”. Every time you make a new plot, it will go in the next available plotting region (regions fill from left to right, and from top to bottom; use mfcol if you wish to fill columns before rows). Make 3 histograms, each that represents a different sector of the fishery: par(mfrow = c(1,3), mar = c(4,1,2,1), oma = c(0,3,0,0)) sapply(levels(dat$fishery), function(f) { hist(dat$hours[dat$fishery == f], main = f, xlab = &quot;&quot;) }) mtext(side = 2, outer = T, line = 1.5, &quot;Frequency&quot;, cex = 0.8) Here, sapply() applied a user-defined function that plots a histogram for a given fishery type to each of the fishery types separately23, which allowed you to only need to type the hist() code once. Notice the par(oma) setting to add outer margins on the left and the mtext() function to add a common y-axis label to all the figures. There are other ways to make multi-panel plots, however, they are beyond the scope of this introductory material. See ?layout for details. With this function you can change the size of certain plots and make them have different shapes (i.e., some squares, some rectangles, etc.), but it takes some pretty involved (though not impossible, by any means) specification of how you want the device to be split up into regions. 2.9 Legends Oftentimes you will want to add a legend to a plot to help people interpret what it is showing. You can add legends to R plots using the low-level plotting function legend(). Add a legend to the bar plot you made earlier with two groups. First, re-make the plot by running the high-level barplot() function, but change the colors of the bars to be shades of blue and red. Once you have the plot made, add the legend: par(mar = c(4,4,2,1)) barplot(x3, beside = T, names.arg = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), col = c(&quot;skyblue2&quot;, &quot;tomato2&quot;)) legend(&quot;topleft&quot;, legend = c(&quot;Group 1&quot;, &quot;Group 2&quot;), fill = c(&quot;skyblue2&quot;, &quot;tomato2&quot;)) The box can be removed using the bty = \"n\" argument and the size can be changed using cex. The position can be specified either with words (like above) or by using x-y coordinates. Here is a more complex example: # 1) extract and sort the hours for two fisheries from fewest to most hours fished t_hrs = sort(dat$hours[dat$fishery == &quot;Tournament&quot;]) n_hrs = sort(dat$hours[dat$fishery == &quot;Non.Tournament&quot;]) # 2) make the plot: plot for t_hrs only, but ensure xlim covers both groups # set the margins: # 4 lines of margin space on bottom and left, # 1 on top and right par(mar = c(4,4,1,1)) plot(x = t_hrs, y = 1:length(t_hrs), type = &quot;o&quot;, lty = 2, xlim = range(c(t_hrs, n_hrs)), xlab = &quot;Hours Fished/Year&quot;, ylab = &quot;Rank within Fishery Samples&quot;, las = 1) # las = 1 says &quot;turn the y-axis tick labels to be horizontal&quot; # 3) add info for the other fishery points(x = n_hrs, y = 1:length(n_hrs), type = &quot;o&quot;, lty = 1, pch = 16) # 4) add the legend legend(&quot;topleft&quot;, legend = c(&quot;Tournament&quot;, &quot;Non-Tournament&quot;), lty = c(2,1), pch = c(1, 16), bty = &quot;n&quot;) Notice that you need to be careful about the order of how you specify which lty and pch settings match up with the elements of the legend argument. In the plot() code, you specified that the lty = 2 but didn’t specify what pch should be (it defaults to 1). So when you put the “Tournament” group first in the legend argument vector, you must be sure to use the corresponding plotting codes. The first element of the lty argument matches up with the first element of legend, and so on. Note the other new plotting trick used in the code above: the rotation of y-axis tick mark labels using las = 1. 2.10 Exporting Plots There are two main ways to save static permanent copies of your R plots. The first is a quick-and-dirty point-and-click method that saves the plots in low-resolution. Because you have to point and click, this cannot be automated. The second method produces cleaner-looking high-resolution plots with code that can be embedded in your script, ensuring the same exact plot will be created each time the code is executed. 2.10.1 Click Save If your plot is in the RStudio built-in graphics device: Right above the plot, click Export &gt; Save as Image. Change the name, dimensions and file type. If your plot is in a plotting device window (opened with windows() or quartz(): Simply go to File &gt; Save. All plots will be saved in the working directory by default. You can also just copy the plot to your clipboard (File &gt; Copy to the clipboard &gt; bitmap) and paste it where you want. You should save one of the plots you made in this Chapter using this approach. 2.10.2 Use a function to place plot in a new file If you are producing plots for a final report or publication, you want the output to be as clean-looking as possible and you want them to be fully reproducible so when something changes with your data or analysis in review, you can reproduce the same figure with the new results. You can save high-resolution plots using the following steps: # step 1: Make a pixels per inch object ppi = 600 # step 2: Call the figure file creation function png(&quot;TestFigure.png&quot;, h = 8 * ppi, w = 8 * ppi, res = ppi) # step 3: Run the plot # put all of your plotting code here (without windows()) # step 4: Close the device dev.off() A file will be saved in your working directory containing the plot made by the code in step 3 above. The ppi object is pixels-per-inch. When you specify h = 8 * ppi, you are saying “make a plot with height equal to 8 inches”. There are similar functions to make PDFs, tiff files, jpegs, etc. If you use pdf(), simply specify h and w without the ppi and no res argument. You should save one of the plots you made in this Chapter using this approach. 2.11 Bonus Topic: Error Bars Rarely should you ever present estimates without some measure of uncertainty. The most common way for visualizing the uncertainty in an estimate is by using error bars, which can be added to an R plot using the lower-level function arrows(). To use arrows(), you need: Vectors of the x and y coordinates of the lower bound of the error bars Vectors of the x and y coordinates of the upper bound of the error bars The syntax for arrows() is as follows: arrows(x0, y0, x1, y1, ...), where x0 and y0 are the coordinates you are drawing “from” (e.g., lower limits) and the x1 and y1 are the coordinates you are drawing “to” (e.g., upper limits). The ... represents other arguments to change how the error bars look. Calculate the means of the hours fished in each fishery sector and plot them. (If you don’t remember how tapply() works, revisit Section 1.9). x_bar = tapply(dat$hours, dat$fishery, mean) par(mar = c(4,4,1,1)) barplot(x_bar) Say you want to add error bars that represent 95% confidence intervals on the mean. You can create a 95% confidence interval using this basic formula: \\[\\begin{equation} \\bar{x} \\pm 1.96 * SE(\\bar{x}), \\tag{2.1} \\end{equation}\\] where \\[\\begin{equation} \\bar{x}=\\frac{1}{n}\\sum_i^n{x_i}, \\tag{2.2} \\end{equation}\\] and \\[\\begin{equation} SE(\\bar{x})=\\frac{\\sqrt{\\frac{\\sum_i^n{(x_i - \\bar{x})^2}}{n-1}}}{\\sqrt{n}} \\tag{2.3} \\end{equation}\\] Begin by creating a function to calculate the standard error (\\(SE(\\bar{x})\\)): calc_se = function(x) { sqrt(sum((x - mean(x))^2)/(length(x)-1))/sqrt(length(x)) } Then calculate the standard errors for each fishery sector: se = tapply(dat$hours, dat$fishery, calc_se) Then calculate the lower and upper limits of your error bars: lwr = x_bar - 1.96 * se upr = x_bar + 1.96 * se Then draw them on using the arrows() function: par(mar = c(4,4,1,1)) mp = barplot(x_bar, ylim = range(c(0, upr))) arrows(x0 = mp, y0 = lwr, x1 = mp, y1 = upr, length = 0.1, angle = 90, code = 3) Notice four things: The use of mp to specify the x coordinates. If you do mp = barplot(...), mp will contain the x coordinates of the midpoint of each bar. x0 and x1 are the same: you want to have vertical bars, so these must be the same while y1 and y2 differ. The use of ylim = range(c(0, upr)): you want the y-axis to show the full range of all the error bars. The three arguments at the end of arrows(): length = 0.1: the length of the arrow heads, fiddle with this until you like it. angle = 90: the angle of the arrow heads, you want 90 here for the error bars. code = 3: indicates that arrow heads should be drawn on both ends of the arrow. Exercise 2 For this exercise, you will be making a few plots and changing how they look to suit your taste. You will use a real data set (sockeye.csv, see the instructions on how to acquire the data files) from a sockeye salmon (Oncorhynchus nerka) population from the Columbia/Snake River system, obtained from Kline and Flagg (2014). This population spawns in Redfish Lake in Idaho, which feeds into the Salmon River, which is a tributary of the Snake River. In order to reach the lake, the sockeye salmon must successfully pass through a total of eight dams that have fish passage mechanisms in place. The Redfish Lake population is one of the most endangered sockeye populations in the U.S. and travels farther (1,448 km), higher (1,996 m), and is the southernmost breeding population of all sockeye populations in the world (Kline and Flagg 2014). Given this uniqueness, a captive breeding program was initiated in 1991 to conserve the genes from this population. These data came from both hatchery-raised and wild fish and include average female spawner weight (g), fecundity (number of eggs), egg size (eggs/g), and % survival to the eyed-egg stage. The solutions to this exercise are found at the end of this book (here). You are strongly recommended to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped. Create a new R script called Ex2.R and save it in the Chapter2 directory. Read in the data set sockeye.csv. Produce a basic summary of the data and take note of the data classes, missing values (NA), and the relative ranges for each variable. Make a histogram of fish weights for only hatchery-origin fish. Set breaks = 10 so you can see the distribution more clearly. Make a scatter plot of the fecundity of females as a function of their body weight for wild fish only. Use whichever plotting character (pch) and color (col) you want. Change the main title and axes labels to reflect what they mean. Change the x-axis limits to be 600 to 3000 and the y-axis limits to be 0 to 3500. (Hint: The NAs will not cause a problem. R will only use points where there are paired records for both x and y and ignore otherwise). Add points for the same variables, but from hatchery fish. Use a different plotting character and a different color. Add a legend to the plot to differentiate between the two types of fish. Make a multi-panel plot in a new window with box-and-whisker plots that compare (1) spawner weight, (2) fecundity, and (3) egg size between hatchery and wild fish. (Hint: each comparison will be on its own panel). Change the titles of each plot to reflect what you are comparing. Save the plot as a .png file in your working directory with a file name of your choosing. EXERCISE 2 BONUS Make a bar plot comparing the mean survival to eyed-egg stage for each type of fish (hatchery and wild). Add error bars that represent 95% confidence intervals. Change the names of each bar, the main plot title, and the y-axis title. Adjust the margins so there are 2 lines on the bottom, 5 on the left, 2 on the top, and 1 on the right. References "],
["ch3.html", "Chapter 3 Basic Statistics Chapter Overview Before You Begin 3.1 The General Linear Model 3.2 The Generalized Linear Model 3.3 Probability Distributions 3.4 Bonus Topic: Non-linear Regression Exercise 3", " Chapter 3 Basic Statistics Chapter Overview In this chapter, you will get familiar with the basics of using R for the purpose it was designed: statistical analysis. You will learn how to: fit and interpret the output from various general linear models: simple linear regression models ANOVA (same as \\(t\\)-test but with more than two groups) ANCOVA models Interactions conduct basic model selection fit basic GLMs: the logistic regression model Bonus topic: fitting non-linear regression models using nls() R has gained popularity as a statistics software and is commonly used both in academia and governmental resource agencies. This popularity is likely a result of its power, flexibility, intuitive nature, and price (free!). For many students, this chapter may be the one that is most immediately useful for applying R to their own work. IMPORTANT NOTE: If you did not attend the sessions corresponding to Chapters 1 or 2, you are recommended to walk through the material found in those chapters before proceeding to this material. Also note that if you are confused about a topic, you can use CTRL + F to find previous cases where that topic has been discussed in this book. Before You Begin You should create a new directory and R script for your work in this Chapter. Create a new R script called Ch3.R and save it in the directory C:/Users/YOU/Documents/R-Book/Chapter3. Set your working directory to that location. Revisit the material in Sections 1.2 and 1.3 for more details on these steps. 3.1 The General Linear Model Much of this chapter will focus on the general linear model, so it is important to become familiar with it. The general linear model is a family of models that allows you to determine the relationship (if any) between some continuous response variable (\\(y\\)) and some predictor variable(s) (\\(x_n\\)) and is often written as: \\[\\begin{equation} y_i=\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_j x_{ij}+ ... + \\beta_n x_{in} + \\varepsilon_i, \\varepsilon_i \\sim N(0,\\sigma), \\tag{3.1} \\end{equation}\\] where the subscript \\(i\\) represents an individual observation. The predictor variable(s) can be either categorical (i.e., grouping variables used in ANOVA, \\(t\\)-test, etc.), continuous (regression), or a combination of categorical and continuous (ANCOVA). The main purpose of fitting a linear model is to estimate the coefficients (\\(\\beta\\)), and in some cases to determine if their values are “significantly” different from the value assumed by some null hypothesis. The model makes several assumptions about the residuals24 to obtain estimates of the coefficients. For reliable inference, the residuals must: be independent be normally-distributed have constant variance across the range that \\(x_n\\) was observed In R, the general linear model is fitted using the lm() function. The basic syntax is lm(y ~ x, data = dat)25; it says: “fit a model with y as the response variable and x as the sole predictor variable, and look for those variables in a data frame called dat”. 3.1.1 Simple Linear Regression Read the data found in sockeye.csv (see in the instructions for help with acquiring the data) into R. This is the same data set you used in Exercise 2 - revisit this section for more details on the meaning of the variables. These are real data and are presented in Kline and Flagg (2014). dat = read.csv(&quot;../Data/sockeye.csv&quot;) head(dat) ## year type weight fecund egg_size survival ## 1 1991 hatch NA NA NA NA ## 2 1992 hatch NA NA NA NA ## 3 1993 hatch 1801 2182 12.25 46.58 ## 4 1994 hatch 1681 2134 7.92 50.98 ## 5 1995 hatch 2630 1576 21.61 68.06 ## 6 1996 hatch 2165 2171 8.74 63.43 To fit a regression model using lm(), both x and y must be continuous (numeric) variables. In the data set dat, two such variables are called weight and fecund. Fit a regression model where you link the average fecundity (number of eggs) of fish sampled in an individual year to the average weight (in grams) for that year. Ignore for now that the fish come from two sources: hatchery and wild origin. fit1 = lm(fecund ~ weight, data = dat) If you run just the fit1 object, you will see the model you ran along with the coefficient estimates of the intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)): fit1 ## ## Call: ## lm(formula = fecund ~ weight, data = dat) ## ## Coefficients: ## (Intercept) weight ## 1874.6496 0.2104 The mathematical formula for this model looks like this: \\[\\begin{equation} y_i=\\beta_0 + \\beta_1 x_{i1} + \\varepsilon_i, \\varepsilon_i \\sim N(0,\\sigma), \\tag{3.2} \\end{equation}\\] where \\(x_{i1}\\) is weight. The coefficients are interpreted as: \\(\\beta_0\\): the y-intercept (mean fecund at zero weight) \\(\\beta_1\\): the slope (change in fecund for one unit increase in weight) For more information about the model fit, you can use the summary() function: summary(fit1) ## ## Call: ## lm(formula = fecund ~ weight, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -873.67 -389.28 -71.65 482.96 1041.24 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1874.6496 269.4369 6.958 4.33e-08 *** ## weight 0.2104 0.1803 1.167 0.251 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 500.6 on 35 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.03745, Adjusted R-squared: 0.009945 ## F-statistic: 1.362 on 1 and 35 DF, p-value: 0.2511 Again the coefficient estimates are shown, but now you see the uncertainty on the parameter estimates (standard errors), the test statistic, and the p-value testing the null hypothesis that each coefficient has a zero value. Here you can see that the p-value does not support rejection of the null hypothesis that the slope is zero. You can see the residual standard error (variability of data around the fitted line and the estimate of \\(\\sigma\\)), the \\(R^2\\) value (the proportion of variation in fecund explained by variation in weight), and the p-value of the overall model. You can easily see the model fit by using the abline() function. Make a new plot and add the fitted regression line: plot(fecund ~ weight, data = dat, col = &quot;grey&quot;, pch = 16, cex = 1.5) abline(fit1) It fits, but not very well. It seems there are two groups: one with data points mostly above the line and one with data points mostly below the line. You’ll now run a new model to test whether those clusters of points are random or due to actual differences between groups. 3.1.2 ANOVA: Categorical predictors ANOVA models attempt to determine if the means of different groups are different. You can fit them in the same basic lm() framework. But first, notice that: class(dat$type); levels(dat$type) ## [1] &quot;factor&quot; ## [1] &quot;hatch&quot; &quot;wild&quot; tells you the type variable is a factor. It has levels of \"hatch\" and \"wild\" which indicate the origin of the adult spawning fish sampled each year. If you pass lm() a predictor variable with a factor class, R will automatically fit it as an ANOVA model. See Section 1.5 for more details on factors. Factors have levels that are explicitly ordered. By default, this ordering happens alphabetically: if your factor has levels \"a\", \"b\", and \"c\", they will be assigned the order of 1, 2 and 3, respectively. You can always see how R is ordering your factor by doing something similar to this: pairs = cbind( as.character(dat$type), as.numeric(dat$type) ) head(pairs); tail(pairs) ## [,1] [,2] ## [1,] &quot;hatch&quot; &quot;1&quot; ## [2,] &quot;hatch&quot; &quot;1&quot; ## [3,] &quot;hatch&quot; &quot;1&quot; ## [4,] &quot;hatch&quot; &quot;1&quot; ## [5,] &quot;hatch&quot; &quot;1&quot; ## [6,] &quot;hatch&quot; &quot;1&quot; ## [,1] [,2] ## [39,] &quot;wild&quot; &quot;2&quot; ## [40,] &quot;wild&quot; &quot;2&quot; ## [41,] &quot;wild&quot; &quot;2&quot; ## [42,] &quot;wild&quot; &quot;2&quot; ## [43,] &quot;wild&quot; &quot;2&quot; ## [44,] &quot;wild&quot; &quot;2&quot; The functions as.character() and as.numeric() are coercion functions: they attempt to change the way something is interpreted. Notice that the level \"hatch\" is assigned the order 1 because it comes before \"wild\" alphabetically. The first level is termed the reference level because it is the group that all other levels are compared to when fitting a model. You can change the reference level using dat$type_rlvl = relevel(dat$type, ref = \"wild\"). You are now ready to fit the ANOVA model, which will measure the size of the difference in the mean fecund between different levels of the factor type: fit2 = lm(fecund ~ type, data = dat) Think of this model as being written as: \\[\\begin{equation} y_i=\\beta_0 + \\beta_1 x_{i1} + \\varepsilon_i \\tag{3.3} \\end{equation}\\] and assume that \\(x_{i1} = 0\\) if observation \\(i\\) is a fish from the \"hatch\" level and \\(x_{i1} = 1\\) if observation \\(i\\) is a fish from the \"wild\" level. Note that Equations (3.2) and (3.3) are the same, the only thing that differs is the coding of the variable \\(x_{i1}\\). In the ANOVA case: \\(\\beta_0\\) (the intercept) is the mean fecund for the \"hatch\" level and \\(\\beta_1\\) is the difference in mean fecund: \"wild\" - \"hatch\". So when you run coef(fit2) to extract the coefficient estimates and get: ## (Intercept) typewild ## 1846.2500 713.3971 you see that the mean fecundity of hatchery fish is about 1846 eggs and that the average wild fish has about 713 more eggs than the average hatchery fish across all years. The fact that the p-value associated with the typewild coefficient when you run summary(fit2) is less than 0.05 indicates that there is statistical evidence that the difference in means is not zero. Verify your interpretation of the coefficients: m = tapply(dat$fecund, dat$type, mean, na.rm = T) # b0: m[1] ## hatch ## 1846.25 # b1: m[2] - m[1] ## wild ## 713.3971 3.1.3 ANCOVA: Continuous and categorical predictors Now that you have seen that hatchery and wild fish tend to separate along the fecundity axis (as evidenced by the ANOVA results above), you would like to include this in your original regression model. You will fit two regression lines within the same model: one for hatchery fish and one for wild fish. This model is called an ANCOVA model and looks like this: \\[\\begin{equation} y_i=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i \\tag{3.4} \\end{equation}\\] If \\(x_{i1}\\) is type coded with 0’s and 1’s as in Section 3.1.2 and \\(x_{i2}\\) is weight, then the coefficients are interpreted as: \\(\\beta_0\\): the y-intercept of the \"hatch\" level (the reference level) \\(\\beta_1\\): the difference in mean fecund at the same weight: \"wild\" - \"hatch\" \\(\\beta_2\\): the slope of the fecund versus weight relationship (this model assumes the lines have common slopes, i.e., that the lines are parallel) You can fit this model and extract the coefficients table from the summary: fit3 = lm(fecund ~ type + weight, data = dat) summary(fit3)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1039.6417645 175.0992184 5.937444 1.038268e-06 ## typewild 866.9909998 96.2473387 9.007948 1.578239e-10 ## weight 0.5173716 0.1051039 4.922477 2.164460e-05 And you can plot the fit: plot(fecund ~ weight, data = dat, col = &quot;grey&quot;, pch = ifelse(dat$type == &quot;hatch&quot;, 1, 16), cex = 1.5) abline(coef(fit3)[c(1,3)], lty = 2) abline(sum(coef(fit3)[c(1,2)]), coef(fit3)[3]) legend(&quot;bottom&quot;, legend = c(&quot;Hatchery&quot;, &quot;Wild&quot;), pch = c(1,16), lty = c(2,1), col = &quot;grey&quot;, pt.cex = 1.5, bty = &quot;n&quot;, horiz = T) Study this code to make sure you know what each line is doing. Use what you know about the meanings of the three coefficients to decipher the two abline() commands. Remember that abline() takes takes two arguments: a is the intercept and b is the slope. 3.1.4 Interactions Above, you have included an additional predictor variable (and parameter) in your model to help explain variation in the fecund variable. However, you have assumed that the effect of weight on fecundity is common between hatchery and wild fish (note the parallel lines in the figure above). You may have reason to believe that the effect of weight depends on the origin of the fish, e.g., wild fish may tend to accumulate more eggs than hatchery fish for the same increase in weight. Cases where the magnitude of the effect depends on the value of another predictor variable are known as “interactions”. You can write the interactive ANCOVA model like this: \\[\\begin{equation} y_i=\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\beta_3 x_{i1} x_{i2} + \\varepsilon_i \\tag{3.5} \\end{equation}\\] If \\(x_{i1}\\) is type coded with 0’s and 1’s as in Section 3.1.2 and \\(x_{i2}\\) is weight, then the coefficients are interpreted as: \\(\\beta_0\\): the y-intercept of the \"hatch\" level (the reference level) \\(\\beta_1\\): the difference in y-intercept between the \"wild\" level and the \"hatch\" level. \\(\\beta_2\\): the slope of the \"hatch\" level \\(\\beta_3\\): the difference in slope between the \"wild\" level and the \"hatch\" level. You can fit this model: fit4 = lm(fecund ~ type + weight + type:weight, data = dat) # or # fit4 = lm(fecund ~ type * weight, data = dat) The first option above is more clear in its statement, but both do the same thing. Plot the fit. Study these lines to make sure you know what each is doing. Use what you know about the meanings of the four coefficients to decipher the two abline() commands. plot(fecund ~ weight, data = dat, col = &quot;grey&quot;, pch = ifelse(dat$type == &quot;hatch&quot;, 1, 16), cex = 1.5) abline(coef(fit4)[c(1,3)], lty = 2) abline(sum(coef(fit4)[c(1,2)]), sum(coef(fit4)[c(3,4)])) legend(&quot;bottom&quot;, legend = c(&quot;Hatchery&quot;, &quot;Wild&quot;), pch = c(1,16), lty = c(2,1), col = &quot;grey&quot;, pt.cex = 1.5, bty = &quot;n&quot;, horiz = T) Based on the coefficients table: summary(fit4)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1175.7190847 174.545223 6.7358995 1.125255e-07 ## typewild -42.8721082 398.980751 -0.1074541 9.150794e-01 ## weight 0.4300894 0.105592 4.0731253 2.732580e-04 ## typewild:weight 0.7003389 0.299104 2.3414560 2.539681e-02 It seems that fish of the different origins have approximately the same intercept, but that their slopes are quite different. 3.1.5 AIC Model Selection You have now fitted four different models, each that makes different claims about how you can predict the fecundity of a given sockeye salmon at Redfish Lake. If you are interested in determining which of these models you should use for prediction, you need to use model selection. Model selection attempts to find the model that is likely to have the smallest out-of-sample prediction error (i.e., future predictions will be close to what actually happens). One model selection metric is Akaike’s Information Criterion (AIC), which is excellently described with ecological examples in Anderson, Burnham, and Thompson (2000). Lower AIC values mean the model should have better predictive performance. Obtain a simple AIC table from your fitted model objects and sort the table by increasing values of AIC: tab = AIC(fit1, fit2, fit3, fit4) tab[order(tab$AIC),] ## df AIC ## fit4 5 522.0968 ## fit3 4 525.7834 ## fit2 3 543.6914 ## fit1 3 568.9166 In general, AIC values that are different by more than 2 units are interpreted as having importantly different predictive performance (Anderson, Burnham, and Thompson 2000). Based on this very quick-and-dirty analysis, it seems that in predicting future fecundity, you would want to use the interactive ANCOVA model. 3.2 The Generalized Linear Model The models you fitted above are called “general linear models”. They all make the assumption that the residuals (\\(\\varepsilon_i\\)) are normally-distributed and that the response variable and the predictor variables are linearly-related. Oftentimes data and analyses do not follow this assumption. For such cases you should use the broader family of statistical models known as generalized linear models26. 3.2.1 Logistic Regression One example is in the case of binary data. Binary data have two opposite outcomes, e.g., success/failure, lived/died, male/female, spawned/gravid, happy/sad, etc. If you want to predict how the probability of one outcome over its opposite changes depending on some other variable, then you need to use the logistic regression model, which is written as: \\[\\begin{equation} logit(p_i)=\\beta_0 + \\beta_1 x_{i1} + ... + \\beta_j x_{ij}+ ... + \\beta_n x_{in}, y_i \\sim Bernoulli(p_i) \\tag{3.6} \\end{equation}\\] Where \\(p_i\\) is the probability of success for trial \\(i\\) (\\(y_i = 1\\)) at the values of the predictor variables \\(x_{ij}\\). The \\(logit(p_i)\\) is the link function that links the linear parameter scale to the data scale. It constrains the value of \\(p_i\\) to be between 0 and 1 regardless of the values of the \\(\\beta\\) coefficients. The logit link function can be expressed as: \\[\\begin{equation} logit(p_i) = log\\left(\\frac{p_i}{1-p_i}\\right) \\tag{3.7} \\end{equation}\\] which is the log odds - the natural logarithm of the odds, which is a measure of how likely the event is to happen relative to it not happening27. Make an R function to calculate the transformation performed by the link function: logit = function(p) { log(p/(1 - p)) } The generalized linear model calculates the log odds of an outcome: logit(p[i]) (which is given by the \\(\\beta\\) coefficients and the \\(x_{ij}\\) data in Equation (3.6)), so if you want to know the odds ratio of the outcome, you can simply take the inverse log. But, if you want to know the probability of the outcome p[i], you have to apply the inverse logit function: \\[\\begin{equation} expit(\\eta_i)=\\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} \\tag{3.8} \\end{equation}\\] where \\(\\eta_i = logit(p_i)\\). Make an R function to perform the inverse logit transformation to the probability scale: expit = function(eta) { # lp stands for logit(p) exp(eta)/(1 + exp(eta)) } Because all of the fitted \\(\\beta\\) coefficients are on the log scale, you cannot make easy interpretations about the relationships between the predictor variables and the observed outcomes without making these transformations. The following example will take you through the steps to fit a logistic regression model and how to interpret the model outputs using these functions. Fit a logistic regression model to the sockeye salmon data. None of the variables of interest are binary, but you can create one. Look at the variable dat$survival. This is the average % survival of all eggs laid that make it to the “eyed-egg” stage. Suppose any year with over 70% egg survival is considered a successful year, so create a new variable binary which takes on a 0 if dat$survival is less than 70% and a 1 otherwise. dat$binary = ifelse(dat$survival &lt; 70, 0, 1) This will be your response variable (\\(y\\)) and your model will estimate how the probability (\\(p\\)) of binary being a 1 changes (or doesn’t) depending on the value of other variables (\\(x_{n}\\)). Analogous to the simple linear regression model (Section 3.1.1), estimate how \\(p\\) changes with weight: (How does the probability of having a successful clutch relate to the weight of the fish?) fit1 = glm(binary ~ weight, data = dat, family = binomial) summary(fit1)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.363441330 1.76943946 2.466002 0.01366306 ## weight -0.002819271 0.00125243 -2.251040 0.02438303 The coefficients are interpreted as (remember, “success” is defined as having at least 70% egg survival to the stage of interest): \\(\\beta_0\\): the log odds of success for a year in which average fish weight was 0 (which is not all that important, let alone difficult to interpret). It can be transformed into more interpretable quantities: \\(e^{\\beta_0}\\) is the odds of success for a year in which average fish weight was 0 and \\(expit(e^{\\beta_0})\\) is the probability of success for a year in which average fish weight was 0. \\(\\beta_1\\): the additive effect of fish weight on the log odds of success. More interpretable expressions are: \\(e^{\\beta_1}\\) is the ratio of the odds of success at two consective weights (e.g., 1500 and 1501) and Claims about \\(e^{\\beta_1}\\) are made as “for every one gram increase in average weight, success became \\(e^{\\beta_1}\\) times as likely to happen”. You can predict the probability of success at any weight using \\(expit(\\beta_0 + \\beta_1 weight)\\): # create a sequence of weights to predict at wt_seq = seq(min(dat$weight, na.rm = T), max(dat$weight, na.rm = t), length = 100) # extract the coefficients and get p p = expit(coef(fit1)[1] + coef(fit1)[2] * wt_seq) You can plot the fitted model: plot(p ~ wt_seq, type = &quot;l&quot;, lwd = 3, ylim = c(0,1), las = 1) Fit another model comparing the probability of success between hatchery and wild fish (analogous to the ANOVA model in Section 3.1.2): fit2 = glm(binary ~ type, data = dat, family = binomial) summary(fit2)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.2006707 0.4494666 -0.4464641 0.6552620 ## typewild 1.3793257 0.7272845 1.8965421 0.0578884 An easier way to obtain the predicted probability is by using the predict function: predict(fit2, newdata = data.frame(type = c(&quot;hatch&quot;, &quot;wild&quot;)), type = &quot;response&quot;) ## 1 2 ## 0.4500000 0.7647059 This plugs in the two possible values of the predictor variable and asks for the fitted probabilities. Incorporate the origin type into your original model: fit3 = glm(binary ~ type + weight, data = dat, family = binomial) and obtain the fitted probabilities for each group at each weight: p_hatch = predict( fit3, newdata = data.frame(type = &quot;hatch&quot;, weight = wt_seq), type = &quot;response&quot; ) p_wild = predict( fit3, newdata = data.frame(type = &quot;wild&quot;, weight = wt_seq), type = &quot;response&quot; ) and plot them: plot(p_wild ~ wt_seq, type = &quot;l&quot;, lwd = 3, lty = 1, ylim = c(0,1), las =1, xlab = &quot;Weight (g)&quot;, ylab = &quot;Pr(&gt;70% Egg Survival)&quot; ) lines(p_hatch ~ wt_seq, lwd = 3, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Hatchery&quot;, &quot;Wild&quot;), lty = c(2,1), lwd = 3, bty = &quot;n&quot;) Look for an interaction (all the code is the same except use glm(binary ~ type * weight) instead of glm(binary ~ type + weight) and change everything to fit4 instead of fit3). plot(p_wild ~ wt_seq, type = &quot;l&quot;, lwd = 3, lty = 1, ylim = c(0,1), las =1, xlab = &quot;Weight (g)&quot;, ylab = &quot;Pr(&gt;70% Egg Survival)&quot; ) lines(p_hatch ~ wt_seq, lwd = 3, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Hatchery&quot;, &quot;Wild&quot;), lty = c(2,1), lwd = 3, bty = &quot;n&quot;) 3.2.2 AIC Model Selection You may have noticed that you just did the same analysis with binary as the response instead of fecund. Perform an AIC analysis to determine which model is likely to be best for prediction: tab = AIC(fit1, fit2, fit3, fit4) tab[order(tab$AIC),] ## df AIC ## fit1 2 45.40720 ## fit3 3 46.00622 ## fit4 4 47.33006 ## fit2 2 50.07577 The best model includes weight only, although there is not much confidence in this conclusion (based on how similar the AIC values are). 3.3 Probability Distributions A probability distribution is a way of representing the probability of an event or value of a parameter and they are central to statistical theory. Some of the most commonly used distributions are summarized in Table 3.1, along with the suffixes of the functions in R that correspond to each distribution. For an excellent and ecologically-focused description of probability distributions, checkout Chapter 4 in Bolker (2008) on them28. Table 3.1: A brief description of probability distributions commonly used in ecological problems, including the function suffix in R. Type Distribution Common Uses R Suffix Continuous Normal Models the relative frequency of outcomes that are symmetric around a mean, can be negative -norm() Lognormal Models the relative frequency of outcomes that are normally-distributed on the log-scale -lnorm() Uniform Models values that are between two endpoints and that all occur with the same frequency -unif() Beta Models values that are between 0 and 1 -beta() Discrete Binomial Models the number of successes from a given number of trials when there are only two possible outcomes and all trials have the same probability of success -binom() Multinomial The same as the binomial distribution, but when there are more than two possible outcomes -multinom() Poisson Used for count data in cases where the variance and mean are roughly equal -pois() In R, there are four different ways to use each of these distribution functions (each has a separate prefix): The probability density (or mass) function (d-): the height of the probability distribution function at some given value of the random variable. The cumulative density function (p-): what is the sum of the probability densities for all random variables below the input argument q. The quantile function (-q): what value of the random variable do p% fall below? The random deviates function (-r): generates random variables from the distribution in proportion to their probability density. Suppose that \\(x\\) represents the length of individual age 6 largemouth bass in your private fishing pond. Assume that \\(x \\sim N(\\mu=500, \\sigma=50)\\)29. Here is the usage of each of the distribution functions and a plot illustrating them: # parameters mu = 500; sig = 50 # a sequence of possible random variables (fish lengths) lengths = seq(200, 700, length = 100) # a sequence of possible cumulative probabilities cprobs = seq(0, 1, length = 100) # use the four functions densty = dnorm(x = lengths, mean = mu, sd = sig) # takes specific lengths cuprob = pnorm(q = lengths, mean = mu, sd = sig) # takes specific lengths quants = qnorm(p = cprobs, mean = mu, sd = sig) # takes specific probabilities random = rnorm(n = 1e4, mean = mu, sd = sig) # takes a number of random deviates to make # set up plotting region: see ?par for more details # notice the tricks to clean up the plot par( mfrow = c(2,2), # set up 2x2 regions mar = c(3,3,3,1), # set narrower margins xaxs = &quot;i&quot;, # remove &quot;x-buffer&quot; yaxs = &quot;i&quot;, # remove &quot;y-buffer&quot; mgp = c(2,0.4,0), # bring in axis titles ([1]) and tick labels ([2]) tcl = -0.25 # shorten tick marks ) plot(densty ~ lengths, type = &quot;l&quot;, lwd = 3, main = &quot;dnorm()&quot;, xlab = &quot;Fish Length (mm)&quot;, ylab = &quot;Density&quot;, las = 1, yaxt = &quot;n&quot;) # turns off y-axis axis(side = 2, at = c(0.002, 0.006), labels = c(0.002, 0.006), las = 2) plot(cuprob ~ lengths, type = &quot;l&quot;, lwd = 3, main = &quot;pnorm()&quot;, xlab = &quot;Fish Length (mm)&quot;, ylab = &quot;Cumulative Probability&quot;, las = 1) plot(quants ~ cprobs, type = &quot;l&quot;, lwd = 3, main = &quot;qnorm()&quot;, xlab = &quot;P&quot;, ylab = &quot;P Quantile Length (mm)&quot;, las = 1) hist(random, breaks = 50, col = &quot;grey&quot;, main = &quot;rnorm()&quot;, xlab = &quot;Fish Length (mm)&quot;, ylab = &quot;Frequency&quot;, las = 1) box() # add borders to the histogram Figure 3.1: The four -norm functions with input (x-axis) and output (y-axis) displayed. Notice that pnorm() and qnorm() are inverses of one another: if you put the output of one into the input of the other, you get the original input back: qnorm(pnorm(0)) ## [1] 0 pnorm(0) asks R to find the probability that \\(x\\) is less than zero for the standard normal distribution (\\(N(0,1)\\) - this is the default if you don’t specify mean and sd). qnorm(pnorm(0)) asks R to find the value of \\(x\\) that pnorm(0) * 100% of the possible values fall below. If the nesting is confusing, this line is the same as: p = pnorm(0) qnorm(p) 3.4 Bonus Topic: Non-linear Regression You fitted linear and logistic regression models in Sections 3.1.1 and 3.2.1, however, R allows you to fit non-linear regression models as well. First, read the data into R: dat = read.csv(&quot;../Data/feeding.csv&quot;); summary(dat) ## prey cons ## Min. : 1.00 Min. : 1.00 ## 1st Qu.:11.25 1st Qu.: 8.00 ## Median :26.00 Median :11.00 ## Mean :25.08 Mean : 9.92 ## 3rd Qu.:37.75 3rd Qu.:13.00 ## Max. :49.00 Max. :15.00 These are hypothetical data from an experiment in which you were interested in quantifying the functional feeding response30 of a fish predator on zooplankton in an aquarium. You experimentally manipulated the prey density (prey) and counted how many prey items were consumed (cons). Plot the data: plot(cons ~ prey, data = dat, cex = 1.5, pch = 16, col = &quot;grey&quot;) You can see a distinct non-linearity to the relationship. The Holling Type II functional response31 has this functional form: \\[\\begin{equation} y_i=\\frac{ax_i}{1+ahx_i} + \\varepsilon_i, \\varepsilon_i \\sim N(0, \\sigma) \\tag{3.9} \\end{equation}\\] where \\(x_i\\) is prey and \\(y_i\\) is cons. You can fit this model in R using the nls() function: fit = nls(cons ~ (a * prey)/(1 + a * h * prey), data = dat, start = c(a = 3, h = 0.1)) In general, it behaves very similarly to the lm() function, however there are a few differences: You need to specify the functional form of the curve you are attempting to fit. In using lm(), the terms are all additive (e.g., type + weight), but in using nls(), this is not the case. For example, note the use of division. You may need to provide starting values for the parameters (coefficients) you are estimating. This is because nls() will use a search algorithm to find the parameters of the best fit line, and it may need to have a reasonable idea of where to start looking for it to work properly. You cannot plot the fit using abline() anymore, because you have more parameters than just a slope and intercept, and the relationship between \\(x\\) and \\(y\\) is no longer linear. Despite these differences, you can obtain similar output as from lm() by using the summary(), coef(), and predict() functions. prey_seq = seq(min(dat$prey), max(dat$prey), length = 100) cons_seq = predict(fit, newdata = data.frame(prey = prey_seq)) Draw the fitted line over top of the data: plot(cons ~ prey, data = dat, cex = 1.5, pch = 16, col = &quot;grey&quot;) lines(cons_seq ~ prey_seq, lwd = 3) Exercise 3 You should create a new R script called Ex3.R in your working directory for this chapter. You will again be using the sockeye.csv data found in Kline and Flagg (2014). The solutions to this exercise are found at the end of this book (here). You are strongly recommended to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped. Perform the same analyses as conducted in Section 3.1 (simple linear regression, ANOVA, ANCOVA, ANCOVA with interaction), using egg_size as the response variable. The predictor variables you should use are type (categorical) and year. You should plot the fit for each model separately and perform an AIC analysis. Practice interpreting the coefficient estimates. Perform the same analyses as conducted in Section 3.2, this time using a success threshold of 80% survival to the eyed-egg stage (instead of 70%). Use egg_size and type as the predictor variables. You should plot the fitted lines for each model separately and perform an AIC analysis. Practice interpreting the coefficient estimates. Make the same graphic as in Figure 3.1 with at least one of the other distributions listed in Table 3.1 (other than the multinomial - being a multivariate distribution, it wouldn’t work well with this code). Try thinking of a variable from your work that meets the uses of each distribution in Table 3.1 (or one that’s not listed). If you run into trouble, check out the help file for that distribution32. Exercise 3 Bonus Fit a von Bertalanffy growth model to the data found in the growth.csv data file. Visit Section 4.7.1 (particularly Equation (4.2)) for details on this model. Use the initial values: linf = 600, k = 0.3, t0 = -0.2. Plot the fitted line over top of the data. References "],
["ch4.html", "Chapter 4 Monte Carlo Methods Chapter Overview Before You Begin Layout of This Chapter 4.1 Introducing Randomness 4.2 Reproducing Randomness 4.3 Replication 4.4 Function Writing 4.5 Summarization 4.6 Simulation-Based Examples 4.7 Resampling-Based Examples 4.8 Exercise 4", " Chapter 4 Monte Carlo Methods Chapter Overview Simulation modeling is one of the primary reasons to move away from spreadsheet-type programs (like Microsoft Excel) and into a program like R. R allows you to replicate the same (possibly complex and detailed) calculations over and over with different random values. You can then summarize and plot the results of these replicated calculations all within the same program. Analyses of this type are called Monte Carlo methods: they randomly sample from a set of quantities for the purpose of generating and summarizing a distribution of some statistic related to the sampled quantities. If this concept is confusing, hopefully this chapter will clarify. In this chapter, you will learn the basic skills needed for simulation (i.e., Monte Carlo) modeling in R including: introduce randomness to a model repeat calculations many times with replicate() and for() loops summarization of many values from a distribution more advanced function writing IMPORTANT NOTE: If you did not attend the sessions corresponding to Chapters 1 or 2 or 3, you are recommended to walk through the material found in those chapters before proceeding to this material. Remember that if you are confused about a topic, you can use CTRL + F to find previous cases where that topic has been discussed in this book. Before You Begin You should create a new directory and R script for your work in this Chapter. Create a new R script called Ch4.R and save it in the directory C:/Users/YOU/Documents/R-Book/Chapter4. Set your working directory to that location. Revisit the material in Sections 1.2 and 1.3 for more details on these steps. Layout of This Chapter This chapter is divided into two main sections: Required Material (Sections 4.1 - 4.5) which is necessary to understand the examples in this chapter and the subsequent chapters Example Cases (Sections 4.6 and 4.7) which apply the skills learned in the required material. In the workshop session, you will walkthrough 2-3 of these example cases at the choice of the group of the participants. If you are interested in simulation modeling, you are suggested to work through all of the example cases, as slightly different tricks will be shown in the different examples. 4.1 Introducing Randomness A critical part of simulation modeling is the use of random processes. A random process is one that generates a different outcome according to some rules each time it is executed. They are tightly linked to the concept of uncertainty: you are unsure about the outcome the next time the process is executed. There are two basic ways to introduce randomness in R: random deviates and resampling. 4.1.1 Random deviates In Section 3.3, you learned about using probability distributions in R. One of the uses was the r- family of distribution functions. These functions create random numbers following a random process specified by a probability distribution. Consider animal survival as an example. At the end of each year, each individual alive at the start can either live or die. There are two outcomes here, and suppose each animal has an 80% chance of surviving. The number of individuals that survive is the result of a binomial random process in which there were \\(n\\) individuals alive at the start of this year and \\(p\\) is the probability that any one individual survives to the next year. You can execute one binomial random process where \\(p = 0.8\\) and \\(n = 100\\) like this: rbinom(n = 1, size = 100, prob = 0.8) ## [1] 82 The result you get will almost certainly be different from the one printed here. That is the random component. You can execute many such binomial processes by changing the n argument. Plot the distribution of expected surviving individuals: survivors = rbinom(1000, 100, 0.8) hist(survivors, col = &quot;skyblue&quot;) Another random process is the lognormal process: it generates random numbers such that the log of the values are normally-distributed with mean equal to logmean and standard deviation equal to logsd: hist(rlnorm(1000, 0, 0.1), col = &quot;skyblue&quot;) There are many random processes you can use in R. Checkout Table 3.1 for more examples as well as the help files for each individual function for more details. 4.1.2 Resampling Using random deviates works great for creating new random numbers, but what if you already have a set of numbers that you wish to introduce randomness to? For this, you can use resampling techniques. In R, the sample() function is used to sample size elements from the vector x: sample(x = 1:10, size = 5) ## [1] 5 7 2 9 8 You can sample with replacement (where it is possible to sample the same element two or more times): sample(x = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), size = 10, replace = T) ## [1] &quot;b&quot; &quot;c&quot; &quot;c&quot; &quot;b&quot; &quot;a&quot; &quot;c&quot; &quot;a&quot; &quot;c&quot; &quot;c&quot; &quot;c&quot; You can set probabilities on the sampling of different elements33: sample(x = c(&quot;live&quot;, &quot;die&quot;), size = 10, replace = T, prob = c(0.8, 0.2)) ## [1] &quot;live&quot; &quot;live&quot; &quot;die&quot; &quot;live&quot; &quot;live&quot; &quot;live&quot; &quot;live&quot; &quot;die&quot; &quot;live&quot; &quot;live&quot; Notice that this is the same as the binomial random process above, but with only 10 trials and the printing of the outcomes rather than the number of successes. 4.2 Reproducing Randomness For reproducibility purposes, you may wish to get the same exact random numbers each time you run your script. To do this, you need to set the random seed, which is the starting point of the random number generator your computer uses. If you run these two lines of code, you should get the same result as printed here: set.seed(1234) rnorm(1) ## [1] -1.207066 4.3 Replication To use Monte Carlo methods, you need to be able to replicate some random process many times. There are two main ways this is commonly done: either withreplicate() or with for() loops. 4.3.1 replicate() The replicate() function executes some expression many times and returns the output from each execution. Say we have a vector x, which represents 30 observations of fish length (mm): x = rnorm(30, 500, 30) We wish to build the sampling distribution of the mean length “by hand”. We can sample randomly from it, calculate the mean, then repeat this process many times: means = replicate(n = 1000, expr = { x_i = sample(x, length(x), replace = T) mean(x_i) }) If we take mean(means) and sd(means), that should be very similar to mean(x) and se(x). Create the se() function (also shown in Section 2.11) and prove this to yourself: se = function(x) sd(x)/sqrt(length(x)) mean(means); mean(x) ## [1] 493.4968 ## [1] 493.4166 sd(means); se(x) ## [1] 4.967572 ## [1] 5.044153 4.3.2 The for() loop In programming, a loop is a command that does something over and over until it reaches some point that you specify. R has a few types of loops: repeat(), while(), and for(), to name a few. for() loops are among the most common in simulation modeling. A for() loop repeats some action for however many times you tell it for each value in some vector. The syntax is: for (var in seq) { expression(var) } The loop calculates the expression for values of var for each element in the vector seq. For example: for (i in 1:5) { print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 The print() command will be executed 5 times: once for each value of i. It is the same as: i = 1; print(i^2); i = 2; print(i^2); i = 3; print(i^2); i = 4; print(i^2); i = 5; print(i^2) If you remove the print() function, see what happens: for (i in 1:5) { i^2 } Nothing is printed to the console. R did the calculation, but did not show you or store the result. Often, you’ll need to store the results of the calculation in a container object: results = numeric(5) This makes an empty numeric vector of length 5 that are all 0’s. You can store the output of your loop calculations in results: for (i in 1:5) { results[i] = i^2 } results ## [1] 1 4 9 16 25 When i^2 is calculated, it will be placed in the element results[i]. This was a trivial example, because you would never use a for() loop to do things as simple as vectorized calculation. The expression (1:5)^2 would give the same result with significantly less code (see Section 1.6). However, there are times where it is advantageous to use a loop. Particularly in cases where: the calculations in one element are determined from the value in previous elements, such as in time series models the calculations have multiple steps you wish to store multiple results you wish to track the progress of your calculations As an illustration for item (1) above, build a (very) basic population model. At the start of the first year, the population abundance is 1000 individuals and grows by an average factor of 1.1 per year (reproduction and death processes result in a growth rate of 10%) before harvest. The growth rate varies randomly, however. Each year, the 1.1 growth factor has variability introduced by small changes in survival and reproductive process. Model these variations as lognormal random variables. After production, 8% of the population is harvested. Simulate the abundance at the end of the year for 100 years: nt = 100 # number of years N = NULL # container for abundance N[1] = 1000 # first end-of-year abundance for (t in 2:nt) { # N this year is N last year * growth * # randomness * fraction that survive harvest N[t] = (N[t-1] * 1.1 * rlnorm(1, 0, 0.1)) * (1 - 0.08) } Plot the abundance time series: plot(N, type = &quot;l&quot;, pch = 15, xlab = &quot;Year&quot;, ylab = &quot;Abundance&quot;) Examples of the other three utilities of using for() loops over replicate are shown in the example cases and exercises. 4.4 Function Writing In Monte Carlo analyses, it is often useful to wrap code into functions. This allows for easy replication and setting adjustment (e.g., if you wanted to compare the growth trajectories of two populations with differing growth rates). As an example, turn the population model shown above into a function: pop_sim = function(nt, grow, sd_grow, U, plot = F) { N = NULL # empty flexible vector container N[1] = 1000 for (t in 2:nt) { N[t] = (N[t-1] * grow * rlnorm(1, 0, sd_grow)) * (1 - U) } if (plot) { plot(N, type = &quot;l&quot;, pch = 15, xlab = &quot;Year&quot;, ylab = &quot;Abundance&quot;) } N } This function takes five inputs: nt: the number of years, grow: the population growth rate, sd_grow: the amount of annual variability in the growth rate U: the annual exploitation rate plot: whether you wish to have a plot created. It has a default setting of FALSE: if you don’t specify plot = T when you call pop_sim(), you won’t see a plot made. It returns one output: the vector of population abundance. Execute your simulation function once using the same settings as before: pop_sim(100, 1.1, 0.1, 0.08, T) Now, you wish to replicate this simulation 1000 times. Use the replicate() function to do this: out = replicate(n = 1000, expr = pop_sim(100, 1.1, 0.1, 0.08, F)) If you do dim(out), you’ll see that years are stored as rows (there are 100 of them) and replicates are stored as columns (there are 1000 of them). Notice how wrapping the code in the function made the replicate() call easy. Here are some advantages of wrapping code like this into a function: If you do the same task at multiple places in your script, you don’t need to type all of the code to perform the task, just the function call. If you need to change the way the function behaves (i.e., the function body), you only need to change it in one place: in the function definition. You can easily change the settings of the code (e.g., whether or not you want to see the plot) in one place Function writing can lead to shorter scripts Function writing can lead to more readable code (if it is easy for readers to interpret what your functions do - informative function and argument names, as well as documentation can help here) 4.5 Summarization After replicating a calculation many times, you will need to summarize the results. Here are several examples using the out matrix from Section 4.4. 4.5.1 Central Tendency You can calculate the mean abundance each year across your iterations using the apply() function (Section 1.9): N_mean = apply(out, 1, mean) N_mean[1:10] ## [1] 1000.000 1017.150 1034.970 1052.022 1068.358 1086.700 1102.685 ## [8] 1125.383 1141.774 1159.921 You could do the same thing using median rather than mean. The mode is more difficult to calculate in R, if you need to get the mode, try to Google it34. 4.5.2 Variability One of the primary reasons to conduct a Monte Carlo analysis is to obtain estimates of variability. You can summarize the variability easily using the quantile() function: # obtain the 10% and 90% quantiles each year across iterations N_quants = apply(out, 1, function(x) quantile(x, c(0.1, 0.9))) Notice how a user-defined function was passed to apply(). Now plot the summary of the randomized abundances as a time series like before: plot(N_mean, type = &quot;l&quot;, ylim = c(0, 10000)) lines(N_quants[1,], lty = 2) lines(N_quants[2,], lty = 2) The range within the two dashed lines represents the range that encompassed the central 80% of the random abundances each year. 4.5.3 Frequencies Often you will want to count how many times something happened. In some cases, the fraction of times something happened can be interpreted as a probability of that event occuring. The table() function is useful for counting occurrences of discrete events. Suppose you are interested in how many of your iterations resulted in fewer than 1000 individuals at year 10: out10 = ifelse(out[10,] &lt; 1000, &quot;less10&quot;, &quot;greater10&quot;) table(out10) ## out10 ## greater10 less10 ## 649 351 Suppose you are also interested in how many of your iterations resulted in fewer than 1100 individuals at year 20: out20 = ifelse(out[20,] &lt; 1100, &quot;less20&quot;, &quot;greater20&quot;) table(out20) ## out20 ## greater20 less20 ## 587 413 Now suppose you are interested in how these two metrics are related: table(out10, out20) ## out20 ## out10 greater20 less20 ## greater10 486 163 ## less10 101 250 One example of an interpretation of this output might be that populations that were greater than 1000 at year 10 were commonly greater than 1100 at year 20. Also, if a population was less than 1000 at year 10, it was more likely to be less than 1100 at year 20 than to be greater than it. You can turn these into probabilities (if you believe your model represents reality) by dividing each cell by the total number of iterations: round(table(out10, out20)/1000, 2) ## out20 ## out10 greater20 less20 ## greater10 0.49 0.16 ## less10 0.10 0.25 4.6 Simulation-Based Examples 4.6.1 Test rnorm In this example, you will verify that the function rnorm() works the same way that qnorm() and pnorm() indicate that it should work. That is, you will verify that random deviates generated using rnorm() have the same properties as the true normal distribution given by qnorm() and pnorm(). Hopefully it will also reinforce the way the random, quantile, and cumulative distribution functions work in R. First, specify the mean and standard deviation for this example: mu = 500; sig = 30 Now make up n (any number of your choosing, something greater than 10) random deviates from this normal distribution: random = rnorm(100, mu, sig) Test the quantiles (obtain the values that p * 100% of the quantities fall below, both for random numbers and from the qnorm() function): p = seq(0.01, 0.99, 0.01) random_q = quantile(random, p) normal_q = qnorm(p, mu, sig) plot(normal_q ~ random_q); abline(c(0,1)) The fact that all the quantiles fall around the 1:1 line suggests the n random samples are indeed from a normal distribution. Any deviations you see are due to sampling errors. If you increase n to n = 1e6 (one million), you’ll see no deviations. This is called a q-q plot, and is frequently used to assess the fit of data to a distribution. Now test the random values in their agreement with the pnorm() function. Plot the cumulative density functions for the truly normal curve and the one approximated by the random deviates: q = seq(400, 600, 10) random_cdf = ecdf(random) random_p = random_cdf(q) normal_p = pnorm(q, mu, sig) plot(normal_p ~ q, type = &quot;l&quot;, col = &quot;blue&quot;) points(random_p ~ q, col = &quot;red&quot;) The ecdf() function obtains the empirical cumulative density function (which is just pnorm() for a sample). It allows you to plug in any random variable and obtain the probability of having one less than it. 4.6.2 Stochastic Power Analysis A power analysis is one where the analyst wishes to determine how much power they will have to detect an effect. Power is inversely related to the probability of making a Type II Error: failing to reject a false null hypothesis35. In other words, having high power means that you have a high chance of detecting an effect if an effect truly exists. Power is a function of the effect size, the sample size n, and the variability in the data. Strong effects are easier to detect than weak ones, more samples increase the test’s sensitivity (the ability to detect weak effects), and lower variability results in more power. You can conduct a power analysis using stochastic simulation (i.e., a Monte Carlo analysis). Here, you will write a power analysis to determine how likely are you to be able to correctly identify what you deem to be a biologically-meaningful difference in survival between two tagging procedures. You know one tagging procedure has approximately a 10% mortality rate (10% of tagged fish die within the first 12 hours as result of the tagging process). Another cheaper, and less labor-intensive method has been proposed, but before implementing it, your agency wishes to determine if it will have a meaningful impact on the reliability of the study or on the ability of the crew to tag enough individuals that will survive long enough to be useful. You and your colleagues determine that if the mortality rate of the new tagging method reaches 25%, then gains in time and cost-efficiency would be offset by needing to tag more fish (because more will die). You have decided to perform a small-scale study to determine if using the new method could result in 25% or more mortality. The study will tag n individuals using both methods (new and old) and track the fraction that survived after 12 hours. Before performing the study however, you deem it important to determine how large n needs to be to answer this question. You decide to use a stochastic power analysis to help your research group. The small-scale study can tag a total of at most 100 fish with the currently available resources. Could you tag fewer than 100 total individuals and still have a high probability of detecting a statistically significant difference in mortality? The stochastic power analysis approach works like this (this is called psuedocode): Simulate data under the reality that the difference is real with n observations per treatment, where n &lt; 100/2 Fit the model that will be used when the real data are collected to the simulated data Determine if the difference was detected with a significant p-value Replicate steps 1 - 3 many times Replicate step 4 while varying n over the interval from 10 to 50 Determine what fraction of the p-values were deemed significant at each n Step 2 will require fitting a generalized linear model; for a review, revisit Section 3.2 (specifically Section 3.2.1 on logistic regression). First, create a function that will generate data, fit the model, and determine if the p-value is significant (steps 1-3 above): sim_fit = function(n, p_old = 0.10, p_new = 0.25) { ### step 1: create the data ### # generate random response data dead_old = rbinom(n, size = 1, prob = p_old) dead_new = rbinom(n, size = 1, prob = p_new) # create the predictor variable method = rep(c(&quot;old&quot;, &quot;new&quot;), each = n) # create a data.frame to pass to glm df = data.frame(dead = c(dead_old, dead_new), method = method) # relevel so old is the reference df$method = relevel(df$method, ref = &quot;old&quot;) ### step 2: fit the model ### fit = glm(dead ~ method, data = df, family = binomial) ### step 3: determine if a sig. p-value was found ### # extract the p-value pval = summary(fit)$coef[2,4] # determine if it was found to be significant pval &lt; 0.05 } Next, for steps 4 and 5, set up a nested for loop. This will have two loops: one that loops over sample sizes (step 5) and one that loops over replicates of each sample size (step 4). First, create the looping objects and containers: I = 500 # the number of replicates at each sample size n_try = seq(10, 50, 10) # the test sample sizes N = length(n_try) # count them # container: out = matrix(NA, I, N) # matrix with I rows and N columns Now perform the nested loop. The inner-loop iterations will be completed for each element of n in the sequence 1:N. The output (which is one element: TRUE or FALSE based on the significance of the p-value) is stored in the corresponding row and column for that iteration of that sample size. for (n in 1:N) { for (i in 1:I) { out[i,n] = sim_fit(n = n_try[n]) } } You now have a matrix of TRUE and FALSE elements that indicates whether a significant difference was found at the \\(\\alpha = 0.05\\) level if the effect was truly as large as you care about. You can obtain the proportion of all the replicates at each sample size that resulted in a significant difference using the mean() function with apply(): plot(apply(out, 2, mean) ~ n_try, type = &quot;l&quot;, xlab = &quot;Tagged Fish per Treatment&quot;, ylab = &quot;Probability of Finding Effect (Power)&quot;) Even if you tagged 100 fish total, you would only have a 49% chance of saying the effect (which truly is there!) is present under the null hypothesis testing framework. Suppose you and your colleagues aren’t relying on p-values in this case, and are purely interested in how precisely the effect size would be estimated. Adapt your function to determine how frequently you would be able to estimate the true mortality of the new method within +/- 5% based on the point estimate only (the estimate for the tagging mortality of the new method must be between 0.2 and 0.3 for a successful study). Change your function to calculate this additional metric and re-run the analysis: sim_fit = function(n, p_old = 0.10, p_new = 0.25) { # create the data dead_old = rbinom(n, size = 1, prob = p_old) dead_new = rbinom(n, size = 1, prob = p_new) # create the predictor variable method = rep(c(&quot;old&quot;, &quot;new&quot;), each = n) # create a data.frame to pass to glm df = data.frame(dead = c(dead_old, dead_new), method = method) # relevel so old is the reference df$method = relevel(df$method, ref = &quot;old&quot;) # fit the model fit = glm(dead ~ method, data = df, family = binomial) # extract the p-value pval = summary(fit)$coef[2,4] # determine if it was found to be significant sig_pval = pval &lt; 0.05 # obtain the estimated mortality rate for the new method p_new_est = predict(fit, data.frame(method = c(&quot;new&quot;)), type = &quot;response&quot;) # determine if it is +/- 5% from the true value prc_est = p_new_est &gt;= (p_new - 0.05) &amp; p_new_est &lt;= (p_new + 0.05) # return a vector with these two elements c(sig_pval = sig_pval, prc_est = unname(prc_est)) } # containers: out_sig = matrix(NA, I, N) # matrix with I rows and N columns out_prc = matrix(NA, I, N) # matrix with I rows and N columns for (n in 1:N) { for (i in 1:I) { tmp = sim_fit(n = n_try[n]) # run sim out_sig[i,n] = tmp[&quot;sig_pval&quot;] # extract and store significance metric out_prc[i,n] = tmp[&quot;prc_est&quot;] # extract and store precision metric } } par(mfrow = c(1,2), mar = c(4,4,1,0)) plot(apply(out_sig, 2, mean) ~ n_try, type = &quot;l&quot;, xlab = &quot;Tagged Fish per Treatment&quot;, ylab = &quot;Probability of Finding Effect (Power)&quot;) plot(apply(out_prc, 2, mean) ~ n_try, type = &quot;l&quot;, xlab = &quot;Tagged Fish per Treatment&quot;, ylab = &quot;Probability of a Precise Estimate&quot;) It seems that even if you tagged 50 fish per treatment, you would have a 60% chance of estimating that the mortality rate is between 0.2 and 0.3 if it was truly 0.25. You and your colleagues consider these results and determine that you will need to somehow acquire more funds to tag more fish in the small-scale study in order to have a high level of confidence in the results. 4.6.3 Harvest Policy Analysis In this example, you will simulate population dynamics under a more realistic model than in Sections 4.3.2 and 4.4 for the purpose of evaluating different harvest policies. Suppose you are a fisheries research biologist, and a commercial fishery for pink salmon (Oncorhynchus gorbuscha) takes place in your district. For the past 10 years, it has been fished with an exploitation rate of 40% (40% of the fish that return each year have been harvested, exploitation rate is abbreviated by \\(U\\)), resulting in an average annual harvest of 8.5 million fish. The management plan is up for evaluation this year, and your supervisor has asked you to prepare an analysis that determines if more harvest could be sustained if a different exploitation rate were to be used in the future. Based on historical data, your best understanding implies that the stock is driven by Ricker spawner-recruit dynamics. That is, the total number of fish that return this year (recruits) is a function of the total number of fish that spawned (spawners) in the year of their birth. The Ricker model can be written this way: \\[\\begin{equation} R_t = \\alpha S_{t-1} e^{-\\beta S_{t-1} + \\varepsilon_t} ,\\varepsilon_t \\sim N(0,\\sigma) \\tag{4.1} \\end{equation}\\] where \\(\\alpha\\) is a parameter representing the maximum recruits per spawner (obtained at very low spawner abundances) and \\(\\beta\\) is a measure of the strength of density-dependent mortality. Notice that the error term is in the exponent, which makes \\(e^{\\varepsilon_t}\\) lognormal. You have estimates of the parameters36: \\(\\alpha = 6\\) \\(\\beta = 1 \\times 10^{-7}\\) \\(\\sigma = 0.4\\) You decide that you can build a policy analysis by simulating the stock forward through time under different exploitation rates. With enough iterations of the simulation, you will be able to see whether a different exploitation rate can provide more harvest than what is currently being extracted. First, write a function for your population model. Your function must: take the parameters, dimensions (number of years), and the policy variable (\\(U\\)) as input arguments simulate the population using Ricker dynamics calculate and return the average harvest and escapement over the number of future years you simulated. # Step #1: name the function and give it some arguments ricker_sim = function(ny, params, U) { # extract the parameters out by name: alpha = params[&quot;alpha&quot;] beta = params[&quot;beta&quot;] sigma = params[&quot;sigma&quot;] # create containers # this is a neat trick to condense your code: R = S = H = NULL # initialize the population in the first year # start the population at being fished at 40% # with lognormal error R[1] = log(alpha * (1 - 0.4))/(beta * (1 - 0.4)) * exp(rnorm(1, 0, sigma)) S[1] = R[1] * (1 - U) H[1] = R[1] * U # carry simulation forward through time for (y in 2:ny) { # use the ricker function with random lognormal noise R[y] = S[y-1] * alpha * exp(-beta * S[y-1] + rnorm(1, 0, sigma)) #harvest and spawners are the same as before S[y] = R[y] * (1 - U) H[y] = R[y] * U } # wrap output in a list object list( mean_H = mean(H), mean_S = mean(S) ) } Use the function once: params = c(alpha = 6, beta = 1e-7, sigma = 0.4) out = ricker_sim(U = 0.4, ny = 20, params = params) #average annual harvest (in millions) round(out$mean_H/1e6, digits = 2) ## [1] 7.99 If you completed the stochastic power analysis example (Section 4.6.2), you might see where this is going. You are going to replicate applying a fixed policy many times to a random system. This is the Monte Carlo part of the analysis. The policy part is that you will compare the output from several candidate exploitation rates to inform a decision about which is best. This time, set up your analysis using sapply() (to iterate over different values of \\(U\\)) and replicate() (to iterate over different random populations fished at each \\(U\\)) instead of performing a nested for() loop as in previous examples: U_try = seq(0.4, 0.6, 0.01) n_rep = 2000 H_out = sapply(U_try, function(u) { replicate(n = n_rep, expr = { ricker_sim(U = u, ny = 20, params = params)$mean_H/1e6 }) }) The nested replicate() and sapply() method is a bit cleaner than a nested for() loop, but you have less control over the format of the output. Plot the output of your simulations using a boxplot. To make things easier, give H_out column names representing the exploitation rate: colnames(H_out) = U_try boxplot(H_out, outline = F, xlab = &quot;U&quot;, ylab = &quot;Harvest (Millions of Fish)&quot;, col = &quot;tomato&quot;, las = 1) It appears the stock could produce more harvest than its current 8.5 million fish per year if it was fished harder. However, your supervisors also do not want to see the escapement drop below three-quarters of what it has been in recent history (75% of approximately 13 million fish). They ask you to obtain the expected average annual escapement as well as harvest. You can simply re-run the code above, but extracting S_mean rather than H_mean. Call this output S_out and plot it just like harvest (if you’re curious, this blue color is col = \"skyblue\"): After seeing this information, your supervisor realizes they are faced with a trade-off: the stock could produce more with high exploitation rates, but they are concerned about pushing the stock too low would be unsustainable. They tell you to determine the probability that the average escapement would not be pushed below 75% of 13 million at each exploitation rate, as well as the probability that the average annual harvests will be at least 20% greater than they are currently (approximately 8.5 million fish). Given your output, this is easy: # determine if each element meets escapement criterion Smeet = S_out &gt; (0.75 * 13) # determine if each element meets harvest criterion Hmeet = H_out &gt; (1.2 * 8.5) # calculate the probability of each occuring at a given exploitation rate # remember, mean of a logical vector calculate the proportion of TRUEs p_Smeet = apply(Smeet, 2, mean) p_Hmeet = apply(Hmeet, 2, mean) You plot this for your supervisor as follows: # the U levels to highlight on plot plot_U = seq(0.4, 0.6, 0.05) # create an empty plot par(mar = c(4,4,1,1)) plot(p_Smeet ~ p_Hmeet, type = &quot;n&quot;, xlab = &quot;Probability of Meeting Harvest Criterion&quot;, ylab = &quot;Probability of Meeting Escapement Criterion&quot;) # add gridlines abline(v = seq(0, 1, 0.1), col = &quot;grey&quot;) abline(h = seq(0, 1, 0.1), col = &quot;grey&quot;) #draw on the tradeoff curve lines(p_Smeet ~ p_Hmeet, type = &quot;l&quot;, lwd = 2) # add points and text for particular U policies points(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U], pch = 16, cex = 1.5) text(p_Smeet[U_try %in% plot_U] ~ p_Hmeet[U_try %in% plot_U], labels = U_try[U_try %in% plot_U], pos = c(1,1,1,2,2)) Equipped with this analysis, your supervisor plans to go to the policy-makers with the recommendation of adjusting the exploitation rate policy to use \\(U = 0.5\\), because they think it balances the trade-off. Notice how if the status quo was maintained, your model suggests you would have complete certainty of staying where you are now: escapement will remain above 75% of its current level with a 100% chance, but you would have no chance of improving harvests to greater than 20% of their current level. Small increases in the exploitation rate (e.g., from 0.4 to 0.45) have a reasonably large gain in harvest performance, but hardly any losses for the escapement criterion. Your supervisor is willing to live with a 90% chance that the escapement will stay where they desire in order to gain a &gt;80% chance of obtaining the desired amount of increases in harvest. The utility of using Monte Carlo methods in this example is the ability to calculate the probability of some event you are interested in. There are analytical (i.e., not simulation-based) solutions to predict the annual harvest and escapement from a fixed \\(U\\) from a population with parameters \\(\\alpha\\) and \\(\\beta\\), but by incorporating randomness, you were able to obtain the relative weights of outcomes other than the expectation under the deterministic Ricker model, thereby allowing the assignment of probabilities to meeting the two criteria. 4.7 Resampling-Based Examples 4.7.1 The Bootstrap Say you have a fitted model from which you want to propagate the uncertainty in some derived quantity. Consider the case of the von Bertalanffy growth model. This is a non-linear model used to predict the size of an organism (weight or length) based on its age. The model can be written for a non-linear regression model (see Section 3.4) as: \\[\\begin{equation} L_i = L_{\\infty}\\left(1 - e^{-k(age_i-t_0)}\\right) + \\varepsilon_i, \\varepsilon_i \\sim N(0, \\sigma) \\tag{4.2} \\end{equation}\\] where \\(L_i\\) and \\(age_i\\) are the observed length and age of individual \\(i\\), respectively, and \\(L_{\\infty}\\), \\(k\\), and \\(t_0\\) are parameters to be estimated. The interpretations of the parameters are as follows: \\(L_{\\infty}\\): the maximum average length achieved \\(k\\): a growth coefficient linked to metabolic rate. It specifies the rate of increase in length as the fish ages early in life \\(t_0\\): the theoretical age when length equals zero (the x-intercept). Use the data set growth.csv for this example (see the instructions on acquiring data files). Read in and plot the data: dat = read.csv(&quot;../Data/growth.csv&quot;) plot(length ~ age, data = dat, pch = 16, col = &quot;grey&quot;) Due to a large amount of variability in individual growth rates, the relationship looks pretty noisy. Notice how you have mostly young fish in your sample: this is characteristic of “random” sampling of fish populations. Suppose you would like to obtain the probability that an average-sized fish of each age is sexually mature. You know that fish of this species mature at approximately 450 mm, and you simply need to determine the fraction of all fish at each age that are greater than 450 mm. However, you don’t have any observations for some ages (e.g., age 8), so you cannot simply calculate this fraction based on your raw data. You need to fit the von Bertalanffy growth model, then carry the statistical uncertainty from the fitted model forward to the predicted length-at-age. This would be difficult to obtain using only the coefficient estimates and their standard errors, because of the non-linear relationship between the \\(x\\) and \\(y\\) variables. Enter the bootstrap, which is a Monte Carlo analysis using an observed data set and a model. The pseudocode for a bootstrap analysis is: Resample from the original data (with replacement) Fit a model of interest Derive some quantity of interest from the fitted model Repeat steps 1 - 3 many times Summarize the randomized quantities from step 4 In this example, you will apply a bootstrap approach to obtain the distribution of expected fish lengths at each age, then use these distributions to quantify the probability that an averaged-sized fish of each age is mature (i.e., greater than 450 mm). You will write a function for each of steps 1 - 3 above. The first is to resample the data: randomize = function(dat) { # number of observed pairs n = nrow(dat) # sample the rows to determine which will be kept keep = sample(x = 1:n, size = n, replace = T) # retreive these rows from the data dat[keep,] } Notice the use of replace = T here: without this, there would be no bootstrap. You would just sample the same observations over and over, their order in the rows would just be shuffled. Next, write a function to fit the model (revisit Section 3.4 for more details on nls()): fit_vonB = function(dat) { nls(length ~ linf * (1 - exp(-k * (age - t0))), data = dat, start = c(linf = 600, k = 0.3, t0 = -0.2) ) } This function will return a fitted model object when executed. Next, write a function to predict mean length-at-age: # create a vector of ages ages = min(dat$age):max(dat$age) pred_vonB = function(fit) { # extract the coefficients ests = coef(fit) # predict length-at-age ests[&quot;linf&quot;] * (1 - exp(-ests[&quot;k&quot;] * (ages - ests[&quot;t0&quot;]))) } Notice your function will use the object ages even though it was not defined in the function. This has to do with lexical scoping and environments, which are beyond the scope of this introductory material. If you’d like more details, see the section in Wickham (2015) on it37. Basically, if an object with the same name as one defined in the function exists outside of the function, the function will use the one that is defined within the function. If there is no object defined in the function with that name, it will look outside of the function for that object. Now, use these three functions to perform one iteration: pred_vonB(fit = fit_vonB(dat = randomize(dat = dat))) You can wrap this inside of a replicate() call to perform step 4 above: set.seed(2) out = replicate(n = 100, expr = { pred_vonB(fit = fit_vonB(dat = randomize(dat = dat))) }) dim(out) ## [1] 10 100 It appears the rows are different ages and the columns are different bootstrapped iterations. Summarize the random lengths at each age: summ = apply(out, 1, function(x) c(mean = mean(x), quantile(x, c(0.025, 0.975)))) Plot the data, the summarized ranges of mean lengths, and the length at which all fish are assumed to be mature (450 mm) plot(length ~ age, data = dat, col = &quot;grey&quot;, pch = 16, ylim = c(0, max(dat$length, summ[&quot;97.5%&quot;,])), ylab = &quot;Length (mm)&quot;, xlab = &quot;Age (years)&quot;) lines(summ[&quot;mean&quot;,] ~ ages, lwd = 2) lines(summ[&quot;2.5%&quot;,] ~ ages, col = &quot;grey&quot;) lines(summ[&quot;97.5%&quot;,] ~ ages, col = &quot;grey&quot;) abline(h = 450, col = &quot;blue&quot;) Obtain the fraction of iterations that resulted in the mean length-at-age being greater than 450 mm. This is interpreted as the probability that the average-sized fish of each age is mature: p_mat = apply(out, 1, function(x) mean(x &gt; 450)) plot(p_mat ~ ages, type = &quot;b&quot;, pch = 17, xlab = &quot;Age (years)&quot;, ylab = &quot;Probability of Average Fish Mature&quot;) This maturity schedule can be used by fishery managers in attempting to decide which ages should be allowed to be harvested and which should be allowed to grow more38. Because each age has an associated expected length, managers can use what they know about the size selectivity of various gear types to set policies that attempt to target some ages more than others. 4.7.2 Permutation Test In the previous example (Section 4.7.1), you learned about the bootstrap. A related Monte Carlo analysis is the permutation test. This is a non-parametric statistical test used to determine if there is a statistically-significant difference in the mean of some quantity between two populations. It is used in cases where the assumptions of a generalized linear model may not be met, but a p-value is still required. The pseudocode for the permutation test is: Calculate the difference between means based on the original data set Shuffle the group assignments randomly among the observations Calculate the difference between the randomly-assigned groups Repeat steps 2 - 3 many times. This builds the null distribution: the distribution of the test statistic (the difference) assuming the null hypothesis (that there is no difference) in means is true Determine what fraction of the absolute differences were larger than the original difference. This constitutes a two-tailed p-value. One-tailed tests can also be derived using the same steps 1 - 4, which is left as an exercise. Use the data set ponds.csv for this example (see the instructions on acquiring data files). This is the same data set used for Exercise 1B, revisit that exercise for details on this hypothetical data set. Read in and plot the data: dat = read.csv(&quot;ponds.csv&quot;) plot(chl.a ~ treatment, data = dat) It appears as though there is a relatively strong signal indicating a difference. Use the permutation test to determine if it is statistically significant. Step 1 from the pseudocode is to calculate the observed difference between groups: Dobs = mean(dat$chl.a[dat$treatment == &quot;Add&quot;]) - mean(dat$chl.a[dat$treatment == &quot;Control&quot;]) Dobs ## [1] 26.166 Write a function to perform one iteration of steps 2 - 3 from the pseudocode: # x is the group: Add or Control # y is chl.a perm = function(x, y) { # turn x to a character, easier to deal with x = as.character(x) # shuffle the x values: x_shuff = sample(x) # calculate the mean of each group: x_bar_add = mean(y[x_shuff == &quot;Add&quot;]) x_bar_ctl = mean(y[x_shuff == &quot;Control&quot;]) # calculate the difference: x_bar_add - x_bar_ctl } Use your function once: perm(x = dat$treatment, y = dat$chl.a) ## [1] 10.648 Perform step 4 from the pseudocode by replicating your perm() function many times: Dnull = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$chl.a)) Plot the distribution of the null test statistic and draw a line where the originally-observed difference falls: hist(Dnull, col = &quot;grey&quot;) abline(v = Dobs, col = &quot;blue&quot;, lwd = 3, lty = 2) Notice the null distribution is centered on zero: this is because the null hypothesis is that there is no difference. The observation (blue line) falls way in the upper tail of the null distribution, indicating it is unlikely that an effect that large was observed by random chance. The two-tailed p-value can be calculated as: mean(abs(Dnull) &gt;= Dobs) ## [1] 0 Very few (or zero) of the random data sets resulted in a difference greater than what was observed, indicating there is statistical support to the hypothesis that there is a non-zero difference between the two nutrient treatments. 4.8 Exercise 4 In these exercises, you will be adapting the code written in this chapter to investigate slightly different questions. You should create a new R script Ex4.R in your working directory for these exercises so your chapter code is left unchanged. Exercise 4A is based solely on the required material and Exercises 4B - 4F are based on the example cases. You should work through each example before attempting each of the later exercises. The solutions to this exercise are found at the end of this book (here). You are strongly recommended to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped. Exercise 4A: Required Material Only These questions are based on the material in Sections 4.1 - 4.5 only. Simulate flipping an unfair coin (probability of heads = 0.6) 100 times using rbinom(). Count the number of heads and tails. Simulate flipping the same unfair coin 100 times, but using sample() instead. Determine what fraction of the flips resulted in heads. Simulate rolling a fair 6-sided die 100 times using sample(). Determine what fraction of the rolls resulted in an even number. Simulate rolling the same die 100 times, but use the function rmultinom() instead. Look at the help file for details on how to use this function. Determine what fraction of the rolls resulted in an odd number. Solutions Exercise 4B: Test rnorm These questions will require you to adapt the code written in Section 4.6.1 Adapt this example to investigate another univariate probability distribution, like -lnorm(), -pois(), or -beta(). See the help files (e.g., ?rpois) for details on how to use each function. Solutions Exercise 4C: Stochastic Power Analysis These questions will require you to adapt the code written in Section 4.6.2 What sample size n do you need to have a power of 0.8 of detecting a significant difference between the two tagging methods? How do the inferences from the power analysis change if you are interested in p_new = 0.4 instead of p_new = 0.25? Do you need to tag more or fewer fish in this case? Your analysis takes a bit of time to run so you are interested in tracking its progress. Add a progress message to your nested for() loop that will print the sample size currently being analyzed: for (n in 1:N) { cat(&quot;\\r&quot;, &quot;Sample Size = &quot;, n_try[n]) for (i in 1:I) { ... } } Solutions Exercise 4D: Harvest Policy Analysis These questions will require you to adapt the code written in Section 4.6.3 Add an argument to ricker_sim() that will give the user an option to create a plot that shows the time series of recruitment, harvest, and escapement all on the same plot. Set the default to be to not plot the result, in case you forget to turn it off before performing the Monte Carlo analysis. Add an error handler to ricker_sim() that will cause the function to return an error if() the names of the vector passed to the param argument aren’t what the function is expecting. You can use stop(\"Error Message Goes Here\") to have your function stop and return an error. How do the results of the trade-off analysis differ if the process error was larger (a larger value of \\(\\sigma\\))? Add implementation error to the harvest policy. That is, if the target exploitation rate is \\(U\\), make the real exploitation rate in year \\(y\\) be: \\(U_y \\sim Beta(a,b)\\), where \\(a = 100U\\) and \\(b = 100(1-U)\\). You can make there be more implementation error by inserting a smaller number other than 100 here. How does this affect the trade-off analysis? Solutions Exercise 4E: The Bootstrap These questions will require you to adapt the code written in Section 4.7.1 Replicate the bootstrap analysis, but adapt it for the linear regression example in Section 3.1.1. Stop at the step where you summarize the 95% interval range. Compare the 95% bootstrap confidence intervals to the intervals you get by running the predict() function on the original data set with the argument interval = \"confidence\". Solutions Exercise 4F: Permutation Tests These questions will require you to adapt the code written in Section 4.7.2 Adapt the code to perform a permutation test for the difference in each of the zooplankton densities between treatments. Don’t forget to fix the missing value in the chao variable. See Exercise 2 for more details on this. Adapt the code to perform a permutation test for another data set used in this book where there are observations of both a categorical variable and a continuous variable. The data sets sockeye.csv, growth.csv, or creel.csv should be good starting points. Add a calculation of the p-value for a one-tailed test (i.e., that the difference in means is greater or less than zero). Steps 1 - 4 are the same: all you need is Dnull and Dobs. Don’t be afraid to Google this if you are confused. Solutions References "],
["ch5.html", "Chapter 5 Large Data Manipulation Chapter Overview Before You Begin 5.1 Aquiring and Loading Packages 5.2 The Data 5.3 Change format using melt() 5.4 Join Data Sets with merge() 5.5 Lagged vectors 5.6 Adding columns with mutate() 5.7 Apply to all years 5.8 Calculate Daily Means with summarize() 5.9 More summarize() 5.10 Fit the Model Exercise 5", " Chapter 5 Large Data Manipulation Chapter Overview Any data analyst will tell you that oftentimes the most difficult part of an analysis is simply getting the data in the proper format. Real data sets are messy: they have missing values, variables are often stored in multiple columns that would be better stored as rows, the same factor level may be coded as two or more different types39, or the required data are in several separate files. You get the point: sometimes significant data-wrangling may be required before you perform an analysis. In this chapter, you will learn tricks to do more advanced data manipulation in R using two R packages: {reshape2} (Wickham 2007): used for changing data between formats (wide and long) {dplyr} (Wickham et al. 2017): used for large data manipulations with a consistent and readable format. You will be turning daily observations of harvest and escapement (fish that are not harvested) into annual totals for the purpose of fitting a spawner-recruit analysis on a hypothetical pink salmon (Oncorhynchus gorbuscha) data set. More details and context on these terms and topics will be provided later. IMPORTANT NOTE: If you did not attend the sessions corresponding to Chapters 1 or 2, you are recommended to walk through the material found in those chapters before proceeding to this material. Additionally, you will find the material in Section 3.4 helpful for the end of this chapter. Remember that if you are confused about a topic, you can use CTRL + F to find previous cases where that topic has been discussed in this book. Before You Begin You should create a new directory and R script for your work in this Chapter. Create a new R script called Ch5.R and save it in the directory C:/Users/YOU/Documents/R-Book/Chapter5. Set your working directory to that location. Revisit the material in Sections 1.2 and 1.3 for more details on these steps. 5.1 Aquiring and Loading Packages An R package is a bunch of code, documentation, and data that someone has written and bundled into a consistent format for other R users to install and use in their R sessions. Packages make R incredibly flexible and extensible. If you are trying to do a specialized analysis that is not included in the base R distribution, it is likely that someone has already written a package that will allow you to do it! In this chapter, you will be using some new packages that you likely don’t already have on your computer, so you will need to install them first: install.packages(&quot;dplyr&quot;) install.packages(&quot;reshape2&quot;) This requires an internet connection. You will see some text display in the console telling you the packages are being installed. Once the packages are installed, you need not re-install them in future sessions. However, if you install a new version of R, you will likely need to update your packages (i.e., re-install them). Now that the packages are on your computer, you will need to load them into the current session: library(dplyr) library(reshape2) These messages are telling you that there are functions in the {dplyr} package that have the same names as those already being used in the {stats} and {base} R packages. This is fine so long as you don’t want to use the original functions. If you wish to use the {base} version of the filter() function rather than the {dplyr} version, use it like this: base::filter(). Each time you close and reopen R, you will need to load any packages you want to use using library() again. 5.2 The Data You have daily observations of catch and escapement for every other year between 1917 and 2015. Pink salmon have a simple life history where fish that spawned in an odd year return as adults to spawn in the next odd year. There is a commercial fishery in this system located at the river mouth which harvests fish as they enter the river. A counting tower is located upstream of the fishing grounds that counts the number of fish that escaped the fishery and will have a chance to spawn (this number is termed “escapement”). The ultimate goal is to obtain annual totals of spawners and total run (catch + escapement) for the purpose of fitting a spawner recruit analysis. You will perform some other analyses along the way directed at quantifying patterns in run timing. Read in the two data sets for this chapter (see the instructions for details on acquiring data files): catch = read.csv(&quot;../Data/daily_catch.csv&quot;) esc = read.csv(&quot;../Data/daily_escape.csv&quot;) Look at the first 6 rows and columns of the catch data: catch[1:6,1:6] ## doy y_1917 y_1919 y_1921 y_1923 y_1925 ## 1 160 175 229 245 135 2685 ## 2 161 221 501 1379 1504 2361 ## 3 162 242 355 1149 13 277 ## 4 163 90 197 52 2721 548 ## 5 164 134 428 674 747 1209 ## 6 165 76 224 1211 1231 772 The column doy is the day of the year that each record corresponds to, and each column represents a different year. Notice the format of the esc data is the same: esc[1:6,1:6] ## doy y_1917 y_1919 y_1921 y_1923 y_1925 ## 1 161 90 189 161 73 1700 ## 2 162 113 412 907 819 1495 ## 3 163 124 292 756 7 176 ## 4 164 46 162 34 1481 347 ## 5 165 69 351 443 407 766 ## 6 166 39 184 796 670 489 But notice the first doy is one day later for esc than for catch. This is because of the time lag for fish to make it from the fishery grounds to the counting tower (a one day swim: fish that were in the fishing grounds on day d passed the counting tower on day d+1 if they were not harvested). 5.3 Change format using melt() These data are in what is called wide format: different levels of the year variable are stored as columns. A long format would have three columns: one for doy, year, and catch or esc. Turn the catch data frame into long format using the melt() function from {reshape2}: long.catch = melt(catch, id.var = &quot;doy&quot;, variable.name = &quot;year&quot;, value.name = &quot;catch&quot;) head(long.catch) ## doy year catch ## 1 160 y_1917 175 ## 2 161 y_1917 221 ## 3 162 y_1917 242 ## 4 163 y_1917 90 ## 5 164 y_1917 134 ## 6 165 y_1917 76 The first argument is the data frame to reformat, the second argument (id.var) is the variable that identifies one observation from another, here it is the doy that the count occurred on. In this case, it is actually all we need to run melt properly. The optional variable.name and value.name arguments specify the names of the other two columns. These default to “variable” and “value” if not specified. Do the same thing for escapement: long.esc = melt(esc, id.var = &quot;doy&quot;, variable.name = &quot;year&quot;, value.name = &quot;esc&quot;) head(long.esc) ## doy year esc ## 1 161 y_1917 90 ## 2 162 y_1917 113 ## 3 163 y_1917 124 ## 4 164 y_1917 46 ## 5 165 y_1917 69 ## 6 166 y_1917 39 Anytime you do a large data manipulation like this, you should compare the new data set with the original to verify that it worked properly. Ask if all of the daily catches for a given year in the original data set match up to the new data set: all(catch$y_2015 == long.catch[long.catch$year == &quot;y_2015&quot;, &quot;catch&quot;]) ## [1] TRUE The single TRUE indicates that every element matches up for 2015 in the catch data, just like they should. To reverse this action (i.e., to go from long format to wide format), you would use the dcast() function from {reshape2}: dcast(long.catch, doy ~ year, value.var = &quot;catch&quot;) The formula specifies which variables are ID variables on the left-hand side, and which variable should get expanded to multiple columns. The value.var argument indicates which variable will be placed into the columns. 5.4 Join Data Sets with merge() You now have two long format data sets. You can turn them into one data set with the merge() function (which is actually in the {base} package). merge() takes two data frames and joins them based on certain grouping variables. Here, you want to combine the data frames long.catch and long.esc into one data frame, and match the rows by the doy and year for each unique pair: dat = merge(x = long.esc, y = long.catch, by = c(&quot;doy&quot;, &quot;year&quot;), all = T) head(dat) ## doy year esc catch ## 1 160 y_1917 NA 175 ## 2 160 y_1919 NA 229 ## 3 160 y_1921 NA 245 ## 4 160 y_1923 NA 135 ## 5 160 y_1925 NA 2685 ## 6 160 y_1927 NA 163 The all = T argument is important to specify here. It says that you wish to keep all of the unique records in the columns passed to by from both data frames. The doy in the catch data ranges from day 160 to day 209, whereas for escapement it ranges from day 161 to day 210. In this system, the fishery opens on the 160th day of every year, and the counting tower starts the 161st day. If you didn’t use all = T, you would drop all the rows in each data set that do not have a match in the other data set (you would lose day 160 and day 210 from every year). Notice that because no escapement observations were ever made on doy 160, the esc value on this day is NA. Notice how the data set is now ordered by doy instead of year. This is not a problem, but if you want to reorder the data frame by year, you can use the arrange() function from {dplyr}: head(arrange(dat, year)) ## doy year esc catch ## 1 160 y_1917 NA 175 ## 2 161 y_1917 90 221 ## 3 162 y_1917 113 242 ## 4 163 y_1917 124 90 ## 5 164 y_1917 46 134 ## 6 165 y_1917 69 76 5.5 Lagged vectors The present task is to the calculate daily cumulative run proportion40 in 2015 for comparison to the other years. Given the information you have, this task would be very cumbersome if not for the flexibility allowed by {dplyr}. Remember the counting tower is an average one day swim for the salmon from the fishing grounds. This means that the escapement counts are lagged relative to the catch counts. If you want the daily run (defined as the number of fish in the fishing grounds each day), you need to account for this lag. An easy way to think about the lag is to show it graphically. First, pull out one year of data using the filter() function in {dplyr}: y15 = filter(dat, year == &quot;y_2015&quot;) head(y15) ## doy year esc catch ## 1 160 y_2015 NA 3342 ## 2 161 y_2015 2293 2352 ## 3 162 y_2015 1614 88 ## 4 163 y_2015 61 3270 ## 5 164 y_2015 2244 5080 ## 6 165 y_2015 3486 379 The {base} analog to the filter() function is: y15 = dat[dat$year == &quot;y_2015&quot;, ] # or y15 = subset(dat, year == &quot;y_2015&quot;) They take about the same amount of code to write, but filter() has some advantages when used with other {dplyr} functions that will be discussed later. Now plot the daily catch and escapement (not cumulative yet) for 2015: plot(catch ~ doy, data = y15, type = &quot;b&quot;, pch = 16, col = &quot;blue&quot;, xlab = &quot;DOY&quot;, ylab = &quot;Count&quot;, main = &quot;Raw Data&quot;) lines(esc ~ doy, data = y15, type = &quot;b&quot;, pch = 16, col = &quot;red&quot;) legend(&quot;topright&quot;, legend = c(&quot;Catch&quot;, &quot;Escapement&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, lty = 1, bty = &quot;n&quot;, cex = 0.8) Notice how the escapement counts are shifted to the right by one day. This is the lag. To account for it, you can use the lag() function from {dplyr}. lag() works by lagging some vector by n elements (it shifts every element in the vector n places to the right by putting n NAs at the front and removing the last n elements from the original vector). One way to fix the lag problem would be to use lag() on catch to make the days match up with esc (don’t lag esc because that would just lag it by n additional days): plot(lag(catch, n = 1) ~ doy, data = y15, type = &quot;b&quot;, pch = 16, col = &quot;blue&quot;, xlab = &quot;DOY&quot;, ylab = &quot;Count&quot;, main = &quot;With lag(catch)&quot;) lines(esc ~ doy, data = y15, type = &quot;b&quot;, pch = 16, col = &quot;red&quot;) legend(&quot;topright&quot;, legend = c(&quot;Catch&quot;, &quot;Escapement&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = 16, lty = 1, bty = &quot;n&quot;, cex = 0.8) Notice how the two curves line up better now. 5.6 Adding columns with mutate() Apply this same idea to get the total daily run (what was harvested plus what was not). Make a function to take the two vectors (catch and escapement), lag one, and then sum them. Howevery, you don’t want to move catch forward a day; you need to move escapement back a day. The opposite of lag() is lead(). The way to think of this is that lag() lags a vector, and lead() unlags a vector. lead() shifts every element to the left by n elements by removing the first n elements of the original vector and adding n NAs to the end. This is what your function should look like: lag_sum = function(x, y, n = 1){ # x is lagged from y by n days rowSums(cbind(lead(x, n), y), na.rm = T) } Note the use of na.rm = T to specify that summing a number with an NA should still return a number. If two NAs are summed, the result will be a zero. Try out the lag_sum() function: lag_sum(y15$esc, y15$catch) Add this column to your data set. You add columns in {dplyr} using mutate(): y15 = mutate(y15, run = lag_sum(esc, catch)) The {base} equivalent to mutate() is: y15$run = lag_sum(y15$esc, y15$catch) mutate() has advantages when used with other {dplyr} functions that will be shown soon. Use the new variable run to calculate two new variables: the cumulative run by day (crun) and cumulative run proportion by day (cprun). Cumulative means that each new element is the value that occurred on that day plus all of the values that happened on days before it. You can use R’s cumsum() for this: y15 = mutate(y15, crun = cumsum(run), cprun = crun/sum(run, na.rm = T)) Here is one advantage of mutate(): you can refer to a variable you just created to make a new variable within the same function. You would have to split this into two lines to do it in {base}. Indeed, you could have made run, crun, and cprun all in the same mutate() call. Plot cprun41: plot(cprun ~ doy, data = y15, type = &quot;l&quot;, ylim = c(0, 1), xlab = &quot;DOY&quot;, ylab = &quot;Cumulative Run Proportion&quot;, lwd = 2) 5.7 Apply to all years You have just obtained the daily and cumulative run for one year (2015), but {dplyr} makes it easy to apply these same manipulations to all years. To really see the advantage of {dplyr}, you’ll need to learn to use piping. A code pipe is one that takes the result of one function and inserts it into the input of another function. Here is a basic example of a pipe: rnorm(10) %&gt;% length ## [1] 10 The pipe took the result of rnorm(10) and passed it as the first argument to the length function. This allows you to string together commands so you aren’t continuously making intermediate objects or nesting a ton of functions together. The pipe essentially does this: x = rnorm(10); length(x); rm(x) ## [1] 10 The reason piping works so well with {dplyr} is because its functions are designed to take a data frame as input as the first argument and return another data frame as output. This allows you to string them together with pipes. Do a more complex pipe: take your main data frame (dat), group it by year, and pass it to a mutate() call that will add the new three columns to the entire data set: dat = dat %&gt;% group_by(year) %&gt;% mutate(run = lag_sum(esc, catch), crun = cumsum(run), cprun = crun/sum(run, na.rm = T)) arrange(dat, year) ## # A tibble: 2,550 x 7 ## # Groups: year [50] ## doy year esc catch run crun cprun ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 160 y_1917 NA 175 265 265 0.0127 ## 2 161 y_1917 90 221 334 599 0.0288 ## 3 162 y_1917 113 242 366 965 0.0464 ## 4 163 y_1917 124 90 136 1101 0.0530 ## 5 164 y_1917 46 134 203 1304 0.0627 ## 6 165 y_1917 69 76 115 1419 0.0683 ## 7 166 y_1917 39 89 134 1553 0.0747 ## 8 167 y_1917 45 139 210 1763 0.0848 ## 9 168 y_1917 71 127 192 1955 0.0941 ## 10 169 y_1917 65 61 92 2047 0.0985 ## # ... with 2,540 more rows The group_by() function is spectacular: you grouped the data frame by year, which tells the rest of the functions in the pipe to apply its commands to each year independently. With this function, you can group your data in any way you please and calculate statistics on a group-by-group basis. Note that in this example, it doesn’t do much for you, because the data are so neat (all years have the same number of days, and NAs in all of the same places, etc.), but for data that are messier, this trick is very handy. Also note that the dat object looks a little different. When you print it, it only shows the first 10 rows. This is a {dplyr} feature that prevents you from printing a huge data set to the console. Now plot the cumulative run proportion by day for each year, highlighting 2015: par(sp) # make an empty plot plot(x = 0, y = 0, type = &quot;n&quot;, xlim = range(dat$doy), ylim = c(0,1), xlab = &quot;DOY&quot;, ylab = &quot;Cumulative Run Proportion&quot;) years = levels(dat$year) # use sapply to &quot;loop&quot; through years, drawing lines for each tmp = sapply(years, function(x) { lines(cprun ~ doy, data = filter(dat, year == x), col = ifelse(x == &quot;y_2015&quot;, &quot;blue&quot;, &quot;grey&quot;)) }) 5.8 Calculate Daily Means with summarize() Oftentimes, you want to calculate a statistic for a value across grouping variables, like year or site or day. Remember the tapply() function does this in {base}. Here you want to calculate the mean cumulative run proportion for each day averaged across years. For this, you can make use of the summarize() function in {dplyr}. Begin by ungrouping the data to remove the year groups and grouping the data by doy. This will allow you to calculate the mean proportion by day. You then pass this grouped data frame to summarize() which will apply the mean() function to the cumulative run proportions across all years on a particular day. It will do this for all days: mean_cprun = dat %&gt;% ungroup %&gt;% group_by(doy) %&gt;% summarize(cprun = mean(cprun)) head(mean_cprun) ## # A tibble: 6 x 2 ## doy cprun ## &lt;int&gt; &lt;dbl&gt; ## 1 160 0.00970 ## 2 161 0.0194 ## 3 162 0.0291 ## 4 163 0.0387 ## 5 164 0.0491 ## 6 165 0.0579 The way to do this in {base} is with tapply(): tapply(dat$cprun, dat$doy, mean)[1:6] ## 160 161 162 163 164 165 ## 0.009699779 0.019400196 0.029111609 0.038735562 0.049130566 0.057901058 Note that you get the same result, only the output from tapply() is a vector, and the output of summarize() is a data frame. One disadvantage of the summarize() function, however, is that it can only be used to apply functions that give one number as output (e.g., mean(), max(), sd(), length(), etc.). These are called aggregate functions: they take a bunch of numbers and aggregate them into one number. tapply() does not have this constraint. Illustrate this by trying to use the range() function (which combines the minimum and maximum values of some vector into another vector of length 2). # with summarise dat %&gt;% ungroup %&gt;% group_by(doy) %&gt;% summarize(range = range(cprun)) # with tapply tapply(dat$cprun, dat$doy, range)[1:5] You can see that tapply() allows you to do this (but gives a list), whereas summarize() returns a short and informative error. You could very easily fix the problem by making minimum and maximum columns in the same summarize() call: dat %&gt;% ungroup %&gt;% group_by(jday) %&gt;% summarize(min = min(cprun), max = max(cprun)) Add the mean cumulative run proportion to our plot (use lines() just like before, add it after the sapply() call): lines(cprun ~ doy, data = mean_cprun, lwd = 3) It looks like 2015 started like an average year but the peak of the run happened a couple days earlier than average. 5.9 More summarize() Your colleagues have been talking lately about how the run has been getting earlier and earlier in recent years. To determine if their claims are correct, you decide to develop a method to find the day on which 50% of the run has passed (the median run date) and plot it over time. If there is a downward trend, then the run has been getting earlier. First, you need to define another function to find the day that corresponds to 50% of the run. If there were days when the cumulative run on that day equaled exactly 0.5, we could simply use which(): ind = which(dat$cprun == 0.5) dat$jday[ind] The which() function is useful: it returns the indices (element places) for which the condition is TRUE. However, you need to do a workaround here, since the median day is likely not exactly 0.5, and it must be .5 in order for == to return a TRUE). You will give the function two vectors and it will look for a value that is closest to 0.5 in one vector and pull out the corresponding value in the second vector: find_median_doy = function(p,doy) { ind = which.min(abs(p - 0.5)) doy[ind] } This function will take every element of p, subtract 0.5, take the absolute value of the results (make it positive, even if it is negative), and find the element number that is smallest. This will be the element with the value closest to 0.5. Note that you could find the first quartile (0.25) or third quartile (0.75) of the run by inserting these in for 0.5 above42. Try your function on the 2015 data only: # use function med_doy15 = find_median_doy(p = y15$cprun, doy = y15$doy) # pull out the day it called the median to verify it works filter(y15, doy == med_doy15) %&gt;% select(cprun) ## cprun ## 1 0.5023032 The select() function in {dplyr} extracts the variables requested from the data frame. If there is a grouping variable set using group_by(), it will be returned as well. It looks like the function works. Now combine it with summarize() to calculate the median day for every year: med_doy = dat %&gt;% group_by(year) %&gt;% summarize(doy = find_median_doy(cprun, doy)) head(med_doy) ## # A tibble: 6 x 2 ## year doy ## &lt;fct&gt; &lt;int&gt; ## 1 y_1917 184 ## 2 y_1919 188 ## 3 y_1921 179 ## 4 y_1923 184 ## 5 y_1925 182 ## 6 y_1927 188 Now, use your {dplyr} skills to turn the year column into a numeric variable: # get years as a number med_doy$year = ungroup(med_doy) %&gt;% # extract only the year column select(year) %&gt;% # turn it to a vector and extract unique values unlist %&gt;% unname %&gt;% unique %&gt;% # replace &quot;y_&quot; with nothing gsub(pattern = &quot;y_&quot;, replacement = &quot;&quot;) %&gt;% # turn to a integer vector as.integer Now plot the median run dates as a time series: plot(doy ~ year, data = med_doy, pch = 16) It doesn’t appear that there is a trend in either direction, but fit a regression line because you can (Section 3.1.1): fit = lm(doy ~ year, data = med_doy) summary(fit)$coef plot(doy ~ year, data = med_doy, pch = 16) abline(fit, lty = 2) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 193.781416567 27.80436398 6.9694605 8.190067e-09 ## year -0.005138055 0.01414108 -0.3633424 7.179445e-01 The results tell you that there is a 0.005 day decrease in median run date for every 1 year that has gone by and that it is not significantly different from a zero day decrease per year. The statistical evidence does not support your colleagues’ speculations. 5.10 Fit the Model You have done some neat data exploration exercises (and hopefully learned {dplyr} and piping along the way!), but your ultimate task with this data set is to run a spawner-recruit analysis on all the data. A spawner-recruit analysis is one that links the number of total fish produced in a year (the recruits) to the total number of fish that produced them (the spawners). This is done in an attempt to describe the productivity and carrying capacity of the population and to obtain biological reference points off of which harvest policies can be set. You will need to summarize the daily data into annual totals of escapement and total run: sr_dat = dat %&gt;% summarize(S = sum(esc, na.rm = T), R = sum(run, na.rm = T)) head(sr_dat) ## # A tibble: 6 x 3 ## year S R ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 y_1917 7031 20785 ## 2 y_1919 19647 43553 ## 3 y_1921 45106 113682 ## 4 y_1923 69160 196212 ## 5 y_1925 66306 171006 ## 6 y_1927 96622 243212 You didn’t need to use group_by() here because the data were still grouped from an earlier task. But, there is no harm in putting it in, and it would help make your code more readable if you were to include it. That is the main data manipulation you need to do in order to run the spawner-recruit analysis. You can fit the model using nls() (Section 3.4). The model you will fit is called a Ricker spawner-recruit model and has the form: \\[\\begin{equation} R_t = \\alpha S_{t-1} e^{-\\beta S_{t-1} + \\varepsilon_t} ,\\varepsilon_t \\sim N(0,\\sigma) \\tag{5.1} \\end{equation}\\] where \\(\\alpha\\) is a parameter representing the maximum recruits per spawner (obtained at very low spawner abundances) and \\(\\beta\\) is a measure of the strength of density-dependent mortality. Notice that the error term is in the exponent, which makes \\(e^{\\varepsilon_t}\\) lognormal. To get nls() to fit this properly, we will need to fit to \\(log(R_t)\\) as the response variable. Write a function that will predict log recruitment from spawners given the two parameters (ignore the error term): ricker = function(S, alpha, beta) { log(alpha * S * exp(-beta * S)) } Fit the model to the data using nls(). Before you fit it, however, you’ll need to do another lag. This is because the run that comes back in one year was spawned by the escapement the previous odd year (you only have odd years in the data). This time extract the appropriate years “by-hand” rather than using the lag() or lead() functions as before: # the number of years nyrs = nrow(sr_dat) # the spawners to fit to: S_fit = sr_dat$S[1:(nyrs - 1)] # the recruits to fit to: R_fit = sr_dat$R[2:nyrs] Fit the model: fit = nls(log(R_fit) ~ ricker(S_fit, alpha, beta), start = c(alpha = 6, beta = 0)) coef(fit); summary(fit)$sigma ## alpha beta ## 5.341553e+00 6.896867e-06 ## [1] 0.4047541 Given that the true values for these data were: \\(\\alpha = 6\\), \\(\\beta = 8\\times10^{-6}\\), and \\(\\sigma = 0.4\\), these estimates look pretty good. Plot the recruits versus spawners and the fitted line: # plot the S-R pairs plot(R_fit ~ S_fit, pch = 16, col = &quot;grey&quot;, cex = 1.5, xlim = c(0, max(S_fit)), ylim = c(0, max(R_fit))) # extract the estimates ests = coef(fit) # obtain and draw on a fitted line S_line = seq(0, max(S_fit), length = 100) R_line = exp(ricker(S = S_line, alpha = ests[&quot;alpha&quot;], beta = ests[&quot;beta&quot;])) lines(R_line ~ S_line, lwd =3) # draw the 1:1 line abline(0, 1, lty = 2) The diagonal line is the 1:1 replacement line: where 1 spawner would produce 1 recruit. The distance between this line and the curve is the theoretical harvestable surplus available at each spawner abundance that would keep the stock at a fixed abundance. The biological reference points that might be used in harvest management are: \\[\\begin{eqnarray*} &amp;&amp; S_{MAX}=\\frac{1}{\\beta},\\\\ &amp;&amp; S_{eq}=log(\\alpha) S_{MAX},\\\\ &amp;&amp; S_{MSY}=S_{eq} \\left(0.5-0.07*log(\\alpha)\\right) \\tag{5.2} \\end{eqnarray*}\\] Where \\(S_{MAX}\\) is the spawner abundance expected to produce the maximum recruits, \\(S_{eq}\\) is the spawner abundance that should produce exactly replacement recruits, and \\(S_{MSY}\\) is the spawner abundance that is expected to produce the maximum surplus (all under the assumption of no environmental variability). You can calculate the reference points given in Equation (5.2) from your parameter estimates: Smax = 1/ests[&quot;beta&quot;] Seq = log(ests[&quot;alpha&quot;]) * Smax Smsy = Seq * (0.5 - 0.07 * log(ests[&quot;alpha&quot;])) brps = round(c(Smax = unname(Smax), Seq = unname(Seq), Smsy = unname(Smsy)), -3) Draw these reference points onto your plot and label them: abline(v = brps, col = &quot;blue&quot;) text(x = brps, y = max(R_fit) * 1.08, labels = names(brps), xpd = T, col = &quot;blue&quot;) Exercise 5 In this exercise, you will be working with another simulated salmon data set (from an age-structured population this time, like Chinook salmon Oncorhynchus tshawytscha). You have information on individual fish passing a weir. A weir is a blockade in a stream that biologists use to count fish as they pass through. Most of the day, the weir is closed and fish stack up waiting to pass. Then a couple times a day, biologists open a gate in the weir and count fish as they swim through. Each year, the biologists sample a subset of fish that pass to get age, sex, and length. There are concerns that by using large mesh gill nets, the fishery is selectively taking the larger fish and that this is causing a shift in the size-at-age, age composition, and sex composition. If this is the case, it could potentially mean a reduction in productivity since smaller fish have fewer eggs. You are tasked with analyzing these data to determine if these quantities have changed overtime. Note that documenting directional changes in these quantities over time and the co-occurrence of the use of large mesh gill nets does not imply causation. The solutions to this exercise are found at the end of this book (here). You are strongly recommended to make a good attempt at completing this exercise on your own and only look at the solutions when you are truly stumped. Use the data found in asl.csv (age, sex, length) (see the instructions for details on acquiring the data files for this book). Create a new R script Ex5.R in your working directory. Open this script and read the data file into R. If this is a new session, load the {dplyr} package. Count the number of females that were sampled each year using the {dplyr} function n() within a summarize() call (Hint: n() works just like length - it counts the number of records). Calculate the proportion of females by year. Plot percent females over time. Does it look like the sex composition has changed over time? Calculate mean length by age, sex, and year. Come up with a way to plot a time series of mean length-at-age for both sexes. Does it look like mean length-at-age has changed over time for either sex? Exercise 5 Bonus Calculate the age composition by sex and year (what proportion of all the males in a year were age 4, age 5, age 6, age 7, and same for females). Plot the time series of age composition by sex and year. Does it look like age composition has changed over time? References "],
["ch6.html", "Chapter 6 Mapping and Spatial Analysis 6.1 Chapter Overview 6.2 Before You Begin 6.3 Geospatial Data 6.4 Importing Geospatial Data 6.5 Plotting 6.6 Manipulating Spatial Data 6.7 Analysis 6.8 Creating a Presentable Map 6.9 Other R Mapping Packages Exercise 6 Exercise 6 Bonus", " Chapter 6 Mapping and Spatial Analysis This chapter was contributed by Henry Hershey 6.1 Chapter Overview R is a relatively under-used tool for creating Geographic Information Systems (GIS). Most people use ArcGIS, QGIS, or Google Earth to display and analyze spatial data. However, R can do much of what you might want to do in those programs, with the added benefit of allowing you to create a reproducible script file to share. Workflows can be difficult to replicate in ArcGIS because of the point-click user interface. With R, anyone can replicate your geospatial analysis with your script file and data. In this chapter, you will learn the basics of: creating a GIS in R mapping parts of your GIS “selecting by attribute” (A.K.A.subset() or dplyr::filter() in R) manipulating spatial objects You will map and analyze data from a brown bear (Ursus arctos) tracking study (Kaczensky et al. 1999). In Slovenia, a railroad connects the capitol Ljubljana with the coastal city of Koper, and passes right through brown bear country. You will use R to assess potential sites to build a hypothetical wildlife overpass so that bears can safely cross the railroad. It is worth mentioning that if you are left wanting more information about geospatial analysis in R after this brief introduction, a thorough description of more advanced techniques is presented for free in Lovelace, Nowosad, and Muenchow (2018). 6.2 Before You Begin You should create a new directory and R script for your work in this chapter called Ch6.R and save it in the directory C:/Users/YOU/Documents/R-Book/Chapter6. Set your working directory to that location. Revisit the material in Sections 1.2 and 1.3 for more details on these steps. For this chapter, it will be helpful to have the data in your working directory. In the Data/Ch6 folder (see the instructions on acquiring the data files), you’ll find: a file named bear.csv, folders named railways, states, and SVN_adm, and an R script called points_to_line.R (Walker 2015). Copy (or cut if you’d like) all of these files/folders from Data/Ch6 into your working directory. Mapping in R requires many packages. Install all of the following packages: install.packages( c(&quot;sp&quot;,&quot;rgdal&quot;,&quot;maptools&quot;, &quot;rgeos&quot;,&quot;raster&quot;,&quot;scales&quot;, &quot;adehabitatHR&quot;,&quot;dismo&quot;, &quot;prettymapr&quot;) ) You will need {dplyr} (Wickham et al. 2017) as well, if you did not install it already for Chapter 5. Rather than load all the packages at the top of the script (as is typically customary), this chapter will load and briefly describe each package immediately prior to using it for the first time. 6.3 Geospatial Data There are two types of geospatial data: vector43 and raster data. R is able to handle both, but for the sake of simplicity, this chapter will mostly deal with vector data. There are three types of vector data that you should be familiar with (Figure 6.1): A point is a pair of coordinates (x = longitude, y = latitude) that represent the location of some observed object or event A line is a path between two or more ordered points. The endpoints of a line are called vertices. A polygon is an area bounded by vertices that are connected in order. In a polygon, the first and last vertex are the same. In this first section, you will learn how to load these different types of data and create spatial objects that you can manipulate and visualize in R. Figure 6.1: The three basic types of GIS vector data 6.4 Importing Geospatial Data 6.4.1 .csv files Begin by loading in coordinate data from bear.csv and creating a points layer from them. The points are from a telemetry study that tracked brown bear movements in south western Slovenia for a period of about 10 years (Kaczensky et al. 1999). Each observation in this data set represents what is called a relocation event - meaning a bear was detected again after it was initially tagged. The variables measured at each relocation event include a location, a time, and several other variables. bear = read.csv(&quot;bear.csv&quot;,stringsAsFactors = F) colnames(bear) ## [1] &quot;event.id&quot; &quot;visible&quot; ## [3] &quot;timestamp&quot; &quot;location.long&quot; ## [5] &quot;location.lat&quot; &quot;behavioural.classification&quot; ## [7] &quot;comments&quot; &quot;location.error.text&quot; ## [9] &quot;sensor.type&quot; &quot;individual.taxon.canonical.name&quot; ## [11] &quot;tag.local.identifier&quot; &quot;individual.local.identifier&quot; ## [13] &quot;study.name&quot; &quot;utm.easting&quot; ## [15] &quot;utm.northing&quot; &quot;utm.zone&quot; There are quite a few variables in these data that are extraneous, so just keep the bare necessities: the date and time a bear was relocated, the name of the observed bear, and the x and y coordinates of its relocation. Also, omit the observations that have an NA value for any of those variables with na.omit(): bear = na.omit(bear[,c(&quot;timestamp&quot;,&quot;tag.local.identifier&quot;, &quot;location.long&quot;,&quot;location.lat&quot;)]) colnames(bear) = c(&quot;timestamp&quot;,&quot;ID&quot;,&quot;x&quot;,&quot;y&quot;) head(bear) ## timestamp ID x y ## 1 1994-04-23 22-00-00 ancka 14.32837 45.88914 ## 2 1994-04-24 16-40-00 ancka 14.37175 45.87072 ## 3 1994-04-25 14-45-00 ancka 14.39852 45.86659 ## 4 1994-04-26 12-30-00 ancka 14.41215 45.88695 ## 5 1994-04-28 12-00-00 ancka 14.41260 45.87476 ## 6 1994-04-29 09-30-00 ancka 14.40471 45.88957 Notice that the bears have names: unique(bear$ID). Do some more simple data wrangling: # how many records of each bear are there? table(bear$ID) ## ## ancka clio dinko dusan ivan jana janko joze jure klemen ## 351 20 11 43 7 111 7 24 17 19 ## lucia maja metka milan mishko nejc polona srecko urosh vanja ## 169 247 66 3 129 117 208 138 21 65 ## vera vinko ## 92 33 # bears that were relocated fewer than 5 times # cannot be used in future analysis so filter them out library(dplyr) bear = bear %&gt;% group_by(ID) %&gt;% filter(n() &gt;= 5) # now how many bears are there? unique(bear$ID) ## [1] &quot;ancka&quot; &quot;clio&quot; &quot;dinko&quot; &quot;dusan&quot; &quot;ivan&quot; &quot;jana&quot; &quot;janko&quot; ## [8] &quot;joze&quot; &quot;jure&quot; &quot;klemen&quot; &quot;lucia&quot; &quot;maja&quot; &quot;metka&quot; &quot;mishko&quot; ## [15] &quot;nejc&quot; &quot;polona&quot; &quot;srecko&quot; &quot;urosh&quot; &quot;vanja&quot; &quot;vera&quot; &quot;vinko&quot; Now, use the {sp} package (Pebesma and Bivand 2018) to create a spatial object out of your standard R data frame. You can think of this object like a layer in GIS. {sp} lets you create layers with or without attribute tables, but if your spatial data have other attributes like an ID or a timestamp variable, you should always create an object with class SpatialPointsDataFrame to make sure those variables/attributes are stored. In order to convert a data frame into a SpatialPointsDataFrame, you need to specify four arguments: data: the data frame being converted, coords: the coordinates in that dataframe, coords.nrs: the indices for those coordinate vectors in the data data.frame, and proj4string: the projected coordinate system of the data. You will use the WGS84 coordinate reference system. More on coordinate reference systems later in Section 6.6.144. library(sp) bear = SpatialPointsDataFrame( data = bear, coords = bear[,c(&quot;x&quot;,&quot;y&quot;)], coords.nrs = c(3,4), proj4string = CRS(&quot;+init=epsg:4326&quot;) ) Without any reference data these points are essentially useless. You will need to load some more spatial data to get your bearings. You have more spatial data, but they are in a different format. 6.4.2 .shp files The {rgdal} package (Bivand, Keitt, and Rowlingson 2018) facilitates loading spatial data files (i.e., shapefiles) into R. The function readOGR() takes a standard shapefile (.shp), and converts it into a spatial object of the appropriate class (e.g., points or polygons). It takes two arguments: the data source name (dsn, the directory), and the layer name (layer). When you download a shapefile from an open-source web portal, it will often have accompanying files that store the attribute data. Store all of these files in a folder with the same name as the shapefile. Now load in the shapefile that contains a polygon for the boundary of Slovenia (Boundaries 2018) from the directory, so you can see where in the country the brown bears were detected: #load all the shapefiles for the background map. #you&#39;ll learn how to add a basemap later library(rgdal) #border of slovenia slovenia = readOGR(dsn = &quot;./SVN_adm&quot;,layer = &quot;SVN_adm0&quot;) There are two important differences between readOGR() and read.csv(): The directory shortening syntax is not the same. Notice the period in the data source names. This indicates your working directory. When calling the layer name, the file extension .shp is not required. Notice the feature class of slovenia is a polygon: class(slovenia) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; Now read in two other shapefiles: one showing the statistical regions (aka states) of Slovenia (Boundaries 2018) one showing the railways in Slovenia (MapCruzin 2018) #major railroads in slovenia railways = readOGR(dsn = &quot;./railways&quot;, layer = &quot;railways&quot;) #statistical areas (states) stats = readOGR(dsn = &quot;./SVN_adm&quot;, layer = &quot;SVN_adm1&quot;, stringsAsFactors = F) 6.5 Plotting Plotting spatial objects in R is a breeze. See what happens if you just plot the bear object: plot(bear) You can clean up your map a bit with standard {graphics} arguments: library(scales) par(mar = c(2,2,1,1)) plot(bear, col = alpha(&quot;blue&quot;, 0.5), pch = 16, axes = T) The alpha() function from the {scales} package (Wickham 2018) allows you to plot with transparent colors, which is helpful for seeing the high- versus low-density clusters. Now, plot your reference layers to get your bearings: #make a map of all the bear relocations and railways in slovenia par(mar = c(2,2,1,1)) plot(stats, border = &quot;grey&quot;, axes = T) plot(slovenia, lwd = 3, add = T) # you can draw multiple plots on top points(bear, pch = 16, cex = 0.5, # or use a low lvl plot function col = alpha(&quot;blue&quot;, 0.5)) lines(railways, col = &quot;red&quot;, lwd = 3) 6.5.1 Zooming You may want to zoom in on the part of Slovenia where the bears are. Spatial objects in R have a slot45 called bbox which is the “boundary box” of the data in that object. You can use the bbox to specify what the xlim and ylim of your map should be: plot(stats, border = &quot;grey&quot;, axes = T, xlim = bear@bbox[1,], # access the boundary box using @ ylim = bear@bbox[2,]) plot(slovenia, lwd = 3, add = T) points(bear, pch = 16, cex = 0.5, col = alpha(&quot;blue&quot;, 0.5)) plot(railways, add = T, col = &quot;red&quot;, lwd = 3) 6.5.2 Plotting Selections In GIS, a subset is often called a selection; it is a smaller subset of a larger data layer. Say you want to see the relocation events for each individual bear at a time. Use sapply() to apply a function to plot each bear’s track on a separate map. Wrap your code inside of a new PDF device (described in Section 2.10.2) so you can scroll through the plots separately: pdf(&quot;Relocations.pdf&quot;, h = 5, w = 5) sapply(unique(bear$ID), function(id) { par(mar = c(2,2,1,1)) plot(stats, border = &quot;grey&quot;, axes = T, xlim = bear@bbox[1,], # access the boundary box using @ ylim = bear@bbox[2,], main = paste(&quot;Bear:&quot;, id)) plot(slovenia, lwd = 3, add = T) points(bear[bear$ID == id,], type = &quot;o&quot;, pch = 16, cex = 0.5, col = alpha(&quot;blue&quot;, 0.5)) plot(railways, add = T, col = &quot;red&quot;, lwd = 3) }) dev.off() When you run this code, it will look like nothing happened. Go to your working directory and open the newly created file Relocations.pdf to see the output. Note that if you want to make changes to the PDF file by running pdf(...); plot(...); dev.off(), you’ll need to close the file in your PDF viewer beforehand46. 6.6 Manipulating Spatial Data Now that you have some data and you’ve taken a look at it, it’s time to learn a few tricks for manipulating them. Looking at your map, see that some of the bears were detected outside of Slovenia. (Bonus points if you can name the country they’re in). Suppose the Slovenian government can’t build wildlife crossings in other countries, so you have to clip the bear data to the boundary of Slovenia. 6.6.1 Changing the CRS Before you can manipulate any two related layers (e.g., clipping), you have to ensure that the two layers have identical coordinate systems. This can be done easily with the spTransform() function in the {sp} package. In order to obtain the coordinate reference system of a spatial object like bear, all you have to do is call proj4string(bear). You can pass this directly to spTransform() like this: slovenia = spTransform(slovenia, CRS(proj4string(bear))) 6.6.2 Clipping Clipping is as simple as a standard subset in R. You can select the relocations that occured only in Slovenia using: bear = bear[slovenia,] Make the same plot as you did in Section 6.5.1 with the clipped bear points. 6.6.3 Adding Attributes What if you wanted to add an attribute to a points object, like the name of the polygon it occurs in? You can find this by extracting the attribute values of one layer (a target) at locations of another layer (a source) with the over() function from the {sp} package. For example, say you wanted to know what statistical region each bear relocation happened in. In this case, the target is the stats layer, and the source is bear layer. These two layers will need to be in the same projection, and the result will be stored in the column bearstats$NAME_1: # get in same projection stats = spTransform(stats, proj4string(bear)) # determine which polygon of stat each bear relocation occured in bearstats = over(bear,stats) head(bearstats) ## ID_0 ISO NAME_0 ID_1 NAME_1 TYPE_1 ## 1 208 SVN Slovenia 7 Osrednjeslovenska Statisticna Regije ## 2 208 SVN Slovenia 7 Osrednjeslovenska Statisticna Regije ## 3 208 SVN Slovenia 5 Notranjsko-kraÅ¡ka Statisticna Regije ## 4 208 SVN Slovenia 7 Osrednjeslovenska Statisticna Regije ## 5 208 SVN Slovenia 7 Osrednjeslovenska Statisticna Regije ## 6 208 SVN Slovenia 7 Osrednjeslovenska Statisticna Regije ## ENGTYPE_1 NL_NAME_1 VARNAME_1 ## 1 Statistical Region &lt;NA&gt; Ljubljana ## 2 Statistical Region &lt;NA&gt; Ljubljana ## 3 Statistical Region &lt;NA&gt; KraÅ¡ka, Notranj ## 4 Statistical Region &lt;NA&gt; Ljubljana ## 5 Statistical Region &lt;NA&gt; Ljubljana ## 6 Statistical Region &lt;NA&gt; Ljubljana Extract just the column you care about and see how many relocations occurred in each state: bearstats = data.frame(stat = bearstats$NAME_1) table(bearstats$stat) ## ## GoriÅ¡ka Jugovzhodna Slovenija Notranjsko-kraÅ¡ka ## 65 2 823 ## Osrednjeslovenska ## 978 Then, you can recombine your extracted attribute values (in bearstats) to your original layer (bear) with spCbind() from the {maptools} package (Bivand and Lewin-Koh 2018): library(maptools) bear = spCbind(bear, bearstats$stat) head(bear@data) ## timestamp ID x y bearstats.stat ## 1 1994-04-23 22-00-00 ancka 14.32837 45.88914 Osrednjeslovenska ## 2 1994-04-24 16-40-00 ancka 14.37175 45.87072 Osrednjeslovenska ## 3 1994-04-25 14-45-00 ancka 14.39852 45.86659 Notranjsko-kraÅ¡ka ## 4 1994-04-26 12-30-00 ancka 14.41215 45.88695 Osrednjeslovenska ## 5 1994-04-28 12-00-00 ancka 14.41260 45.87476 Osrednjeslovenska ## 6 1994-04-29 09-30-00 ancka 14.40471 45.88957 Osrednjeslovenska Now determine how many times each bear was relocated in each state: table(bear$ID, bear$bearstats.stat) 6.7 Analysis Now that you know how to import, plot, and manipulate your data, it’s time to do some analysis. The {rgeos} package (Bivand and Rundel 2018) has a few tools for simple geometric calculations like finding the distance between two points, or the area of a polygon. However, more specialized analyses are often only available in other packages. Two common analytical tasks in animial tracking are: Calculating the home range of an animal, i.e., defining the area where most of the relocations occurred. Finding intersections between tracks and some other line (e.g., river, state boundary, or railroad). In this section, you will use the {rgeos} package and another specialized package to do these tasks. 6.7.1 Home Range Analysis If you look at the plots in Section 6.5.2, it looks like some bears live very close to the railroad, but do not cross it, some live very far away, and others may have crossed it multiple times. Which bears’ home ranges are intersected by the railroad? In order to determine this, you will have to calculate the home range of each animal with the mcp function in the {adehabitatHR} package (Calenge 2006). library(adehabitatHR) #the mcp function requires coordinates #be in the Universal Transverse Mercator system bear = spTransform(bear,CRS(&quot;+proj=utm +north +zone=33 +ellps=WGS84&quot;)) # calculate the home range. mcp is one of 5 functions to do this cp = mcp(xy=bear[,2], percent=95,unin=&quot;m&quot;,unout=&quot;km2&quot;) mcp calculates the minimum convex polygon bounded by the extent of the points which are within some percentile of closeness to the centroid of a group. Points that are very far away from the center are excluded. The standard percentile is 95%. mcp requires three other arguments: xy: the grouping variable of the spatial points data frame (the ID of each bear), unin: the units of the input (meters is the default), and unout: the units of the output. If you run cp, you’ll see that the areas of each polygon are stored in the object in square kilometers. Now, plot the homeranges of each bear, label them using the {rgeos} package, and overlay the railroad on a map. library(rgeos) #match the coordinate system of the home ranges object with the railways layer cp = spTransform(cp, CRS(proj4string(slovenia))) railways = spTransform(railways, CRS(proj4string(slovenia))) #keep only the homeranges that include some part of the railway cp = cp[railways,] # plot the polygons par(mar = c(2,2,1,1)) plot(cp,col=alpha(&quot;blue&quot;, 0.5), axes = T) #rgeos has a bunch of neat functions like this one polygonsLabel(cp, labels = cp$id, method = &quot;buffer&quot;, col = &quot;white&quot;, doPlot=T) lines(railways, lwd = 3,col = &quot;red&quot;) 6.7.2 Finding Intersections Between Two Layers Now that you know which bears crossed the railroad, find out where. You’ll need a user-defined function called points_to_line(), which is stored as a script file in your working directory (Walker 2015). If you ever write a function, you can store it in a script file, and then bring it into your current session using the source() function. This prevents you from needing to paste all the function code every time you want to use it in a new script. Save as many functions as you want in a single script, and they will all be added to your Global Environment when you source() the file47. source(&quot;points_to_line.R&quot;) source(&quot;Data/Ch6/points_to_line.R&quot;) source() essentially highlights all the code in a script and runs it, without you ever having to open the script. # {maptools} is needed by points_to_line() library(maptools) # change CRS bear = spTransform(bear, CRS(proj4string(slovenia))) #turn the bear relocations into tracks bearlines = points_to_line( as.data.frame(bear), long=&quot;x&quot;,lat=&quot;y&quot;, id_field=&quot;ID&quot;, sort_field = &quot;timestamp&quot;) Now that your points have been turned into tracks for each bear, see where they intersect the railroad using the gIntersection() function from the {rgeos} package. Hang on though, this will take your computer a while to run (between 5 and 20 minutes). Optionally, you can start a timer and have the {beepr} package (Bååth 2018) make a sound when the geoprocessing calculations are completed: library(beepr) start = Sys.time() crossings = gIntersection(bearlines, railways) Sys.time() - start; beep(10) 6.8 Creating a Presentable Map Now that you have a points object with the crossings stored, make a map that you can present to policy makers. First, you should get a nicer basemap than R’s blank slate. Use the {dismo} (Hijmans et al. 2017) and {raster} (Hijmans 2017) packages to get one: library(dismo); library(raster) base = gmap(bear, type = &quot;terrain&quot;, lonlat = T) plot(base) Now, put the rest of the layers in the same projection as the basemap: # put the other layers in the same crs as the basemap bear = spTransform(bear, basemap@crs) #note that the method for getting the crs is different for raster objects railways = spTransform(railways, base@crs) slovenia = spTransform(slovenia, base@crs) proj4string(crossings) = base@crs Now, plot the crossings in blue on top of the basemap with the railroad in red. You can add a scale bar and north arrow using the {prettymapr} (Dunnington 2017) package! # plot the basemap plot(base) # draw on the railways and crossing events lines(railways, lwd = 10, col = &quot;red&quot;) points(crossings, col = alpha(&quot;blue&quot;, 0.5), cex = 5, pch = 16) # draw on a legend legend(&quot;bottom&quot;, legend = c(&quot;Railway&quot;, &quot;Crossing Events&quot;), lty = c(1,NA), col = c(&quot;red&quot;, alpha(&quot;blue&quot;, 0.5)), pch = c(NA, 16), lwd = 10, cex = 5, horiz = T, bty = &quot;n&quot;) # draw on other map components: scale bar and north arrow look at maptools addscalebar(plotunit = &quot;latlon&quot;, htin = 0.5, label.cex = 5, padin = c(0.5,0.5)) addnortharrow(pos = &quot;topleft&quot;, scale = 5, padin = c(2, 2.5)) Where are the bears crossing the railroad? It looks like there are two areas of the railroad that get the most bear activity. One in the hairpin turn, and one that’s more spread out between Logatec and Unec. Perhaps there should be wildlife crossings in those areas to protect the more “adventurous” bears. 6.9 Other R Mapping Packages This chapter has covered many of R’s basic mapping capabilities using the built-in plot() functionality, but there are certainly other frameworks in R. Here are a few examples, and a quick Google search should provide you with plenty of information to get up and running. 6.9.1 {ggmap} Just like for {ggplot2} (Wickham and Chang 2016), many R users find the {ggmap} (Kahle and Wickham 2016) package more intuitive than creating maps with plot() like you have done in this chapter. If you have messed around with {ggplot2} or would like a slightly different plotting workflow, look into it more. 6.9.2 {leaflet} The {leaflet} package (Cheng, Karambelkar, and Xie 2018) allows you to make interactive maps. This will only work if your output is HTML-based48 library(leaflet) leaflet() %&gt;% #add a basemap addProviderTiles(providers$Esri.WorldGrayCanvas, group = &quot;Grey&quot;) %&gt;% # change the initial zoom fitBounds(slovenia@bbox[&quot;x&quot;,&quot;min&quot;], slovenia@bbox[&quot;y&quot;,&quot;min&quot;], slovenia@bbox[&quot;x&quot;,&quot;max&quot;], slovenia@bbox[&quot;y&quot;,&quot;max&quot;]) %&gt;% # fill in slovenia addPolygons(data = slovenia, color = &quot;grey&quot;) %&gt;% # draw the railroad addPolylines(data = railways,opacity = 1, color = &quot;red&quot;) %&gt;% # draw the relocations addCircleMarkers(data = bear, color = &quot;blue&quot;, clusterOptions = markerClusterOptions()) %&gt;% # draw the crossing events addCircleMarkers(data = crossings, color = &quot;yellow&quot;, clusterOptions = markerClusterOptions()) Exercise 6 Load the caves.shp shapefile (Republic of Slovenia 2018) from your working directory. Add the data to one of the maps of Slovenia you created in this chapter. How many caves are there? How many caves are in each statistical area? Which bear has the most caves in its homerange? Exercise 6 Bonus Find some data online for a system you are interested in and do similar activities as shown in this chapter. Good examples for obtaining open access spatial data are: Administrative boundaries: https://gadm.org/ Animal tracking data sets: https://www.movebank.org/ US Geological Survey: https://www.usgs.gov/products/maps/gis-data References "],
["exercise-solutions.html", "Exercise Solutions Exercise 1 Solutions Exercise 2 Solutions Exercise 3 Solutions Exercise 4 Solutions Exercise 5 Solutions Exercise 6 Solutions", " Exercise Solutions Exercise 1 Solutions Exercise 1A Solutions 1. Create a new file in your working directory called Ex1A.R. Go to File &gt; New File &gt; R Script. This will create an untitled R script. Go the File &gt; Save, give it the appropriate name and click Save. If your working directory is already set to C:/Users/YOU/Documents/R-Book/Chapter1, then the file will be saved there by default. You can create a new script using CTRL + SHIFT + N as well. 2. Enter these data (found in Table 1.1) into vectors. Call the vectors whatever you would like. Should you enter the data as vectors by rows, or by columns? (Hint: remember the properties of vectors). Because you have both numeric and character data classes for a single row, you should enter them by columns: Lake = c(&quot;Big&quot;, &quot;Small&quot;, &quot;Square&quot;, &quot;Circle&quot;) Area = c(100, 25, 45, 30) Time = c(1000, 1200, 1400, 1600) Fish = c(643, 203, 109, 15) 3. Combine your vectors into a data frame. Why should you use a data frame instead of a matrix? You should use a data frame because, unlike matrices, they can store multiple data classes in the different columns. Refer back the sections on matrices (Section 1.4.3) and data frames (Section 1.4.4) for more details. df = data.frame(Lake, Area, Time, Fish) 4. Subset all of the data from Small Lake. Refer back to Section 1.7 for details on subsetting using indices and by column names, see Section 1.11 for details on logical subsetting. df[df$Lake == &quot;Small&quot;,] # or df[3,] 5. Subset the area for all of the lakes. Refer to the suggestions for question 4 for more details. df$Area # or df[,2] 6. Subset the number of fish for Big and Square Lakes only. Refer to the suggestions for question 4 for more details. df[df$Lake == &quot;Big&quot; | df$Lake == &quot;Square&quot;,&quot;Fish&quot;] # or df$Fish[c(1,3)] 7. You realize that you sampled 209 fish at Square Lake, not 109. Fix the mistake. There are two ways to do this, can you think of them both? Which do you think is better? The two methods are: Fix the mistake in the first place it appears: when you made the Fish vector. If you change it there, all other instances in your code where you use the Fish object will be fixed after you re-run everything. Fish = c(643, 203, 209, 15) # re-run the rest of your code and see the error was fixed Fix the cell in the data frame only: df[df$Lake == &quot;Square&quot;,&quot;Fish&quot;] = 209 The second method would only fix the data frame, so if you wanted to use the vector Fish outside of the data frame, the error would still be present. For this reason, the first method is likely better. 8. Save your script. Close RStudio and re-open your script to see that it was saved. File &gt; Save or CTRL + S Exercise 1B Solutions First, did you find the error? It is the #VALUE! entry in the chao column. You should have R treat this as an NA. The two easiest ways to do this are to either enter NA in that cell or delete its contents. You can do this easily by opening ponds.csv in Microsoft Excel or some other spreadsheet editor. 1. Read in the data to R and assign it to an object. After placing ponds.csv (and all of the other data files) in the location C:/Users/YOU/Documents/R-Book/Data and creating Ex1B.R in your working directory: dat = read.csv(&quot;../Data/ponds.csv&quot;) 2. Calculate some basic summary statistics of your data using the summary() function. summary(dat) 3. Calculate the mean chlorophyll a for each pond (Hint: pond is a grouping variable). Remember the tapply() function. The first argument is the variable you wish to calculate a statistic for (chlorophyll), the second argument is the grouping variable (pond), and the third argument is the function you wish to apply. tapply(dat$chl.a, dat$pond, mean) 4. Calculate the mean number of Chaoborus for each treatment in each pond using tapply(). (Hint: You can group by two variables with: tapply(dat$var, list(dat$grp1, dat$grp2), fun). The hint pretty much gives this one away: tapply(dat$chao, list(dat$pond, dat$treatment), mean) 5. Use the more general apply() function to calculate the variance for each zooplankton taxa found only in pond S-28. First, subset only the correct pond and the zooplankton counts. Then, specify you want the var() function applied to the second dimension (columns). Finally, because chao has an NA, you’ll need to include the na.rm = T argument. apply(dat[dat$pond == &quot;S.28&quot;,c(&quot;daph&quot;, &quot;bosm&quot;, &quot;cope&quot;, &quot;chao&quot;)], 2, var, na.rm = T) 6. Create a new variable called prod in the data frame that represents the quantity of chlorophyll a in each replicate. If the chlorophyll a in the replicate is greater than 30 give it a “high”, otherwise give it a “low”. (Hint: are you asking R to respond to one question or multiple questions? How should this change the strategy you use?) Remember, you can add a new column to a data set using the df$new_column = something(). If the column new_column doesn’t exist, it will be added. If it exists already, it will be written over. You can use ifelse() (not if()!) to ask if each chlorophyll measurement was greater or less than 30, and to do something differently based on the result: dat$prod = ifelse(dat$chl.a &gt; 30, &quot;high&quot;, &quot;low&quot;) Bonus 1. Use ?table to figure out how you can use table() to count how many observations of high and low there were in each treatment (Hint: table() will have only two arguments.). After looking through the help file, you should have seen that table() has a ... as its first argument. After reading about what it takes there, you would see it is expecting: one or more objects which can be interpretted as factors (including character strings)… So if you ran: table(dat$prod, dat$treatment) ## ## Add Control ## high 8 0 ## low 2 10 You would get a table showing how many high and low chlorophyll observations were made for each treatment. Bonus 2. Create a new function called product() that multiplies any two numbers you specify. See Section 1.14 for more details on user-defined functions. Your function might look like this: product = function(a,b) { a * b } product(4,5) ## [1] 20 Bonus 3. Modify your function to print a message to the console and return the value if() it meets a condition and to print another message and not return the value if it doesn’t. product = function(a,b,z) { result = a * b if (result &lt;= z) { cat(&quot;The result of a * b is less than&quot;, z, &quot;so you don&#39;t care what it is&quot;) } else { cat(&quot;The result of a * b is&quot;, result, &quot;\\n&quot;) result } } product(4, 5, 19) ## The result of a * b is 20 ## [1] 20 product(4, 5, 30) ## The result of a * b is less than 30 so you don&#39;t care what it is The use of cat() here is similar to print(), but it is better for printing messages to the console. Exercise 2 Solutions 1. Create a new R script called Ex2.R and save it in the Chapter2 directory. Read in the data set sockeye.csv. Produce a basic summary of the data and take note of the data classes, missing values (NA), and the relative ranges for each variable. File &gt; New File &gt; R Script, then File &gt; Save &gt; call it Ex2.R &gt; Save. Then: dat = read.csv(&quot;../Data/sockeye.csv&quot;) summary(dat) 2. Make a histogram of fish weights for only hatchery-origin fish. Set breaks = 10 so you can see the distribution more clearly. hist(dat[dat$type == &quot;hatch&quot;,&quot;weight&quot;], breaks = 10) 3. Make a scatter plot of the fecundity of females as a function of their body weight for wild fish only. Use whichever plotting character (pch) and color (col) you wish. Change the main title and axes labels to reflect what they mean. Change the x-axis limits to be 600 to 3000 and the y-axis limits to be 0 to 3500. (Hint: The NAs will not cause a problem. R will only use points where there are paired records for both x and y and ignore otherwise). plot(fecund ~ weight, data = dat[dat$type == &quot;wild&quot;,], main = &quot;Fecundity vs. Weight&quot;, pch = 17, col = &quot;red&quot;, cex = 1.5, xlab = &quot;Weight (g)&quot;, xlim = c(600, 3000), ylab = &quot;Fecundity (#eggs)&quot;, ylim = c(0, 3500)) All of these arguments are found in Table 2.1. 4. Add points that do the same thing but for hatchery fish. Use a different plotting character and a different color. points(fecund ~ weight, data = dat[dat$type == &quot;wild&quot;,], pch = 15, col = &quot;blue&quot;, cex = 1.5) 5. Add a legend to the plot to differentiate between the two types of fish. legend(&quot;bottomright&quot;, legend = c(&quot;Wild&quot;, &quot;Hatchery&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), pch = c(15, 17), bty = &quot;n&quot;, pt.cex = 1.5 ) Make sure the correct elements of the legend, col, and pch arguments match the way they were specified in the plot() and lines() calls! 6. Make a multi-panel plot in a new window with box-and-whisker plots that compare (1) spawner weight, (2) fecundity, and (3) egg size between hatchery and wild fish. (Hint: each comparison will be on its own panel). Change the titles of each plot to reflect what you are comparing. vars = c(&quot;weight&quot;, &quot;fecund&quot;, &quot;egg_size&quot;) par(mfrow = c(1,3)) sapply(vars, function(v) { plot(dat[,v] ~ dat[,&quot;type&quot;], xlab = &quot;&quot;, ylab = v) }) 7. Save the plot as a .png file in your working directory with a file name of your choosing. One way to do this: ppi = 600 png(&quot;SockeyeComparisons.png&quot;, h = 5 * ppi, w = 7 * ppi, res = ppi) par(mfrow = c(1,3)) sapply(vars, function(v) { plot(dat[,v] ~ dat[,&quot;type&quot;], xlab = &quot;&quot;, ylab = v) }) dev.off() Bonus 1. Make a bar plot comparing the mean survival to eyed-egg stage for each type of fish (hatchery and wild). Add error bars that represent 95% confidence intervals. First, adapt the calc_se() function to be able to cope with NAs: calc_se = function(x, na.rm = F) { # include a option to remove NAs before calculating SE if (na.rm) x = x[!is.na(x)] sqrt(sum((x - mean(x))^2)/(length(x)-1))/sqrt(length(x)) } Then, calculate the mean and standard error for the % survival to the eyed-egg stage: mean_surv = tapply(dat$survival, dat$type, mean, na.rm = T) se_surv = tapply(dat$survival, dat$type, calc_se, na.rm = T) Then, get the 95% confidence interval: lwr_ci_surv = mean_surv - 1.96 * se_surv upr_ci_surv = mean_surv + 1.96 * se_surv Finally, plot the means and intervals: mp = barplot(mean_surv, ylim = c(0, max(upr_ci_surv))) arrows(mp, lwr_ci_surv, mp, upr_ci_surv, length = 0.1, code = 3, angle = 90) Bonus 2. Change the names of each bar, the main plot title, and the y-axis title. mp = barplot(mean_surv, ylim = c(0, max(upr_ci_surv)), main = &quot;% Survival to Eyed-Egg Stage by Origin&quot;, ylab = &quot;% Survival to Eyed-Egg Stage&quot;, names.arg = c(&quot;Hatchery&quot;, &quot;Wild&quot;)) arrows(mp, lwr_ci_surv, mp, upr_ci_surv, length = 0.1, code = 3, angle = 90) Bonus 3. Adjust the margins so there are 2 lines on the bottom, 5 on the left, 2 on the top, and 1 on the right. Place this line above your barplot(...) code: par(mar = c(2,5,2,1)) Exercise 3 Solutions 1. Perform the same analyses as conducted in Section 3.1 (simple linear regression, ANOVA, ANCOVA, ANCOVA with interaction), using egg_size as the response variable. The predictor variables you should use are type (categorical) and year. You should plot the fit for each model separately and perform an AIC analysis. Practice interpretting the coefficient estimates. First, read in the data: dat = read.csv(&quot;../Data/sockeye.csv&quot;) # regression fit1 = lm(egg_size ~ year, data = dat) # anova fit2 = lm(egg_size ~ type, data = dat) # ancova fit3 = lm(egg_size ~ year + type, data = dat) # ancova with interaction fit4 = lm(egg_size ~ year * type, data = dat) 2. Perform the same analyses as conducted in Section 3.2, this time using a success being having greater than 80% survival to the eyed-egg stage. Use egg_size and type as the predictor variables. You should plot the fitted lines for each model separately and perform an AIC analysis. Practice interpretting the coefficient estimates. dat$binary = ifelse(dat$survival &lt; 80, 0, 1) fit1 = glm(binary ~ egg_size, data = dat, family = binomial) fit2 = glm(binary ~ type, data = dat, family = binomial) fit3 = glm(binary ~ egg_size + type, data = dat, family = binomial) fit4 = glm(binary ~ egg_size * type, data = dat, family = binomial) 3. Make the same graphic as in Figure 3.1 with at least one of the other distributions listed in Table 3.1 (other than the multinomial - being a multivariate distribution, it wouldn’t work well with this code). Try thinking of a variable from your work that meets the uses of each distribution in Table 3.1 (or one that’s not listed). If you run into trouble, check out the help file for that distribution. This example uses the lognormal distribution, with meanlog = 10 and sdlog = 0.25: # parameters meanlog = 10; sdlog = 0.25 # a sequence of possible random variables (fish lengths) lengths = seq(0, 8e4, length = 100) # a sequence of possible cumulative probabilities cprobs = seq(0, 1, length = 100) densty = dlnorm(x = lengths, meanlog, sdlog) # takes specific lengths cuprob = plnorm(q = lengths, meanlog, sdlog) # takes specific lengths quants = qlnorm(p = cprobs, meanlog, sdlog) # takes specific probabilities random = rlnorm(n = 1e4, meanlog, sdlog) # takes a number of random deviates to make # set up plotting region: see ?par for more details # notice the tricks to clean up the plot par( mfrow = c(2,2), # set up 2x2 regions mar = c(3,3,3,1), # set narrower margins xaxs = &quot;i&quot;, # remove &quot;x-buffer&quot; yaxs = &quot;i&quot;, # remove &quot;y-buffer&quot; mgp = c(2,0.4,0), # bring in axis titles ([1]) and tick labels ([2]) tcl = -0.25 # shorten tick marks ) plot(densty ~ lengths, type = &quot;l&quot;, lwd = 3, main = &quot;dlnorm()&quot;, xlab = &quot;Random Variable&quot;, ylab = &quot;Density&quot;, las = 1) plot(cuprob ~ lengths, type = &quot;l&quot;, lwd = 3, main = &quot;plnorm()&quot;, xlab = &quot;Random Variable&quot;, ylab = &quot;Cumulative Probability&quot;, las = 1) plot(quants ~ cprobs, type = &quot;l&quot;, lwd = 3, main = &quot;qlnorm()&quot;, xlab = &quot;P&quot;, ylab = &quot;P Quantile Random Variable&quot;, las = 1) hist(random, breaks = 50, col = &quot;grey&quot;, main = &quot;rlnorm()&quot;, xlab = &quot;Fish Length (mm)&quot;, ylab = &quot;Frequency&quot;, las = 1) box() # add borders to the histogram Bonus 1. Fit a von Bertalannfy growth model to the data found in the growth.csv data file. Visit Section 4.7.1 (particularly Equation (4.2)) for details on this model. Use the initial values: linf = 600, k = 0.3, t0 = -0.2. Plot the fitted line over top of the data. Read in the data: dat = read.csv(&quot;../Data/growth.csv&quot;) Fit the model: fit = nls(length ~ linf * (1 - exp(-k * (age - t0))), data = dat, start = c(linf = 600, k = 0.3, t0 = -0.2)) Plot the fit: ages = seq(min(dat$age), max(dat$age), length = 100) pred_length = predict(fit, newdata = data.frame(age = ages)) plot(length ~ age, data = dat) lines(pred_length ~ ages, lwd = 3) Exercise 4 Solutions Exercise 4A Solutions 1. Simulate flipping an unfair coin (probability of heads = 0.6) 100 times using rbinom(). Count the number of heads and tails. flips = rbinom(n = 100, size = 1, prob = 0.6) flips = ifelse(flips == 1, &quot;heads&quot;, &quot;tails&quot;) table(flips) 2. Simulate flipping the same unfair coin 100 times, but using sample() instead. Determine what fraction of the flips resulted in heads. flips = sample(x = c(&quot;heads&quot;, &quot;tails&quot;), size = 100, replace = T, prob = c(0.6, 0.4)) table(flips)[&quot;heads&quot;]/length(flips) 3. Simulate rolling a fair 6-sided die 100 times using sample(). Determine what fraction of the rolls resulted in an even number. rolls = sample(x = 1:6, size = 100, replace = T) mean(rolls %in% c(2,4,6)) 4. Simulate rolling the same die 100 times, but use the function rmultinom() instead. Look at the help file for details on how to use this function. Determine what fraction of the rolls resulted in an odd number. rolls = rmultinom(n = 100, size = 1, prob = rep(1, 6)) dim(rolls) # rows are different outcomes, columns are iterations # get the fraction of odd numbered outcomes sum(rolls[c(1,3,5),])/sum(rolls) Exercise 4B Solutions 1. Adapt this example to investigate another univariate probability distribution, like -lnorm(), -pois(), or -beta(). See the help files (e.g., ?rpois) for details on how to use each function. This solution uses the rbeta(), qbeta(), and pbeta() functions. These are for the beta distribution, which has random variables that are between zero and one. First, create the parameters of the distribution of interest: mean_p = 0.6 # the mean of the random variable B_sum = 100 # controls the variance: bigger values are lower variance # get the shape parameters of the beta dist beta_shape = c(mean_p * B_sum, (1 - mean_p) * B_sum) Then, generate random samples from this distribution: random = rbeta(100, beta_shape[1], beta_shape[2]) Then, test the qbeta() function: p = seq(0.01, 0.99, 0.01) random_q = quantile(random, p) beta_q = qbeta(p, beta_shape[1], beta_shape[2]) plot(beta_q ~ random_q); abline(c(0,1)) Then, test the `pbeta() function: q = seq(0, 1, 0.05) random_cdf = ecdf(random) random_p = random_cdf(q) beta_p = pbeta(q, beta_shape[1], beta_shape[2]) plot(beta_p ~ q, type = &quot;l&quot;, col = &quot;blue&quot;) points(random_p ~ q, col = &quot;red&quot;) Exercise 4C Solutions 1. What sample size n do you need to have a power of 0.8 of detecting a significant difference between the two tagging methods? Simply increase the maximum sample size considered and re-run the whole analysis: n_try = seq(20, 200, 20) It appears you need about 100 fish per treatment to be able to detect an effect of this size. 2. How do the inferences from the power analysis change if you are interested in p_new = 0.4 instead of p_new = 0.25? Do you need to tag more or fewer fish in this case? This is a argument to your function, so simply change its setting when you execute the analysis: #...more code above this tmp = sim_fit(n = n_try[n], p_new = 0.4) #more code after this... Because the effect is larger, it is easier to detect with fewer observations. 3. Your analysis takes a bit of time to run so you are interested in tracking its progress. Add a progress message to your nested for() loop that will print the sample size currently being analyzed. Simply insert the cat() line in the appropriate place in your loop. This is a handy trick for long-running simulations. for (n in 1:N) { cat(&quot;\\r&quot;, &quot;Sample Size = &quot;, n_try[n]) for (i in 1:I) { ... } } Exercise 4D Solutions 1. Add an argument to ricker_sim() that will give the user an option to create a plot that shows the time series of recruitment, harvest, and escapement all on the same plot. Set the default to be to not plot the result, in case you forget to turn it off before performing the Monte Carlo analysis. Change your function to look something like this: ricker_sim = function(ny, params, U, plot = F) { # extract the parameters out by name: alpha = params[&quot;alpha&quot;] beta = params[&quot;beta&quot;] sigma = params[&quot;sigma&quot;] # create containers: # yep, you can do this R = S = H = NULL # initialize the population in the first year # start the population at being fished at 40% # with lognormal error R[1] = log(alpha * (1 - 0.4))/(beta * (1 - 0.4)) * exp(rnorm(1, 0, sigma)) S[1] = R[1] * (1 - U) H[1] = R[1] * U # carry simulation forward through time for (y in 2:ny) { # use the ricker function with random lognormal white noise R[y] = S[y-1] * alpha * exp(-beta * S[y-1] + rnorm(1, 0, sigma)) #harvest and spawners are the same as before S[y] = R[y] * (1 - U) H[y] = R[y] * U } if (plot) { # the I() lets you calculate a quantity within the plot call plot(I(R/1e6) ~ seq(1,ny), type = &quot;l&quot;, col = &quot;black&quot;, xlab = &quot;Year&quot;, ylab = &quot;State (millions of fish)&quot;, ylim = range(c(R, S, H)/1e6) + c(0,0.2)) lines(I(S/1e6) ~ seq(1,ny), col = &quot;blue&quot;) lines(I(H/1e6) ~ seq(1,ny), col = &quot;red&quot;) legend(&quot;top&quot;, legend = c(&quot;R&quot;, &quot;S&quot;, &quot;H&quot;), lty = 1, bty = &quot;n&quot;, col = c(&quot;black&quot;, &quot;blue&quot;, &quot;red&quot;), horiz = T) } # wrap output in a list object list( mean_H = mean(H), mean_S = mean(S) ) } Then use the function: ricker_sim(ny = 20, params = c(alpha = 6, beta = 1e-7, sigma = 0.4), U = 0.4, plot = T) ## $mean_H ## [1] 8145890 ## ## $mean_S ## [1] 12218835 2. Add an error handler to ricker_sim() that will cause the function to return an error if() the names of the vector passed to the param argument aren’t what the function is expecting. You can use stop(“Error Message Goes Here”) to have your function stop and return an error. Error handlers are useful: they catch common errors that someone might make when using your function and return and informative error. Insert this at the top of your function: if (!all(names(params) %in% c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;))) { stop(&quot;the `params` argument must take a named vector with three elements: &#39;alpha&#39;, &#39;beta&#39;, and &#39;sigma&#39;&quot;) } This says, if not all of the names of the params argument are in the specified vector, then stop the execution of the function and return the error message. 3. How do the results of the trade-off analysis differ if the process error was larger (a larger value of \\(\\sigma\\))? Simply increase the process error variance term (the sigma element of params) to be 0.6 and re-run the analysis: The blue line is the higher process error scenario. It seems that if the process error was higher, you would need to sacrifice more in the escapement objective to obtain the same level of the harvest objective than in the case with lower process error. This makes sense: more variability means more iterations will have escapement less than the criterion. 4. Add implementation error to the harvest policy. That is, if the target exploitation rate is \\(U\\), make the real exploitation rate in year \\(y\\) be: \\(U_y \\sim Beta(a,b)\\), where \\(a = 50U\\) and \\(b = 50(1-U)\\). You can make there be more implementation error by inserting a smaller number other than 50 here. How does this affect the trade-off analysis? For this, add a B_sum argument to your function (this represents the 50 value in the question) and use a different randomly generated exploitation rate each year according to the directions: # ... more code above U_real = rbeta(ny, Bsum * U, Bsum * (1 - U)) # more code below... You can use this instead of a fixed exploitation rate each year, for example: # ...inside a loop S[y] = R[y] * (1 - U_real[y]) H[y] = R[y] * U_real[y] # end of a loop... The red line has B_sum = 50 and the black line has B_sum = 1e6 (really large B_sum reduces the variance of U_real around U). It appears that introducing random implementation errors has a similar effect as increasing the process variance. To acheive the same harvest utility as the case with no implementation error, you need to be willing to sacrifice more in terms of escapement utility. See what happens if you increase or decrease the amount of implementation error the management system has in acheiving a target exploitation rate. 5. Visually show how variable beta random variable is with B_sum = 50. Here are two ways that come to mind: # boxplots at various levels of U target B_sum = 50 out = sapply(U_try, function(x) { rbeta(n = 1000, x * B_sum, B_sum * (1 - x)) }) colnames(out) = U_try boxplot(out, outline = F, col = &quot;goldenrod1&quot;) # a time series at one level of U target U = 0.50 plot(rbeta(20, U * B_sum, B_sum * (1 - U)), type = &quot;b&quot;, col = &quot;red&quot;, pch = 15) abline(h = U, col = &quot;blue&quot;, lty = 2, lwd = 2) Exercise 4E Solutions 1. Replicate the bootstrap analysis but adapted for the linear regression example in Section 3.1.1. Stop at the step where you summarize the 95% interval range. First, read in the sockeye.csv data set: dat = read.csv(&quot;../Data/sockeye.csv&quot;) Then, copy the three functions and change the model to be a linear regression for these data: randomize = function(dat) { # number of observed pairs n = nrow(dat) # sample the rows to determine which will be kept keep = sample(x = 1:n, size = n, replace = T) # retreive these rows from the data dat[keep,] } fit_lm = function(dat) { lm(fecund ~ weight, data = dat) } # create a vector of weights weights = seq(min(dat$weight, na.rm = T), max(dat$weight, na.rm = T), length = 100) pred_lm = function(fit) { # extract the coefficients ests = coef(fit) # predict length-at-age ests[&quot;(Intercept)&quot;] + ests[&quot;weight&quot;] * weights } Then perform the bootstrap by replicating these functions many times: out = replicate(n = 5000, expr = { pred_lm(fit = fit_lm(dat = randomize(dat = dat))) }) Finally, summarize and plot them: summ = apply(out, 1, function(x) c(mean = mean(x), quantile(x, c(0.025, 0.975)))) plot(summ[&quot;mean&quot;,] ~ weights, type = &quot;l&quot;, ylim = range(summ)) lines(summ[&quot;2.5%&quot;,] ~ weights, col = &quot;grey&quot;) lines(summ[&quot;97.5%&quot;,] ~ weights, col = &quot;grey&quot;) 2. Compare the 95% bootstrap confidence intervals to the intervals you get by running the predict function on the original data set with the argument interval = \"confidence\" set. You can obtain these same intervals using the predict() function: pred = predict(lm(fecund ~ weight, data = dat), newdata = data.frame(weight = weights), interval = &quot;confidence&quot;) Plot them over top of the bootstrap intervals to verify the bootstrap worked right: The intervals should look approximately correct. Exercise 4F Solutions 1. Adapt the code to perform a permutation test for the difference in each of the zooplankton densities between treatments. Don’t forget to fix the missing value in the chao variable. See Exercise 2 for more details on this. Read in the data: dat = read.csv(&quot;../Data/ponds.csv&quot;) Calculate the difference in means between the treatments for each zooplankton taxon: Dobs_daph = mean(dat$daph[dat$treatment == &quot;Add&quot;]) - mean(dat$daph[dat$treatment == &quot;Control&quot;]) Dobs_bosm = mean(dat$bosm[dat$treatment == &quot;Add&quot;]) - mean(dat$bosm[dat$treatment == &quot;Control&quot;]) Dobs_cope = mean(dat$cope[dat$treatment == &quot;Add&quot;]) - mean(dat$cope[dat$treatment == &quot;Control&quot;]) Dobs_chao = mean(dat$chao[dat$treatment == &quot;Add&quot;], na.rm = T) - mean(dat$chao[dat$treatment == &quot;Control&quot;], na.rm = T) You can use the same perm() function from the chapter to perform this analysis, except you should add an na.rm argument. All you need to change is the variables you pass to y: perm = function(x, y, na.rm = T) { # turn x to a character, easier to deal with x = as.character(x) # shuffle the x values: x_shuff = sample(x) # calculate the mean of each group: x_bar_add = mean(y[x_shuff == &quot;Add&quot;], na.rm = na.rm) x_bar_ctl = mean(y[x_shuff == &quot;Control&quot;], na.rm = na.rm) # calculate the difference: x_bar_add - x_bar_ctl } Then, run the permutation test on each taxon separately: Dnull_daph = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$daph)) Dnull_bosm = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$bosm)) Dnull_cope = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$cope)) Dnull_chao = replicate(n = 5000, expr = perm(x = dat$treatment, y = dat$chao, na.rm = T)) Then, create the histograms for each species showing the null distribution and the observed difference: par(mfrow = c(1,4), mar = c(2,0,2,0)) hist(Dnull_daph, col = &quot;skyblue&quot;, main = &quot;DAPH&quot;, yaxt = &quot;n&quot;) abline(v = Dobs_daph, col = &quot;red&quot;, lwd = 3) hist(Dnull_bosm, col = &quot;skyblue&quot;, main = &quot;BOSM&quot;, yaxt = &quot;n&quot;) abline(v = Dobs_bosm, col = &quot;red&quot;, lwd = 3) hist(Dnull_cope, col = &quot;skyblue&quot;, main = &quot;COPE&quot;, yaxt = &quot;n&quot;) abline(v = Dobs_cope, col = &quot;red&quot;, lwd = 3) hist(Dnull_chao, col = &quot;skyblue&quot;, main = &quot;CHAO&quot;, yaxt = &quot;n&quot;) abline(v = Dobs_chao, col = &quot;red&quot;, lwd = 3) Finally, calculate the two-tailed hypothesis tests: mean(abs(Dnull_daph) &gt;= Dobs_daph) ## [1] 0 mean(abs(Dnull_bosm) &gt;= Dobs_bosm) ## [1] 2e-04 mean(abs(Dnull_cope) &gt;= Dobs_cope) ## [1] 0 mean(abs(Dnull_chao) &gt;= Dobs_chao) ## [1] 0 It appears that the zooplankton densities differed significantly between treatments for each taxon. 2. Adapt the code to perform a permutation test for another data set used in this book where there are observations of both a categorical variable and a continuous variable. The data sets sockeye.csv, growth.csv, or creel.csv should be good starting points. This example will test the difference in mean length between age 3 and 4 fish in the growth.csv data set. Read in the data and extract only age 3 and 4 fish: dat = read.csv(&quot;../Data/growth.csv&quot;) dat = dat[dat$age %in% c(3,4),] Calculate the observed difference in means (age 4 - age 3): Dobs = mean(dat$length[dat$age == 4]) - mean(dat$length[dat$age == 3]) Adapt the perm() function for this example (no na.rm argument is needed): perm = function(x, y) { # shuffle the x values: x_shuff = sample(x) # calculate the mean of each group: x_bar_3 = mean(y[x_shuff == 3]) x_bar_4 = mean(y[x_shuff == 4]) # calculate the difference: x_bar_4 - x_bar_3 } Then, use the function: Dnull = replicate(n = 5000, expr = perm(x = dat$age, y = dat$length)) Plot the null distribution: hist(Dnull, col = &quot;skyblue&quot;) abline(v = Dobs, col = &quot;red&quot;, lwd = 3) Obtain the two-tailed p-value: mean(abs(Dnull) &gt;= Dobs) ## [1] 0.538 Based on this, it appears there is no significant difference between the mean length of age 3 and 4 fish in this example. 3. Add a calculation of the p-value for a one-tailed test (i.e., that the difference in means is greater or less than zero). Steps 1 - 4 are the same: all you need is Dnull and Dobs. Don’t be afraid to Google this if you are confused. To calculate the p-value for a one-tailed test (null hypothesis is that the mean length of age 4 is less than or equal to the mean length of age 3 fish): mean(Dnull &gt;= Dobs) ## [1] 0.2634 To test the opposite hypothesis (null hypothesis is that the mean length of age 4 fish is greater than or equal to the mean length of age 3 fish): mean(Dnull &lt;= Dobs) ## [1] 0.7372 Exercise 5 Solutions First, load {dplyr} and read in the data: library(dplyr) dat = read.csv(&quot;../Data/asl.csv&quot;) head(dat) ## year sex age length ## 1 1966 Male 4 570 ## 2 1966 Male 4 519 ## 3 1966 Male 4 518 ## 4 1966 Male 4 479 ## 5 1966 Female 4 529 ## 6 1966 Female 4 560 1. Count the number of females that were sampled each year using the {dplyr} function n() within a summarize() call (Hint: n() works just like length - it counts the number of records). Pipe dat to a filter() call to extract only females, then group_by() year, then count the number of records: n_female = dat %&gt;% filter(sex == &quot;Female&quot;) %&gt;% group_by(year) %&gt;% summarize(n_female = n()) 2. Calculate the proportion of females by year. Do a similar task as in the previous question, but count all individuals each year, regardless of sex: n_tot = dat %&gt;% group_by(year) %&gt;% summarize(n_tot = n()) Then merge these two data sets: samp_size = merge(n_tot, n_female, by = &quot;year&quot;) Finally, calculate the fraction that were females: samp_size = samp_size %&gt;% mutate(p_female = n_female/n_tot) 3. Plot percent females over time. Does it look like the sex composition has changed over time? plot(p_female ~ year, data = samp_size, type = &quot;b&quot;) It seems that the proportion of females has varied randomly over time but has not systematically changed since the beginning of the data time series. Note that some of this variability is introduced by sampling. 4. Calculate mean length by age, sex, and year. {dplyr} makes this relatively complex calculation easy and returns an intuitive data frame as output: mean_length = dat %&gt;% group_by(year, age, sex) %&gt;% summarize(mean_length = mean(length)) 5. Come up with a way to plot a time series of mean length-at-age for both sexes. Does it look like mean length-at-age has changed over time for either sex? # set up a 2x2 plotting device with specialized margins par(mfrow = c(2,2), mar = c(2,2,2,2), oma = c(2,2,0,0)) # extract the unique ages ages = unique(mean_length$age) # use sapply() to &quot;loop&quot; over ages sapply(ages, function(a) { # create an empty plot plot(1,1, type = &quot;n&quot;, ann = F, xlim = range(dat$year), ylim = range(filter(mean_length, age == a)$mean_length)) # add a title title(paste(&quot;Age&quot;, a)) # draw on the lines for each sex lines(mean_length ~ year, type = &quot;b&quot;, pch = 16, data = filter(mean_length, age == a &amp; sex == &quot;Male&quot;)) lines(mean_length ~ year, type = &quot;b&quot;, pch = 1, lty = 2, data = filter(mean_length, age == a &amp; sex == &quot;Female&quot;)) # draw a legend if the age is 4 if (a == 4) { legend(&quot;topright&quot;, legend = c(&quot;Male&quot;, &quot;Female&quot;), lty = c(1,2), pch = c(16,1), bty = &quot;n&quot;) } }) # add on margin text for the shared axes mtext(side = 1, outer = T, &quot;Year&quot;) mtext(side = 2, outer = T, &quot;Mean Length (mm)&quot;) Bonus 1. Calculate the age composition by sex and year (what proportion of all the males in a year were age 4, age 5, age 6, age 7, and same for females). This follows a similar workflow as in the calculation of the proportion of females: # get the number by age and sex each year n_age_samps = dat %&gt;% group_by(year, age, sex) %&gt;% summarize(age_samps = n()) # get the number by sex each year n_tot_samps = dat %&gt;% group_by(year, sex) %&gt;% summarize(tot_samps = n()) # merge the two: n_samps = merge(n_age_samps, n_tot_samps, by = c(&quot;year&quot;, &quot;sex&quot;)) # calculate the proportion at each age and sex: n_samps = n_samps %&gt;% ungroup() %&gt;% mutate(age_comp = age_samps/tot_samps) Bonus 2. Plot the time series of age composition by sex and year. Does it look like age composition has changed over time? Create the same plots as for Question 5, but with age composition instead of sex composition: # set up a 2x2 plotting device with specialized margins par(mfrow = c(2,2), mar = c(2,2,2,2), oma = c(2,2,0,0)) # extract the unique ages ages = unique(n_samps$age) # use sapply() to &quot;loop&quot; over ages sapply(ages, function(a) { # create an empty plot plot(1,1, type = &quot;n&quot;, ann = F, xlim = range(dat$year), ylim = range(filter(n_samps, age == a)$age_comp) + c(0, 0.05)) # add a title title(paste(&quot;Age&quot;, a)) # draw on the lines for each sex lines(age_comp ~ year, type = &quot;b&quot;, pch = 16, data = filter(n_samps, age == a &amp; sex == &quot;Male&quot;)) lines(age_comp ~ year, type = &quot;b&quot;, pch = 1, lty = 2, data = filter(n_samps, age == a &amp; sex == &quot;Female&quot;)) # draw a legend if the age is 4 if (a == 4) { legend(&quot;topleft&quot;, legend = c(&quot;Male&quot;, &quot;Female&quot;), lty = c(1,2), pch = c(16,1), bty = &quot;n&quot;) } }) # add on margin text for the shared axes mtext(side = 1, outer = T, &quot;Year&quot;) mtext(side = 2, outer = T, &quot;Age Composition by Sex&quot;) Exercise 6 Solutions Note: these solutions assume you have the layers bear, stat, slovenia, railways, and cp as well as all of the packages used in Chapter 6 already loaded into your workspace. ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Code\\au-r-workshop\\Data\\Ch6\\SVN_adm&quot;, layer: &quot;SVN_adm0&quot; ## with 1 features ## It has 70 fields ## Integer64 fields read as strings: ID_0 OBJECTID_1 ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Code\\au-r-workshop\\Data\\Ch6\\railways&quot;, layer: &quot;railways&quot; ## with 214 features ## It has 3 fields ## Integer64 fields read as strings: osm_id ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Code\\au-r-workshop\\Data\\Ch6\\SVN_adm&quot;, layer: &quot;SVN_adm1&quot; ## with 12 features ## It has 9 fields ## Integer64 fields read as strings: ID_0 ID_1 1. Load the caves.shp shapefile (Republic of Slovenia 2018) from your working directory. Add the data to one of the maps of Slovenia you created in this chapter. # load the caves file caves = readOGR(dsn=&quot;./caves&quot;,layer=&quot;EPO_JAMEPoint&quot;) # put all layers the same projection as bear data caves = spTransform(caves, CRS(proj4string(bear))) stats = spTransform(stats, CRS(proj4string(bear))) slovenia = spTransform(slovenia, CRS(proj4string(bear))) railways = spTransform(railways, CRS(proj4string(bear))) # make the plot par(mar = c(2,2,1,1)) plot(stats, border = &quot;grey&quot;, axes = T, xlim = bear@bbox[1,], # access the boundary box using @ ylim = bear@bbox[2,]) plot(slovenia, lwd = 3, add = T) points(bear, pch = 16, cex = 0.5, col = alpha(&quot;blue&quot;, 0.5)) plot(railways, add = T, col = &quot;red&quot;, lwd = 3) # add the caves layer plot(caves, add=T) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;C:\\Code\\au-r-workshop\\Data\\Ch6\\caves&quot;, layer: &quot;EPO_JAMEPoint&quot; ## with 32 features ## It has 2 fields 2. How many caves are there? nrow(caves) ## [1] 32 3. How many caves are in each statistical area? For this, you will need to add a field to your attribute table. Look back to Section 6.6.3 for details on how to do this. library(maptools) caves = spTransform(caves, proj4string(stats)) cavestats = over(caves, stats) caves = spCbind(caves,cavestats$NAME_1) table(caves$cavestats.NAME_1) ## ## GoriÅ¡ka Jugovzhodna Slovenija Notranjsko-kraÅ¡ka ## 1 5 12 ## Obalno-kraÅ¡ka Osrednjeslovenska ## 2 11 4. Which bear has the most caves in its homerange? First, use the over() function to determine which homeranges have which caves (the returnList = T argument will show all of the polygons each point falls in). cp = spTransform(cp, CRS(proj4string(caves))) homecaves = over(caves, cp, returnList = T) Now just count how many caves were in each bear’s homerange (the bear names are the rownames of homecaves). table(unlist(sapply(homecaves,rownames))) ## ## jana nejc srecko ## 5 2 7 "]
]
